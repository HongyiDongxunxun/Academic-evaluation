{
  "original_filename": "full_1816.md",
  "信息检索模型述评": "",
  "樊振强施鸿宝": {
    "context1": "(西安交通大学)",
    "context2": "构造信息检索的抽象模型是信息检索领域取得较大进展的一个重要方面。在这些模型中最有意义的是概率检索模型，它导致用文献的概率特性来表示文献与用户查询的相关性程度以及整个检索过程的具体描述。",
    "context3": "概率检索模型可作为一次检索策略使用，也可以作为带反馈信息的多次交互检索策略使用。一个高效的信息检索系统应该在一次查询中检出尽量多的相关信息，同时保持检出的不相关信息达到极小，亦即应有高的查全率和查准率。由于用户的需求很少能由一组关键词一次完全准确地表达出来，所以用户想要在一次检索后获得满意的结果是不切实际的，也是不可能的。为了获得满意的结果，用户应在检索过程中不断地修正查询要求。查询修正由用户自己去做有许多困难，首先，相关信息和不相关信息事先是无法知道的；第二，将相关信息的内容特征转换为正确的查询公式是比较困难的。因此，查询修正过程难于控制。查询修正过程可以由系统按用户的相关性评价信息去实现。相关性反馈理论正是应此考虑而发展起来的。",
    "context4": "构造反馈查询的目的是检出更多的相关信息，同时使检出的不相关信息达到极小。假定用户的初始查询为 $Q$ ， $D _ { \\pmb { p } }$ 和 $D _ { \\mathrm { ~ } N _ { } - R }$ 分别为相对 $Q$ 的相关文献集和不相关文献 集，那么，直观上新构造的查询 $Q ^ { \\prime }$ 应同 $D _ { R }$ 中的文献有高的相关性，而同 $D _ { \\mathrm { ~ } N _ { } - R }$ 中的文献有尽量低的相关性，即应使得函数",
    "context5": "$$\nF = { \\frac { 1 } { \\mid D \\ / R \\mid } } \\sum _ { { D _ { i } } \\in { D _ { R } } } S I M ( Q , \\ { D _ { i } } ) \\ - { \\frac { 1 } { \\mid D \\ / x _ { - R } \\mid } } \\sum _ { { D _ { i } } \\in { D _ { N - R } } } S I M \\left( Q , \\ / D _ { i } \\right) \\ + \\ldots\n$$",
    "context6": "取最大值。其中 $S I M \\left( Q , D _ { i } \\right)$ 为计算查询 $Q$ 和文献 $\\pmb { D }$ i之间相关性的函数。Rocchio和Salton[1）建议用如下的修正方法",
    "context7": "$$\n\\pmb { Q } ^ { \\prime } = \\pmb { Q } + \\pmb { a } \\ll \\sum _ { D _ { l } \\leqslant D _ { R } } D _ { i } - b \\ll \\sum _ { D _ { j } \\leqslant D _ { N - R } } D _ { j }\n$$",
    "context8": "其中 $\\pmb { a }$ 和 $^ { b }$ 分别为由 $\\smash { D _ { B } }$ 和 $D _ { \\mathrm { ~ } N - R }$ 确定的参数。C．T. $\\mathbf { Y } _ { \\mathbf { u } } \\mathfrak { c } \\mathfrak { z } \\ \\mathbf { \\mathcal { I } }$ 表明 $a = 1 / | D _ { R } |$ ， $b = 1 / \\vert D _ { N _ { - } R } \\vert$ 在实际中较好。",
    "context9": "近年来人们在检索模型研究方面做了大量工作，发表了许多有关文章，其中包括二元独立模型(BI）(3,4)，TP独立模型 $\\mathfrak { C } 5 , \\mathfrak { e } \\mathfrak { z }$ ，2-Poisson模型 $\\zeta 7 , 8 )$ ，联结的2-Poisson 模型（9),受限的二元依赖性模型，一般的二元依赖模型(10)，基于正态分布的模型[4及树依赖性模中知网2等pt所有这些模型构建立在概率与统计决策理论的基础上。其共同的目的是试图利用用户在检索过程中提供的相关性评价信息去构造新的、能更好地反映用户需求的查询陈述，同时对检索到的信息提供一排序的输出，使得相关性高的信息出现在输出表的头部，从而用户首先看到的是最相关的文献。"
  },
  "Bayes决策理论": {
    "context1": "如前所述，现有的大部分概率检索模型均基于统计决策理论，主要是所谓的Bayes决策理论。Bayes决策理论可应用于信息论、模式识别、博奕、信息检索及医学自动诊断等领域。在信息检索中的应用可描述如下，令 $D = \\{ d \\}$ 为一文献集合， $d = ( d ^ { ( 1 ) } , d ^ { ( 2 ) } , \\cdots , d ^ { ( n ) } )$ 为一篇文献的表示，其中 $\\alpha ^ { ( \\textit { i } ) }$ 为描述文献内容特征的标引词 (或关键词)。 $\\pmb { R } = \\left\\{ \\tau _ { 0 } , \\tau _ { 1 } \\right\\}$ 为相关性类的集合， $\\pmb { r } _ { 0 }$ 表示相关类， $r _ { 1 }$ 表示不相关类。 $\\pmb { \\mathrm { \\Omega } } \\pmb { \\mathrm { \\Omega } }$ 为一决策函数，它根据当前可用信息将每篇文献 $\\pmb { d }$ 指派到 $\\pmb { R }$ 中的某一点，即 ${ \\mathfrak { a } } ( d )$ 的值域为 $\\pmb { R }$ 。此外， $\\imath$ 为一损失函数， $l ( { \\pmb { \\alpha } } ( { \\pmb d } ) =$ $r _ { 1 } \\mid r _ { 3 } \\mid i , j = 0$ ,1表示在类 $\\boldsymbol { r } _ { j }$ 中 ${ \\pmb { a } } ( { \\pmb { d } } )$ 取值 $\\textsf { r } _ { i }$ 时所招致的损失（或 ${ \\pmb { \\alpha } } ( { \\pmb d } )$ 动作错误时造成的损失）。对每一文献 $\\pmb { d }$ ，由动作 ${ \\bf \\nabla } ( \\alpha ( d )$ 所招致的损失为",
    "context2": "$$\nL ( \\mathbf { a } ( d )  d  = \\sum _ { \\mathbf { \\lambda } _ { 3 } } { l ( \\mathbf { a } ( d ) = r _ { i }  r _ { j }  ) P ( r _ { j }  d  _ { \\circ }  }\n$$",
    "context3": "形式地讲，问题就是找出一判别函数使得系统的全局损失达到极小，即使得",
    "context4": "$$\nL = \\int L \\left( \\mathbf { a } \\left( d ^ { \\prime } \\right) \\left| \\bar { d } ^ { \\prime } \\right. \\right) P \\left( \\bar { d } ^ { \\prime } \\right) d ( \\bar { d } ^ { \\prime } )\n$$",
    "context5": "达到极小。欲使 $\\pmb { L }$ 达到极小，那么 ${ \\pmb a } ( { \\pmb d } )$ 的取值应使得每一个 $L \\left( \\mathfrak { a } \\left( d \\right) \\left| d \\right. \\right.$ 极小化。为此，对 $j =$ 0,1求",
    "context6": "$$\nL \\left( \\alpha \\left( d \\right) = r _ { j } \\ \\middle \\vert \\ d \\right) = \\ \\sum _ { i } \\ l \\left( \\alpha \\left( d \\right) = r _ { j } \\ \\middle \\vert \\ r _ { i } \\right) P \\left( r _ { i } \\middle \\vert \\ d \\right) ,\n$$",
    "context7": "选取 $r _ { k }$ 使得",
    "context8": "$$\nL ( \\alpha ( d ) = r _ { k } \\ \\bigg \\vert \\ d ) = \\operatorname* { m i n } _ { i } \\{ L ( \\alpha ( d ) = r _ { i } \\  \\begin{array} { c } { { d } } \\end{array}  \\} _ { \\mathrm { o } }\n$$",
    "context9": "然后将 $\\pmb { d }$ 归入类 $\\boldsymbol { r } _ { k }$ 。对库中所有文献归类后，将集合 $D _ { R } = \\left\\{ d \\ : \\left| \\alpha \\left( d \\right) \\right. = r _ { 0 } \\right\\}$ 中的文献按 $\\textbf { \\em L } ( \\mathfrak { a } ( d )$ $= r _ { 0 } \\left[ d \\right)$ 值递增排序。然后选取一阈值 $\\pmb { t }$ ，将 $L ( \\mathbf { a } \\left( \\mathbf { d } \\right) = r _ { 0 } \\left| \\mathbf { d } \\right.$ 小于等于 $\\pmb { t }$ 的诸文献 $\\pmb { d }$ 依次交给用户。",
    "context10": "假定 $\\pmb { \\alpha }$ 正确动作的损失为0，错误动作的损失为1，即",
    "context11": "$$\nl \\left( \\boldsymbol { \\alpha } \\left( \\boldsymbol { d } \\right) = \\boldsymbol { r } _ { i } \\ \\middle | \\boldsymbol { r } _ { j } \\right) \\ = \\big \\lbrace \\begin{array} { r l } { 1 } & { \\ i \\neq j } \\\\ { \\boldsymbol { \\updownarrow } } & { \\ i , \\ j = 0 , 1 } \\end{array}\n$$",
    "context12": "那么",
    "context13": "$$\nL ( \\mathfrak { a } ( d ) = r _ { ! } \\ \\backslash d ) = \\sum _ { j } l ( \\mathfrak { a } ( d ) = r _ { i } \\ \\backslash r _ { j } ) P ( r _ { j } \\ \\backslash d )\n$$",
    "context14": "$$\n= \\sum _ { i \\neq j } P ( r _ { i } | d ) = 1 - P ( r _ { i } | d ) _ { \\circ }\n$$",
    "context15": "由（3）式知，在0一1损失情况应将d归人类 $r _ { k }$ 使得中国知，https:/www.cnki.net",
    "context16": "$$\nP ( r _ { k } \\mid d ) = m a x \\left\\{ P ( r _ { i } \\mid d ) \\right\\} _ { 0 }\n$$",
    "context17": "因此，按 $L ( \\mathfrak { a } ( d ) = r _ { 0 }  d )$ 递增排序等价于按 $\\pmb { P } ( r _ { 0 } | \\pmb { d } )$ 递减排序，即相关性最高的文献先输出。",
    "context18": "在信息检索中，我们主要关心 $\\pmb { P } ( \\pmb { r } _ { 0 } | \\mathbf { d } )$ （或 $\\pmb { P } ( r _ { 1 } \\{ \\pmb { d } \\} )$ ，即一文献 $\\dot { \\boldsymbol { d } }$ 出现时它为相关的(或不相关的）概率。由Bayes公式",
    "context19": "$$\nP ( r _ { i } | d ) = \\frac { P ( d | r _ { i } ) P ( r _ { i } ) } { \\sum _ { j } P ( d | r _ { j } ) P ( r _ { j } ) }\n$$",
    "context20": "假定 $\\pmb { P } ( \\pmb { r } _ { i } )$ 为先验概率，那么我们的主要任务就在于确定 ${ \\pmb P } ( { \\pmb d } | r _ { i } ) { \\pmb j } = 0 . 1 ,$ 。关于如何计算 $P \\left( d \\right)$ $r _ { \\uparrow }$ ）构成了概率模型研究的核心课题，我们将看到下述各模型之间的主要区别之一在于对 $P$ $( d | r _ { 1 } )$ 的不同近似。下面我们分别介绍在此理论基础上建立的各检索模型。"
  },
  "检索过程模型": "",
  "1．二元独立性模型BI": {
    "context1": "在一篇文献 $\\dot { \\pmb { d } } = ( { \\pmb d } ^ { ( 1 ) } , ~ { \\pmb d } ^ { ( 2 ) } , ~ { \\pmb \\cdots } , ~ { \\pmb d } ^ { ( n ) } )$ 中，假定 $\\mathbf { \\vec { d } } ^ { ( \\mathrm { ~ \\textit ~ { ~ i ~ } ~ } ) }$ 为0或1， $d ^ { ( \\textbf { i } ) } = \\mathbf { 1 }$ $\\mathbf { \\nabla } ( \\bar { d } ^ { ( \\mathrm { ~ i ~ } ) } = \\mathrm { ~ \\mathbf { 0 ~ } ~ } )$ ）表示第 $\\textbf { \\em i }$ 个关键词在 $\\scriptstyle { \\dot { \\mathbf { \\alpha } } }$ 的表示中存在（不存在）。另外，假定在相关文献集及不相关文献集中，关键词的出现是相互独立的。由（6）式知，在检索过程的某一步，系统认为一文献 $\\pmb { d }$ 是相关的，那么应有 $P ( r _ { 0 } | d ) { > } P ( r _ { 1 } | d )$ 。因此，我们可以选择检索判别函数",
    "context2": "$$\ng ( d ) = P ( r _ { 0 } | d ) - P ( r _ { 1 } | d ) _ { c }\n$$",
    "context3": "此时，若 $\\pmb { \\beta } ( \\pmb { d } ) > 0$ ，那么说 $\\pmb { d }$ 是相关的，否则为不相关的。同理可以选取",
    "context4": "$$\n{ \\pmb \\mathrm { \\mathstrut } } ^ { \\prime } ( d ) = \\frac { { \\pmb P } ( \\pmb { r } _ { 0 } | d ) } { { \\pmb P } ( \\pmb { r } _ { 1 } | d ) } ,\n$$",
    "context5": "或由对数函数的单调性也可选择",
    "context6": "$$\n\\pmb { \\mathrm { g } } \\left( \\pmb { d } \\right) = \\mathbf { l o g } \\frac { \\pmb { P } ( \\pmb { r } _ { 0 } \\left| \\pmb { d } \\right. ) } { \\pmb { P } ( \\pmb { r } _ { 1 } \\left| \\pmb { d } \\right. ) } .\n$$",
    "context7": "下面的讨论中我们采用（10）作为检索判别函数。现在检索过程可描述如下，对每一文献 $\\pmb { d }$ ，按（10）求一检索状态值RSV，然后选取一适当的阈值 $k$ ，将 $\\pmb { \\mathrm { g } } ( d ) > _ { k }$ 的d选作相关的，即相关文献集 ${ D _ { R } } = \\left\\{ d \\mid g ( d ) > \\middle | k \\right\\} ,$ ，最后将 $\\pmb { D } _ { \\pmb { R } }$ 中的诸文献d按 ${ \\bf \\ddot { \\pmb { \\nu } } } ( { \\pmb d } )$ 的值递减排序输出。",
    "context8": "现在我们来看如何计算（10），由Bayes定理，（10）可表为",
    "context9": "$$\n\\begin{array} { r l } { \\pmb { \\xi } ( \\pmb { d } ) = \\mathrm { l o g ~ } \\frac { \\pmb { P } ( \\pmb { r } _ { 0 } | \\pmb { d } ) } { \\pmb { P } ( \\pmb { r } _ { 1 } | \\pmb { d } ) } { = } \\mathrm { l o g ~ } \\frac { \\pmb { P } ( \\pmb { d } | \\pmb { r } _ { 0 } ) } { \\pmb { P } ( \\pmb { d } | \\pmb { r } _ { 1 } ) } } & { { } } \\\\ { + \\mathrm { l o g ~ } \\frac { \\pmb { P } ( \\pmb { r } _ { 0 } ) } { \\pmb { P } ( \\pmb { r } _ { 1 } ) } { \\circ } } & { { } } \\end{array}\n$$",
    "context10": "由关键词出现的独立性假定有",
    "context11": "故（11）式又可表为",
    "context12": "$$\ng ( d ) = \\log \\ \\frac { \\displaystyle \\prod _ { i = 1 } ^ { n } P ( d ^ { ( i ) } \\mid r _ { 0 } ) } { \\displaystyle \\prod _ { i = 1 } ^ { n } P ( d ^ { ( i ) } \\mid r _ { 1 } ) } - + \\ \\log \\frac { P ( r _ { 0 } ) } { P ( r _ { 1 } ) } ,\n$$",
    "context13": "$$\n\\begin{array} { r l } & { \\boldsymbol { p } _ { i } = P ( d ^ { ( i \\digamma ) } = 1 \\big | \\boldsymbol { r } _ { 0 } ) , 1 - P _ { i } = P ( d ^ { ( i \\digamma ) } = 0 \\big | \\boldsymbol { r } _ { 0 } ) , } \\\\ & { \\boldsymbol { q } _ { i } = P ( d ^ { ( i \\digamma ) } = 1 \\big | \\boldsymbol { r } _ { 1 } ) , 1 - \\boldsymbol { q } _ { i } = P ( d ^ { ( i \\digamma ) } = 0 \\big | \\boldsymbol { r } _ { 1 } ) _ { \\circ } } \\end{array}\n$$",
    "context14": "那么有",
    "context15": "$$\ng ( d ) = \\log \\ \\frac { \\displaystyle \\prod _ { i = 1 } ^ { n } { p _ { i } ^ { \\ : d ( \\textit { i } ) } ( 1 - p _ { i } ) ^ { \\ : ( 1 - d ( \\textit { i } ) ) } } } { \\displaystyle \\prod _ { i = 1 } ^ { n } { q _ { i } ^ { \\ : d ( \\textit { i } ) } ( 1 - q _ { i } ) ^ { \\ : ( 1 - d ( \\textit { i } ) ) } } } + \\log \\ \\frac { P ( r _ { 0 } ) } { P ( r _ { 1 } ) }\n$$",
    "context16": "$$\n= \\sum _ { i = 1 } ^ { n } \\left\\{ { \\begin{array} { l } { { d ^ { ( i { \\mathrm { ~ } } i { \\mathrm { ~ } } ) } { \\bf l o g } \\ { \\frac { p _ { i } } { q _ { i } } } + ( 1 - d ^ { ( i { \\mathrm { ~ } } i { \\mathrm { ~ } } ) } ) { \\bf l o g } { \\frac { 1 - p _ { i } } { { \\bf l } - q _ { i } } } } } \\end{array} } \\right\\} + { \\bf l o g } \\ { \\frac { { \\bf P } ( { \\boldsymbol { r _ { 0 } } } ) } { { \\boldsymbol { P } } ( { \\boldsymbol { r _ { 1 } } } ) } } \\circ { \\frac { { \\bf N } ( { \\boldsymbol { r _ { 0 } } } ) } { { \\bf l } - { \\boldsymbol { E } } ( { \\boldsymbol { r _ { 0 } } } ) } } ,\n$$",
    "context17": "经重新合并和排列后（13）可进一步表为",
    "context18": "$$\ng ( d ) = \\sum _ { i \\mathop { = } 1 } ^ { n } w _ { \\iota } \\cdot d ^ { ( \\iota ) } + w _ { 0 }\n$$",
    "context19": "其中",
    "context20": "$$\n\\begin{array} { r l } & { w _ { i } = \\log \\frac { p _ { i } ( 1 - q _ { i } ) } { q _ { i } ( 1 - P _ { i } ) } \\qquad i = 1 , \\ \\cdots , \\ n } \\\\ & { w _ { 0 } = \\ \\displaystyle \\sum _ { i = 1 } ^ { n } \\log \\frac { 1 - P _ { i } } { 1 - q _ { i } } + \\log \\frac { P ( r _ { 0 } ) } { P ( r _ { 1 } ) } \\circ } \\end{array}\n$$",
    "context21": "由（14）可看出 ${ \\pmb \\beta } ( { \\pmb d } )$ 为 $\\scriptstyle d ^ { ( \\mathrm { ~ i ~ } ) } i = 1$ ，…， $\\pmb { n }$ 的线函数，其中 $\\pmb { w }$ 可看作是经反馈修正后查询中关键词 $\\textbf { \\em i }$ 的权。（14）可看作是查询 $\\pmb { q } = ( \\pmb { w } _ { 1 } , \\ \\cdots , \\ \\pmb { w } _ { n } )$ 和文献 $\\dot { \\mathbf { \\mathfrak { d } } } = ( \\mathbf { \\mathfrak { d } } ^ { ( \\mathrm { ~ 1 ~ } ) }$ ， $\\mathtt { d } ^ { ( 2 ) }$ ，…， $\\alpha ^ { ( \\textit { n } ) }$ ）经由简单匹配函数 $( d , ~ \\ q ) = \\sum _ { \\ i \\ = 1 } ^ { n } d ^ { ( \\ i \\ ) } \\ \\bullet \\ w _ { i }$ 而求得的匹配系数。 ${ \\pmb q } = ( { \\pmb w } _ { 1 } , ~ { \\pmb w } _ { 2 } , ~ { \\pmb \\cdot \\circ } { \\pmb \\cdot } , ~ { \\pmb w } _ { n } )$ 为由反馈信息构造的新查询。由Neyman-Person[13判断规则知， $\\pmb q$ 在该模型下是最佳的，即在给定的查全率等级，反馈查询 $\\pmb q$ 可使查准率达到极大值。",
    "context22": "${ \\pmb B } { \\pmb I }$ 模型是最简单的概率检索模型。它假定了文献的标引是二元的，同时假定了关键词在相关文献集和不相关文献中的出现是独立的。后面我们将看到这些假定是不合理的。 $\\pmb { B I }$ 模型主要用于由用户提供的相关性评价信息（反馈信息）来构造反馈查询，同时提供一排序的输出。为此我们必须估计后验概率 $\\pmb { p _ { i } }$ 和 $\\textstyle | q _ { i }$ 。SparckJones在{3}中给出了用邻接表的极大似然估计方法，另外在‘14中做了进一步的实验。"
  },
  "2.2-Poisson模型": {
    "context1": "2-Poisson 模型的思想方法源于用于系统评价的Swets模型(15,16）,由Harfer‘7’提出。他窗知献集中所有的关键词严格对应两级不同参数的Poisson分布，起初主要用于标识有效的标引词。后由Shi等在[8中进一步的完善和验证，进一步明确假定了文献集中的所有关键词（specialtywords）在相关文献集和不相关文献集中的出现特征严格对应两级不同参数的Poisson分布，即关键词在相关文献集和不相关文献集中的出现频度分别由两个不同参数 $\\boldsymbol { \\mathscr { u } }$ 和 $v$ 来刻划，而在同类中的随机波动反映在Poisson分布自身中。在数学上，Poisson分布用于刻划独立随机事件序列，所以该模型隐含假定了在同类文献中关键词的出现是独立的。",
    "context2": "假定 $\\mathbf { \\nabla } _ { \\pmb { u } _ { i } }$ 为关键词i在相关文献集中出现次数的期望值， ${ \\pmb v } _ { i }$ 为在不相关文献集中出现次数的期望值。那么在相关文献集和不相关文献集中关键词i出现 $k$ 次的概率分别为",
    "context3": "$$\n\\begin{array} { r l } { f _ { 0 } ( \\boldsymbol { k } ) = \\cfrac { e ^ { - v _ { i } } \\boldsymbol { u } _ { i } ^ { \\textit { k } } } { k ! } \\cdot \\textbf { , } } & { } \\\\ { f _ { 1 } ( \\boldsymbol { k } ) } & { = \\cfrac { e ^ { - v _ { i } } \\boldsymbol { v } _ { i } ^ { \\textit { k } } } { k ! } \\cdot \\textbf { \\dots } } \\end{array}\n$$",
    "context4": "进一步假定任一关键词属于相关文献集的概率 $P ( r _ { 0 } ) = \\mathfrak { x }$ ，从而 $\\pmb { P } ( \\pmb { r } _ { 1 } ) = \\pmb { 1 } - \\pmb { \\pi } _ { \\circ }$ 在整个集合中，一文献包含关键词 $\\pmb { i }$ 的 $k$ 次出现概率为",
    "context5": "$$\nf ( k ) = \\pi \\displaystyle \\frac { e ^ { - u } \\ : _ { i } { u _ { i } } ^ { k } } { k ! } - + \\left( 1 - \\pi \\right) \\displaystyle \\frac { e ^ { - v _ { i } } \\ : _ { v _ { i } } ^ { k } } { k ! }\n$$",
    "context6": "2-Poisson模型起初是用来确定一给定关键词是否可用作标引词的，当用于检索场合时，限制查询只能由一个关键词组成。Harter‘7’用了两分布之间的分离度",
    "context7": "$$\nz = \\frac { u _ { i }  v _ { i } } { \\sqrt { u _ { i } + v _ { i } } }\n$$",
    "context8": "作为关键词 $\\dot { \\pmb { \\imath } }$ 的标引值。即标引词的好坏度量为 $z$ 的函数 $f ( z )$ 。实验 $\\mathfrak { c } 7 \\AA )$ 表明采用2-Poisson分布较之单个Poisson分布在近似实际情况方面更有效。",
    "context9": "在Harter的基础上，D。Krait 将该模型推广到查询可含有多个关键词的情况，从而导致了谓之联结的2-Poisson模型。设 $\\cdot d = ( d ^ { ( 1 ) } , ~ d ^ { ( 2 ) } , ~ \\cdots , ~ d ^ { ( n ) } )$ 中的 $\\pmb { d } ^ { ( \\mathrm { ~ \\pmb { ~ i ~ } ~ } ) }$ 为一正整数，标明该词在对应文献中出现次数的期望值，那么由（12）式有",
    "context10": "$$\n\\begin{array} { l } { { \\displaystyle { \\pmb \\Upsilon } ^ { \\alpha } ( d ) = \\log \\ \\frac { \\displaystyle \\prod _ { i = 1 } ^ { n } \\ { \\pmb u } _ { i } ^ { \\ \\alpha \\mathrm { \\tiny ~ ( ~ \\hat { ~ } { ~ 1 ~ } ) ~ } } e ^ { - \\alpha } \\ i \\left/ { d ^ { ( i ~ \\ \\ ) } } \\ ! \\ \\right.}  } { \\displaystyle \\prod _ { i = 1 } ^ { n } \\ { \\ { \\pmb v } _ { i } ^ { \\ \\ \\alpha \\mathrm { \\tiny ~ ( ~ \\hat { ~ } { ~ 1 ~ } ) ~ } } e ^ { - \\nu } \\ i \\left/ { d ^ { ( i ~ \\ ) } } \\ ! \\ \\right.}  } \\ + \\log \\ \\frac { P ( r _ { 0 } ) } { P ( r _ { 1 } ) } }  \\\\ { { \\displaystyle = \\sum _ { i = 1 } ^ { n } d ^ { ( i ~ \\ ) } \\ \\log \\ \\frac { \\displaystyle \\mathcal { U } _ { i } } { \\displaystyle \\mathcal { D } _ { i } } + \\sum _ { i = 1 } ^ { n } \\ ( \\boldsymbol { v } _ { i } - \\boldsymbol { u } _ { i } ) + \\log \\ \\frac { \\displaystyle \\mathcal { \\pi } } { \\displaystyle 1 - \\pi } } , } \\end{array}\n$$",
    "context11": "令",
    "context12": "$$\n\\begin{array} { l l } { { \\pmb w } _ { i } = \\displaystyle \\log \\frac { \\pmb { u } _ { i } } { \\pmb { v } _ { i } } \\qquad i = 1 , 2 , \\cdots , n } \\\\ { \\qquad } \\\\ { { \\pmb w } _ { 0 } = \\displaystyle \\sum _ { i = 1 } ^ { n } ( \\pmb { v } _ { i } - \\pmb { u } _ { i } ) + \\log \\frac { \\pmb { \\pi } } { \\pmb { 1 } - \\pmb { \\pi } } ~ , } \\end{array}\n$$",
    "context13": "我们得到",
    "context14": "$$\n\\pmb { \\mathscr { s } } ( \\pmb { d } ) = \\sum _ { i = 1 } ^ { n } \\pmb { w } _ { i } \\cdot \\pmb { d } ^ { ( i ) } + \\pmb { w } _ { 0 }\n$$",
    "context15": "中国知网https://www.chki.net",
    "context16": "对（19）的解释同对（14）的解释一样，令 $\\pmb q = ( \\pmb { w } _ { 1 } , ~ \\pmb { w } _ { 2 } , ~ \\pmb { \\cdots } , ~ \\pmb { w } _ { n } )$ ，那么再次 $\\pmb q$ 为在 $2 ^ { - }$ Poisson模型下的最佳反馈查询。这里同样要估计参数 $\\pmb { u } _ { i }$ 和 $| v _ { i } \\rrangle$ ，尽管Harter提出用动量矩的一，二，三级导数求解 $\\mathbf { \\boldsymbol { a } } _ { i }$ 和 $| { \\pmb v } _ { i }$ 的方法并在文[8)中做了完善,但这种估计仍然是粗略的近似。这个估计尚未很好地解决。",
    "context17": "2-Poisson模型较之BI模型在理论上似乎更加完善，它假定了文献标引采用关键词频度加权。另外，从数学的观点看，Poisson 分布是二项分布在试验次数趋于无穷时的近似，所以在实际应用中，当文献集合很大时，2-Poisson 模型比BI模型在理论上能更加精确地描述实际情况，从而也更加有效。"
  },
  "3．基于多元正态分布的模型": {
    "context1": "令 $d = ( d ^ { ( 1 ) } , ~ d ^ { ( 2 ) } , ~ \\cdots , ~ d ^ { ( n ) } )$ ，将 $\\pmb { d }$ 看作 $\\pmb { n }$ 维随机向量，假定 $\\pmb { d }$ 在相关文献和不相关文献集中的权的分布服从多元正态分布。在此假定下 $\\mathsf { S I M } ( d , \\mathrm { ~ \\bf ~ Q ~ } )$ 亦服从正态分布[17）。对相关文献而言， $\\pmb { d }$ 服从 $\\pmb { n }$ 元正态分布，均值向量为 $\\pmb { \\mathscr { u } } = ( \\pmb { \\mathscr { u } } _ { 1 } , ~ \\pmb { \\mathscr { u } } _ { 2 } , ~ \\cdots , ~ \\pmb { \\mathscr { u } } _ { n } )$ ，协方差阵为 $\\pmb { \\Sigma } _ { 1 }$ ；对不相关文献而言， $\\pmb { d }$ 也服从 $\\pmb { n }$ 元正态分布，均值向量 $\\pmb { \\cdot v } = ( \\pmb { v } _ { 1 } , \\ v _ { 2 } , \\ \\cdots , \\ v _ { n } )$ ，协方差阵为 $\\Sigma _ { 2 }$ 。由（12）式有",
    "context2": "$$\n{ \\bf \\pi } _ { } ^ { \\mathfrak { s } } ( d ) = \\log \\frac { ( 2 \\pi ) ^ { - \\frac { n } { 2 } } | \\Sigma _ { 1 } | ^ { - 1 } \\exp \\Big \\lbrace - \\displaystyle \\frac { 1 } { 2 } ( d - u ) \\Sigma _ { 1 } ^ { - 1 } ( d - u ) ^ { \\pi } \\Big \\rbrace } { ( 2 \\pi ) ^ { - \\frac { n } { 2 } } | \\Sigma _ { 2 } | ^ { - 1 } \\exp \\Big \\rbrace - \\displaystyle \\frac { 1 } { 2 } ( d - v ) \\Sigma _ { 4 } ^ { - 1 } ( d - v ) ^ { \\pi } \\Big \\rbrace } + \\log \\frac { P ( r _ { 0 } ) } { P ( r _ { 1 } ) } ,\n$$",
    "context3": "$$\n= \\log \\frac { \\ \\left| \\ \\Sigma _ { 2 } \\ \\right| } { \\ \\left| \\ \\Sigma _ { 1 } \\ \\right| \\ } + \\ \\log \\frac { \\ P ( r _ { 0 } ) } { \\ P ( r _ { 1 } ) } + \\frac { 1 } { 2 } \\big \\{ ( d - v ) \\Sigma _ { 2 } ^ { - 1 } ( d - v ) ^ { T } - ( d - u ) \\Sigma _ { 1 } ^ { - 1 } ( d - u ) ^ { T } \\big \\} ,\n$$",
    "context4": "其中 $| \\pmb { \\Sigma } |$ 表示矩阵 $\\pmb { \\Sigma }$ 的行列式值， $\\Sigma ^ { - 1 }$ 为相应的逆阵， $\\pmb { A } \\pmb { T }$ 表示 $\\pmb { A }$ 的转置。",
    "context5": "令",
    "context6": "$$\n{ \\pmb w } _ { 0 } = \\mathrm { { l o g } } \\ \\frac { \\ \\left| \\ { \\pmb \\Sigma } _ { 1 } \\right| } { \\ \\left| \\ { \\pmb \\Sigma } _ { 2 } \\right| } + \\ \\mathrm { l o g } \\ \\frac { \\ { \\pmb P } ( { \\pmb r } _ { 0 } ) } { { \\pmb P } ( { \\pmb r } _ { 1 } ) }\n$$",
    "context7": "那么",
    "context8": "$$\n{ \\pmb \\xi } ( d ) = \\frac { 1 } { 2 } \\left\\{ ( d - v ) \\Sigma _ { 2 } ^ { - 1 } ( d - v ) ^ { T } - ( d - u ) \\Sigma _ { 1 } ^ { - 1 } ( d - u ) ^ { \\bar { T } } \\right\\} + w _ { 0 }\n$$",
    "context9": "显见按（20）对文献排序相当于按",
    "context10": "$$\n( d - v ) \\Sigma _ { 2 } ^ { - 1 } ( d - v ) ^ { \\tau } - ( d - u ) \\Sigma _ { 1 } ^ { - 1 } ( d - u ) ^ { T }\n$$",
    "context11": "排序。下面有两个特殊情况",
    "context12": "（1）当 $\\pmb { \\Sigma } _ { 1 } = \\pmb { \\Sigma } _ { 2 } = \\pmb { \\Sigma }$ 时，（21）可归结到线性形式，因此时",
    "context13": "$$\n( d - v ) \\Sigma _ { 2 } ^ { - 1 } ( d - v ) ^ { \\tau } - ( d - u ) \\Sigma _ { 1 } ^ { - 1 } ( d - u ) ^ { \\tau }\n$$",
    "context14": "$$\n= 2 d \\Sigma ^ { - 1 } ( u - v ) ^ { T } + v \\Sigma ^ { - 1 } v ^ { T } - u \\Sigma ^ { - 1 } u ^ { T }\n$$",
    "context15": "而 $( v \\pmb { \\Sigma } ^ { - 1 } v ^ { T } - \\pmb { u } \\pmb { \\Sigma } ^ { - 1 } u ^ { T } )$ 与 $\\pmb { d }$ 无关可作为常数略去，所以（21）可归约为",
    "context16": "$$\nd \\Sigma ^ { - 1 } ( u - v ) ^ { \\tau }\n$$",
    "context17": "（2）当 $\\pmb { \\Sigma } _ { 1 } = \\pmb { \\Sigma } _ { 2 } = \\pmb { \\Sigma }$ ，且关键词独立出现时，对所有 $i \\neq j \\pm$ 的元素 $\\Sigma _ { i , j } = \\mathrm { ~ 0 ~ }$ ，所以(22)可进一步归约为",
    "context18": "$$\n\\sum _ { i \\ = 1 } ^ { n } \\textit { \\textbf { w } _ { i } } \\cdot d ^ { ( \\textit { i } ) }\n$$",
    "context19": "其中 $\\pmb { w } _ { i } = ( \\pmb { u } _ { i } - \\pmb { v } _ { i } ) / \\pmb { \\sigma } _ { i } ^ { 2 }$ ， $\\sigma _ { i } ^ { 2 } = V A R [ d ^ { ( \\mathrm { ~ i ~ } ) } ]$ ，即 $d ^ { ( \\mathrm { ~ i ~ } ) }$ 的方差。由以上的推导可知 $\\begin{array} { r } { \\pmb { q } = ( \\pmb { w } _ { 1 } , \\cdots , } \\end{array}$ w型下最焦的www.cnki.net"
  },
  "4．树依赖性模型": {
    "context1": "在前述的模型中均假定了在同一相关性类中关键词的出现是独立的，即不考虑关键词之间的相互依赖关系。然而在实际语言表达中，概念或词的出现常常是有联系的，即非独立。尤其在同类文献中，某些特定的概念常常协同出现。所以建立在独立性假定之上的模型不能较确切地反映客观事实，从而这样的模型在实际中是低效的。",
    "context2": "Van Rijsbergen[11,12试图在BI模型的基础上结合关键词之间的相关性信息，使得模型能建立在一较客观的基础上。树依赖性模型可看作是BI模型的修正，它以树的形式结合了关键词之间主要的相关性信息。在前边独立性假定之下有 $P ( d | r _ { k } ) = \\prod _ { i \\ = \\ 1 } ^ { n } P ( d ^ { ( i \\ ) } \\big | r _ { k } )$ ，而在树依赖模型下则将 $\\pmb { P } ( \\pmb { d } | \\pmb { r } _ { k } )$ 表为",
    "context3": "$$\nP ( d | r _ { k } ) { \\simeq } \\prod _ { i { \\ = 1 } } ^ { n } P ( d ^ { ( m i { \\mathrm { \\ } } ) } | d ^ { ( m _ { \\mathrm { \\ } } ) } ( i { \\mathrm { \\ } } ) ^ { \\mathrm { \\prime } } , \\ r _ { k } )\n$$",
    "context4": "其中 $( m _ { 1 } , \\ \\cdots , \\ m _ { n } )$ 为 $( 1 , \\ \\cdots , \\ n )$ 的一个置换，且 $P ( d ^ { ( \\mathrm { ~ i ~ } ) } / d ^ { ( \\mathrm { ~ } m _ { \\mathrm { ~ 0 ~ } } ) } , \\ r _ { k } ) = P ( d ^ { ( \\mathrm { ~ i ~ } ) } / r _ { k } )$ 。这里假定了每个关键词最多只依赖于一个另外的关键词，其中依赖关系由映射函数 ${ \\bf \\Xi } _ { j ( i ) }$ 定义， ${ \\dot { \\jmath } } ( i )$ 为小于 $\\textbf { \\em i }$ 的正整数。",
    "context5": "在树依赖模型中，关键问题是怎样建立一有效的依赖树，即 $j ( i )$ 怎样取值才能在近似式（24）中保留尽可能多的相关性信息。为此 $j ( i )$ 的取值应使得 $\\mathbf { \\mathcal { d } ^ { ( \\textit { i } ) } }$ 和 $\\alpha ^ { ( \\textit { i } \\langle \\textit { i } \\rangle ) }$ 的互信息极大化。所谓互信息可由两关键词相对独立情况的偏差来度量，即 $| \\vec { d } ^ { ( \\textbf { i } ) }$ 和 $| d ^ { ( \\textit { j } ) }$ 的互信息定义为",
    "context6": "$$\nI ( d ^ { ( i ) } , d ^ { ( j ) } ) = \\sum _ { d ^ { ( i ) } , d ^ { ( j ) } } P ( d ^ { ( i ) } , d ^ { ( j ) } ) \\log \\frac { P ( d ^ { ( i ) } , d ^ { ( j ) } ) } { P ( d ^ { ( i ) } ) P ( d ^ { ( j ) } ) } ,\n$$",
    "context7": "若在关键词集合上定义相关性关系，我们可得到一加权图，其结点为关键词，若两词 $| d ^ { \\textsf { ( i ) } }$ 和$d ^ { ( \\textit { j } ) }$ 相关，那么它们之间存在一条边，该边的权为 $I ( d ^ { ( i ) } , d ^ { ( j ) } )$ 。可表明最好的依赖树为由以上的加权图产生的极大生成树MST（Maximum Spanning Tree)。极大生成树为边权的总和达到极大的生成树。这样得到的依赖树可望将关键词之间最重要的相关性信息保留下来。由（12）和（24）可得检索判别函数如下",
    "context8": "$$\ng \\left( d \\right) = \\log \\frac { \\displaystyle \\prod _ { i = 1 } ^ { n } P \\left( d ^ { \\left( i \\right) } \\mid d ^ { \\left( j \\left( i + 1 \\right) \\right) } , r _ { 0 } \\right) } { \\displaystyle \\prod _ { i = 1 } ^ { n } P \\left( d ^ { \\left( i \\right) } \\mid d ^ { \\left( j \\left( i + 1 \\right) \\right) } , r _ { 1 } \\right) } + \\log \\frac { P \\left( r _ { 0 } \\right) } { P \\left( r _ { 1 } \\right) } .\n$$",
    "context9": "后面我们将表明（26）可表为 $| d ^ { ( \\textsf { i } ) }$ 的二次函数。显然（26）要求的计算量较之独立情况要大得多。从（26）可以看出，此时要估计条件后验概率 $P ( d ^ { ( i ) } \\left| ~ d ^ { ( j ( i ) ) } , ~ r _ { k } \\right. ) ~ k = 0$ ，1，条件概率的估计更困难。Harper和Rijsbergen在(12>中采用了一性能类似，但计算较简单的公式如下",
    "context10": "$$\nw _ { i } = \\sum _ { d ^ { ( i ) } , \\textbf { \\ ' } r _ { j } } \\Delta _ { i } { } _ { j } P ( d ^ { ( i ) } , \\textbf { \\em } r _ { j } ) \\log \\frac { P ( d ^ { ( i ) } , \\textbf { \\em } r _ { j } ) } { P ( d ^ { ( i ) } ) } ,\n$$",
    "context11": "其中",
    "context12": "$$\n\\Delta _ { i j } = \\big \\{ \\begin{array} { l l } { { 1 } } & { { d ^ { ( i ) } = r _ { j } } } \\\\  { \\big \\} } & { { \\big \\langle } } \\end{array}\n$$",
    "context13": "易表明（14）式中的 $\\pmb { w }$ 可改写为如下形式",
    "context14": "$$\nw _ { i } = \\sum _ { d ^ { ( i ) } , \\textit { r } _ { j } } \\Delta _ { i j } \\quad \\log ^ { - \\frac { P ( d ^ { ( i ) } , \\boldsymbol { r } _ { j } ) } { P ( \\bar { d } ^ { ( i ) } ) \\overline { { P ( \\boldsymbol { r } _ { j } ) } } ^ { - } } } \\mathrm { ~ o ~ r ~ } \\quad\n$$",
    "context15": "比较（27）和（28）发现（27）为在（28）的每一项上乘了一个因子 $P ( d ^ { ( \\textit { i } ) } | \\boldsymbol { r } _ { \\textit { j } } )$ 。尽管实验(12已表明（26）和（27）有同样的性能上界，然而对（27）的有效性没有理论上的证明。实验也表明结合关键词之间的相关性信息的确可以提高系统的性能。关键词相关性计算方面的研究也不少，如(18)中提出的非正交基向量空间关键词相关性的相关矩阵和算法，然而不幸的是即使在仅结合二元相关或者树依赖这些简单的情况引起了计算复杂性的巨增。同时条件概率较之无条件概率的估计可靠性也要减小。但作为理论上的探讨是很有价值的。"
  },
  "5．一般的二元依赖性模型": {
    "context1": "在上节中，我们限定一关键词最多只依赖一个另外的关键词，即只考虑一阶依赖，关键词之间的依赖关系由一极大生成树来定义。在理论上，我们可以结合任意关键词之间的相关性信息。下面我们将表明，转换BI模型到其它依赖模型仅仅是对匹配函数选择一不同精度 的近似过程，在本节其余部分，为表达上的简洁，我们记 $\\pmb { P } ( \\pmb { d } | \\pmb { r } _ { 0 } )$ 为 $\\pmb { P } ( \\pmb { X } )$ ，记 $\\pmb { P } ( \\pmb { d } | \\pmb { r } _ { 1 } )$ 为$\\pmb { \\mathcal { R } } ( \\pmb { \\chi } )$ 。另外，为含义上的清楚起见，写 $\\pmb { P } ( \\pmb { X } ) = \\pmb { P } ( \\pmb { x } _ { 1 }$ ， $\\pmb { x } _ { 2 }$ ，…， ${ \\pmb x } _ { \\pmb n }$ ）， $\\pmb { R } ( \\pmb { X } ) = \\pmb { R } ( \\pmb { x } _ { 1 }$ ， $\\pmb { x } _ { 2 }$ ，$\\cdots , x _ { n } )$ 。因在实际中直接计算 $\\pmb { P } ( \\pmb { x } _ { 1 } , \\ \\cdots , \\ x _ { n } )$ 是不可能的，我们现在的问题就是要选择一适当的方法去近似 $\\pmb { P } ( \\pmb { x } _ { 1 } , \\cdots , \\pmb { x } _ { n } )$ 。对 $\\pmb { P } ( \\pmb { x } _ { 1 } , \\ \\cdots , \\ \\pmb { x } _ { n } )$ 的近似有许多方法[19)，首先",
    "context2": "$$\nP ( x _ { 1 } , \\cdots , \\ x _ { n } ) = P ( x _ { 1 } ) P ( x _ { 2 } \\left| x _ { 1 } \\right. ) P ( x _ { 3 } \\left| x _ { 2 } , x _ { 1 } \\right. ) \\cdots\n$$",
    "context3": "$$\nP ( x _ { n } \\mid x _ { n - 1 } , \\mathbf { \\alpha } \\cdots , \\mathbf { \\alpha } x _ { 1 } )\n$$",
    "context4": "对 $P ( x _ { k } \\mid x _ { k - 1 } , \\ \\cdots , \\ x _ { 1 } )$ 我们选择 $x _ { i } ( i < k )$ 使得",
    "context5": "$$\nI ( x _ { i } , \\ x _ { k } ) = \\underset { \\mathbb { I } \\leq \\mathsf { i } < \\mathsf { k } } { \\mathsf { m a x } } \\big \\{ I ( x _ { j } , x _ { k } ) \\big \\} ,\n$$",
    "context6": "定义 ${ j \\left( { k } \\right) } = i$ 后， $P ( x _ { k } \\mid x _ { k - 1 } , \\ \\cdots , \\ x _ { 1 } )$ 可近似为 $P ( \\boldsymbol { x } _ { k } | \\boldsymbol { x } _ { j \\mid k } )$ ，所以我们可以做如下近似",
    "context7": "$$\nP ( x _ { 1 } , \\ \\cdots , \\ x _ { n } ) { \\simeq } P ( x _ { 1 } ) P ( x _ { 2 } | x _ { j ( 2 ) } ) { \\cdots } P ( x _ { n } | x _ { j ( n ) } ) ,\n$$",
    "context8": "此乃树依赖模型中所用的近似方法。（29）称为Chow展开式。",
    "context9": "现在我们介绍Bahadur-Lazarsfeld展开式，在该展开式中我们可清楚地看出，对依赖性结合的程度实际上是对匹配函数的近似程度。Bahadur-Lazarsfeld展开式可表为",
    "context10": "$$\n\\pmb { P } ( \\pmb { x } ) = \\pmb { P } _ { 1 } ( \\pmb { x } ) \\left( 1 + \\pmb { A } \\right)\n$$",
    "context11": "其中",
    "context12": "$$\nA = \\sum _ { i < j } \\partial _ { i } \\ _ { j } y _ { i } y _ { j } \\ + \\ \\sum _ { i < j < k } \\partial _ { i } \\ _ { j } k y _ { 1 } y _ { 2 } y _ { k } \\ + \\ \\cdots + \\partial _ { 1 2 } \\ldots \\circ \\mathscr { y } _ { 1 } y _ { 2 } \\cdots \\mathscr { y } _ { n } ,\n$$",
    "context13": "$$\n\\begin{array} { r l } & { \\boldsymbol { y } _ { i } = ( \\boldsymbol { x } _ { i } - \\boldsymbol { p } _ { i } ) \\big / \\sqrt { \\bar { p } _ { i } ( 1 - \\boldsymbol { p } _ { i } ) } , } \\\\ & { \\partial _ { i j } = E ( \\boldsymbol { y } _ { i } \\boldsymbol { y } _ { j } ) , \\partial _ { i j k } = E ( \\boldsymbol { y } _ { i } \\boldsymbol { y } _ { j } \\boldsymbol { y } _ { k } ) , \\cdots , \\cdots , } \\end{array}\n$$",
    "context14": "这里 $E ( \\cdots ]$ 为求期望值。",
    "context15": "$$\n{ \\pmb p } _ { 1 } \\left( { \\pmb x } \\right) = \\prod _ { i = 1 } ^ { n } { \\pmb p } _ { i } ^ { \\phantom { i } x i } \\left( 1 - { \\pmb p } _ { i } \\right) ^ { 1 - x i } \\circ\n$$",
    "context16": "易见Pi(x)为假定关键词独立出现时P(x)的展开式。A可解释为关键词联结（依赖）强度的度量，亦可看作关键词不独立出现时对 $\\pmb { p } _ { 1 } \\left( \\pmb { x } \\right)$ 的校正因子。对（32）两边取对数后有",
    "context17": "$$\n\\log P ( x ) = \\log \\mathcal { P } _ { 1 } \\left( x \\right) + \\log \\left( 1 + A \\right)\n$$",
    "context18": "对乾圈知网4我们可以用近似式 ${ \\| \\| \\| \\cdot \\| } ( { \\| \\pm | \\| \\operatorname { E } [ A } ) ) { \\simeq } A$ ，这样我们得到",
    "context19": "$$\n\\begin{array} { l }  { { \\mathrm { l o g } } \\ P \\left( { \\pmb x } \\right) = \\displaystyle \\sum _ { i = 1 } ^ { n } \\left[ { \\pmb x } _ { i } \\quad \\mathrm { l o g } \\left( \\frac { { \\pmb p } _ { i } } { \\ { \\bf 1 } - { \\pmb p } _ { i } } \\right) \\right\\} + \\displaystyle \\sum _ { i = 1 } ^ { n } \\mathrm { l o g } \\left( 1 - { \\pmb p } _ { i } \\right) \\quad } \\\\ { + \\displaystyle \\sum _ { i < j } \\partial _ { i } \\ j y _ { i } y _ { j } + \\cdots + \\partial _ { 1 2 } \\bullet . . . \\ n ^ { \\tilde { y } _ { 1 } } y _ { 2 } \\cdots \\pmb y _ { n } . } \\end{array}\n$$",
    "context20": "由于 $\\sum _ { i \\ = 1 } ^ { i ^ { 2 } } \\log \\left( 1 - p _ { i } \\right)$ 与 $\\boldsymbol { x } _ { i }$ 无关，故可略去，从而得",
    "context21": "$$\n\\begin{array} { r l } & { \\log \\ P \\left( \\mathbf { x } \\right) = \\displaystyle \\sum _ { i = 1 } ^ { n } \\left\\{ \\ \\log \\ \\left( \\cdot \\frac { p _ { i } } { 1 - p _ { i } } \\right) \\bullet \\mathbf { x } _ { i } \\right\\} + \\displaystyle \\sum _ { i < j } \\partial _ { i } \\ j y _ { i } y _ { j } + \\cdots } \\\\ & { \\qquad + \\ \\partial _ { 1 2 } . . . \\left. \\mathfrak { s } \\frac { y _ { 1 } y _ { 2 } } { 1 + \\delta } \\mathbf { x } _ { i } \\right\\} } \\end{array}\n$$",
    "context22": "对 $R \\left( x \\right)$ 类似地有",
    "context23": "$$\n\\begin{array} { l } { { \\log \\ R ( { \\pmb x } ) = \\displaystyle \\sum _ { i = 1 } ^ { n } \\bigg \\{ \\ \\log \\ \\bigg ( \\displaystyle \\frac { q _ { i } } { 1 - q _ { i } } \\bigg ) \\ \\bullet \\ x _ { i } \\bigg \\} + \\displaystyle \\sum _ { i < j } \\partial _ { i \\ j } \\mathrm { { ^ \\prime } } y _ { i } \\mathrm { { ^ \\prime } } y _ { j } \\mathrm { { ^ \\prime } } + \\cdots } } \\\\ { { + \\partial _ { 1 2 } ^ { \\prime } \\ldots \\ldots \\partial _ { 1 } y _ { 1 } \\mathrm { { ^ \\prime } } y _ { 2 } \\mathrm { { ^ \\prime } } \\ldots y _ { n } \\mathrm { { ^ \\prime } } _ { \\circ } } } \\end{array}\n$$",
    "context24": "若只考虑 $\\scriptstyle \\operatorname { I o g } P ( x )$ 和 $\\log R \\left( x \\right)$ 中的一次项有",
    "context25": "$$\ng \\left( x \\right) = \\log \\frac { P \\left( x \\right) } { R \\left( x \\right) } = \\sum _ { i = 1 } ^ { n } x _ { i } \\ \\bullet \\ \\log \\ - \\frac { p _ { i } \\left( 1 - q _ { i } \\right) } { q _ { i } \\left( 1 - p _ { i } \\right) } \\circ\n$$",
    "context26": "此时我们得到了BI模型中的匹配函数（略去了常数项 $\\scriptstyle w _ { 0 } \\neq$ 。若考虑 $\\pmb { \\mathrm { l o g } } \\pmb { P } ( \\pmb { x } )$ 和 $\\scriptstyle \\{ \\mathbf { o g } R ( x )$ 中的一次及二次项，即考虑一阶依赖时有",
    "context27": "$$\n\\log P \\left( x \\right) = \\log P _ { 1 } \\left( x \\right) + \\sum _ { i < j } \\partial _ { i } \\mathrm { ~ } _ { j } \\mathcal { Y } _ { i } \\mathcal { Y } _ { j }\n$$",
    "context28": "$$\n\\log R \\left( x \\right) = \\log R _ { 1 } \\left( x \\right) + \\sum _ { i < j } \\partial _ { i } ^ { \\prime } { } _ { j } y _ { i } { } ^ { \\prime } y _ { j } { } ^ { \\prime }\n$$",
    "context29": "显见 ${ \\pmb g } ( { \\pmb x } ) = \\log ( { \\pmb P } ( { \\pmb x } ) / R ( { \\pmb x } ) )$ 这时成为 $\\pmb { x }$ ;的二次函数，从而得到限制的二元依赖模型。",
    "context30": "在理论上，我们可对 ${ \\pmb g } ( { \\pmb x } )$ 做任意近似，即考虑关键词之间的所有依赖关系。然而这一过程随着近似的阶数的增加，可能在计算上带来不可容忍的开销。在实际应用中这样的近似也是不必要的。"
  },
  "结 语": {
    "context1": "上面讨论了基于概率统计决策理论的诸检索模型。在BI，2-Poisson模型中均假定了在同一相关性类中关键词的出现是独立的，这种假定仅仅是为了数学处理上的方便。在树依赖模型中试图结合关键词协同出现的特征，并用实验证实了假设关键词之间具有互相依赖关系的合理性。所有这些模型对集合中文献检索状态值RSV的计算在导致极小错误的意义上是最佳的。一般的二元依赖性模型在前述模型的基础上进一步揭示了结合关键词之间的相关性同匹配函数计算之间的内在联系。然而，不幸的是在多数模型中均涉及到一些后验概率的估计问题。此时出现了两个问题：i）选用什么样的估计方法？传统的统计学并未提供这样一可用的现成方法。ii）样本容量的限制，在一个大数据库中，可能包含大量的文献，而一特定的用户所感兴趣的往往是其中很小的一部分。此外，用户能评价的文献集合（样本）是非常有限的。用非常有限的样本要精确估计关键词在数据库中的分布，即或是可能的，在精度上也要受到限制。在一可靠的参数估计方法成为可用之前，这些模型很难用到实际中去。",
    "context2": "除了参数估计上固有的困难外，上述诸模型还未彻底摆脱早期布尔检索策略的影响，用的想关性评价限定在严格的相关/不相关两级上。而在实际中这种严格的相关和不相关很少发生，用户对一文献的评价往往是比较相关或较少相关的。在检索模型中结合多值相关性度量即使在实际实现中可能遇到困难，但至少对理论上的探讨是必要的。此外，一理想的检索模型应克服参数估计这样的困难。一个具有这些特点的模型正在发展之中。"
  },
  "参考文献": {
    "context1": "【1] Rocchio J.J.，Salton G.：Information search optimization and interactive retrieval techniques。AFIPS Proc.1965 FJCC， part 1， 27,AFIPS PressArlington，Va., 293-305   \n〔2] Yu C.T.，Luk W.S.，and Chung T.Y.：A statistical model for relevance fee. dback in information retrieval, Journal of the Association for Computing Machinery，1976，23(2)，273-286   \n【3] Robertson S.F., Jones K. S.: Relevance weighting of search terms, Journal of the American Society for Information Science,1976,27(3),129-146   \n（4） Chow D.，Yu C. T.: On the construction of feedback query， Journal of the Association for Computing Machinery，1982,29(l)，127-151   \n【5] Yu C.T.，Salton G.: Precision weighting-an effective automatic indexing meth. od,Journal of the Association for Computing Machinery, 1975, 23(1),76-88   \n【6） Yu C.T.，Salton G.：Term weighting in IR using the term precision model, Journal of the Association for Computing Machinery，1982,29(1)，152-170   \n【7] Harter S.P.:A probability approach to automatic Keyword indexing，part 1, Journal of the American Society for Information Science,1975,26(4),197-- 206；part 2，1975，26(5)，280-289   \n〔8] Raghavan V.V.，Shi Hong-bao:Evaluation of the 2-Poisson model as a basis for using term frequency data in searching，Proc.6th Ann. Inter.ACM SIGIR-83 Conf.   \n【9] Cooper W.S.，Maron M.E.: Foundtions of probabilistic and utilitytheoretic indexing, 1978,Journal of the Association for Computing Machinery, 25(1),67-80   \n[10] Yu C.T.，Luk W. S.， Su M. K.: On models of information retrieval processes，Information System，1979,4(3)，205-218   \n{11} Van Rijsbergen C.J.：A theoretical basis for the use of co-occurrence data in information retrieval, Journal of Documentation, 1977, 33(2), 106-115   \n[12） Harper D. J.，Van Rijsbergen C. J.：An evaluation of feedback in document retrieval using co-occurrence data, Journal of Documentation, 1978,34(3)，189-216   \n[13] Larson H.： Introduction to Probability Theory and Statistical Inference,3rd ed, New York，John Wiley and Sons Inc.，1982   \n（14) Jones K. S.: Experiments in relevance weighting of search terms, Information Processing and Management, 1979, 15(2), 133-144   \n[15] Swets J. A.: Information retrieval system, Science, 1963, 141(3577)， 245-250   \n[16] Swets J. A.: Effectiveness of information retrieval methods, American Documentation，1969,20，72-89   \n[17] 复旦大学：《概率论》，人民教育出版社，1979   \n[18] 施鸿宝：检索中关键词对相关性及其算法，《情报学报》，1983，2（4），298—305   \n[19] Duda R.O.，Hart P.E.： Pattern Classification and Science Analysis，John Wiley and Sons Inc.,1973"
  },
  "A REVIEW OF INFORMATION RETRIEVAL": {
    "context1": "Fan Zhen-qiang and Shi Hong-bao"
  }
}