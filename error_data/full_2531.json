{
  "original_filename": "full_2531.md",
  "专辑": {
    "context1": "编者按：第四届\"数据分析与知识发现”学术研讨会(2023年12月5日-6日)由中国科学院文献情报中心主办，厦门大学信息学院和《数据分析与知识发现》编辑部联合承办。会议共计征文131篇,经同行评议录用23篇，形成会议专辑出版。"
  },
  "可解释推荐模型中的可解释性方法研究综述": "",
  "高广尚": {
    "context1": "（广西民族大学人工智能学院 南宁 530006)",
    "context2": "摘要：【目的】从嵌入式和事后处理两个角度分别探讨可解释推荐模型中的可解释性机制。【文献范围】在谷歌学术和中国知网中分别以关键词\"explainable recommendation\"interpretable recommendation\"explainableAI\"\"可解释推荐\"进行文献检索,再结合主题筛选,精读并使用追溯法获得可解释性方法研究的代表性文献共64篇。【方法】从嵌入式角度研究推荐的可解释性方法,具体结合知识图谱、深度学习、注意力机制、多任务学习这4个视角进行探讨分析;从事后处理角度研究推荐的可解释性方法,具体结合预定义模板、语句检索、自然语言生成、强化学习、知识图谱这5个视角进行探讨分析;从逻辑思路、性能特点和局限性三个方面详细比较可解释性方法,最后对可解释性研究亟需解决的问题进行展望。【结果】可解释性能够有效提升推荐系统的说服力,也能够提升用户的使用体验,更是提升推荐系统透明度和可信赖性的关键途径。【局限】未深入分析可解释性算法的评价指标。【结论】尽管现有的可解释性方法能在一定程度上满足诸多应用的解释需求,但在对话交互式解释、因果解释等研究中仍然面临诸多挑战。",
    "context3": "关键词：可解释推荐可解释性解释理由嵌入式解释事后处理解释",
    "context4": "分类号：TP393C931  \nDOI: 10.11925/infotech.2096-3467.2023.0691",
    "context5": "引用本文：高广尚.可解释推荐模型中的可解释性方法研究综述[J].数据分析与知识发现,2024，8（8/9)：619.(Gao Guangshang.A Survey of Explainability Methods in Explainable Recommendation Models[J].DatAnalysis and Knowledge Discovery, 2024, 8(8/9): 6-19.)"
  },
  "1引言": {
    "context1": "目前,推荐系统(Recommender Systems)已被广泛应用于Web应用程序中，扮演着过滤海量信息和匹配用户兴趣的基本角色[]。每天，用户浏览新闻平台推荐的新闻、观看电影平台推荐的电影、倾听音乐平台推荐的歌曲,以及购买电商平台推荐的商品等。人们在享受推荐系统提供的推荐服务的同时，也期望它能提供令人信服的高质量解释（Explanations），即推荐的原因和依据。对于用户而言,这在一定程度上反映出,推荐模型的可解释性与准确度至少同等重要[2-3]。事实上,对推荐进行解释不仅有助于提高推荐系统的透明性、说服力、有效性、可信性和用户满意度[45],还可以帮助用户做出更好的决策,并说服他们尝试、订阅或购买所推荐的物品(Item)[5-7]。要想为用户提供量身定制的有说服力的高质量解释,推荐模型必须充分考虑应用场景中用户的历史行为、兴趣内容、偏好倾向和环境信息(如时间、位置)等特征[7-8]。这无疑是一个非常复杂而富有挑战性的过程,近年来正逐渐引起政府、学术界和产业界的广泛关注[9-13]。",
    "context2": "为实现可解释推荐(Explainable Recommendation）,现有研究主要考虑在原有推荐流程中的两个阶段进行：在模型训练过程中生成解释(事先解释)和在给出推荐结果后再生成解释(事后解释)。根据生成解释的方式不同,可解释推荐模型中的可解释性方法大致分为两类:嵌入式解释方法(Embedded)和事后处理解释方法(Post-Hoc)[9,14]。本文在谷歌学术和中国知网中分别以关键词“explainablerecommendation”“interpretablerecommendation”“explainableAI\"“可解释推荐”,并在不限定时间范围的情况下进行文献检索，然后阅读标题和摘要以获取文献研究主题,筛选得到中文文献20篇、英文文献81篇。通过精读并利用追溯法，最终对61篇有代表性的重点文献进行系统梳理,其中英文文献 57篇、中文文献4篇。通过文献调研,同时考虑到虽然可解释性研究获得大量研究人员和从业者的关注，但对于用户真正需要什么样的解释仍缺少足够的研究,本文仅从嵌入式和事后处理两类解释角度出发，对现有研究中有代表性的可解释性方法进行系统梳理,分析其中的研究成果和存在的问题,并在此基础上提出该领域需要进一步研究的科学问题,为拓展可解释推荐的深度和广度研究提供重要的理论指导和实践参考。"
  },
  "2嵌入式解释方法": {
    "context1": "嵌入式解释方法的核心思想是,将解释模块融入推荐模型的构建中,从而使模型在推荐物品的同时还能提供解释[14-5],如图1所示。其中，解释模块用于从物品的辅助信息(SideInformation)中选择特征，并将对推荐准确性贡献最大的部分挑选出来作为解释。通常情况下，用作解释的物品特征往往是一些词组(如\"屏幕清晰\"）、语句(如\"这本书自2018年开始在全世界销量5亿本，是历史上销量最高的一本书\")或图片。该方法将推荐过程和解释过程耦合在一起并融为一体，最后集成为一个模型进行训练，因此被认为是一种模型相关(Model-Intrinsic、Model-Based)的方法。",
    "context2": "现有研究中基于知识图谱(Knowledge Graph）、深度学习（Deep Learning）、注意力机制（Attention",
    "context3": "Mechanism）和多任务学习（Multi-TaskLearning）的解释方法采用嵌入式这一理念。其中，知识图谱方法通过构建实体关系知识图谱，利用知识图谱中的实体属性和关系作为推荐解释。深度学习方法通常结合注意力机制,可以自动学习特征的内在关联,并通过可视化注意力分布产生解释。注意力机制方法本身并不是一种完整的推荐方法，但它可以融入各种推荐模型中，通过自动关注输入的不同方面实现解释的嵌入。多任务学习方法同时优化多个目标，一个任务用于推荐,另一个任务用于生成解释,通过共享特征空间实现解释的嵌入。",
    "context4": "![](images/182653363f321d220c1be0b574a11f2c4f6b28fa6cc59a1b3522ed6ea1dfa7a9.jpg)  \n图1嵌入式解释方法  \nFig.1Embedded Explainability Method",
    "context5": "总体来说,在嵌入式解释方法中，虽然生成的解释具有多样性且与推荐模型密切相关，但要确保这些解释的可读性和一致性是困难的，例如，很难确保所选文本中的情感与评分一致。此外,对于不同的推荐模型,需要设计不同的解释方案。"
  },
  "2.1基于知识图谱的嵌入式解释": {
    "context1": "基于知识图谱解释的思想是，通过在知识图谱中搜索用户和物品之间的连接信息(或关联路径)来提供解释[16-18]。具体而言,利用知识图谱中的路径推理技术，通过强化学习或深度神经网络探索用户与候选物品之间的关联路径实现可解释的推荐,其中，关联路径作为向用户推荐候选物品的解释。从技术上来讲,应用知识图谱进行解释主要从两个方面开展。"
  },
  "(1)基于知识图谱路径推理": {
    "context1": "基于知识图谱路径推理的思路是，寻求元路径(Meta-Paths,定义为一系列实体类型的序列，如用户-电影-导演-电影，即各种实体之间的高阶关系组合)作为用户和物品之间的可能解释。Wang等[19]认为通过探索知识图谱中的相互链接，可以发现用户和物品之间的连接路径,这些路径为用户-物品交互提供了丰富且互补的信息。这种链接不仅揭示了实体和关系的语义，还有助于理解用户的兴趣。鉴于此，作者提出一种知识感知路径循环网络模型(Knowledge-aware Path Recurrent Network,KPRN）,通过组合实体和关系的语义生成路径表示,利用路径内的序列依赖性对路径进行有效推理,推断用户-物品交互的潜在原因。此外,作者还设计了一种新的加权池化操作区分不同路径在连接用户和物品方面的强度，从而赋予模型一定程度的可解释性。总之,该研究通过建模知识图谱中的路径,发掘路径的语义特征并区分路径之间的贡献差异产生可解释的推荐结果,不足之处在于： $\\textcircled{1}$ 知识图谱中的推理路径对用户来说并不直观; $\\textcircled{2}$ 没有考虑解释与推理路径的长度是否相关; $\\textcircled{3}$ 知识图谱可能涉及冗余实体，会导致推荐结果同质化，从而使得解释推荐结果变得更加困难。Xian等[20]提出策略引导路径推理的方法（Policy-Guided Path Reasoning,PGPR）,通过提供知识图谱中的实际路径将推荐与可解释性结合起来。PGPR的主要思想是训练一个强化学习代理,该代理学习在知识图谱环境中从起始用户导航到潜在的“好\"物品，然后利用该代理为每个用户高效地采样推理路径,得到推荐物品，这些被采样的路径作为推荐物品的解释。然而，该方法也存在一些不足： $\\textcircled{1}$ 软奖励策略、用户条件动作剪枝等设计还需进一步优化； $\\textcircled{2}$ 与用户交互生成解释的机制还不完善； $\\textcircled{3}$ 简单地将底层知识图谱视为静态的，忽略了现实世界推荐场景中用户-物品交互的动态性和演化性。"
  },
  "(2)基于知识图谱嵌入": {
    "context1": "基于知识图谱嵌人（KnowledgeGraphEmbedding,KGE)的思路是,利用知识图谱嵌入[21]算法(如翻译距离TransE[22]、语义匹配模型等)将知识图谱中的实体和关系映射到连续的低维稠密向量，然后在嵌入空间中学习各实体之间的高阶连接性（High-OrderConnectivities）,发现重要的路径关系,并根据捕获到的高阶连接路径,选择那些重要程度较高的路径作为最终的解释。Wang等[23]认为传统方法假设每次用户-物品交互都是独立的,忽略了实例或物品之间可能存在的关系（例如,一部电影的导演也是另一部电影的演员）,导致无法从用户的集体行为中提取协同信号。鉴于此,作者提出一种知识图谱注意力网络模型(Knowledge GraphAttentionNetwork,KGAT),以端到端的方式显式建模知识图谱中的高阶连接性,打破了用户-物品之间独立的假设。该模型以递归的方式传播节点的邻居(可以是用户、物品或属性)的嵌入来优化节点的嵌入,并采用注意力机制区分邻居的重要性。基于此,KGAT可以捕获行为和属性上的高阶连接性，并从知识图谱中检索出解释推荐结果的关键路径,为每个推荐提供证据链条。通过对知识图谱的高阶连接性建模,KGAT可以产生对推荐结果具有解释性的推理过程。然而，该模型对相关知识图谱的质量较为敏感,没有考虑过滤掉信息较少的实体，也没有探讨将信息传播和决策过程相结合。"
  },
  "2.2基于深度学习的嵌入式解释": {
    "context1": "基于深度学习解释的思想是，利用卷积神经网络（Convolutional Neural Networks,CNN）、循环神经网络（Recurrent Neural Network,RNN)等深度学习技术和表示学习（RepresentationLearning)技术，从输入数据中深层次地学习和表示用户与物品的非线性特征,获得用户与物品的本质特征,如重要的词、属性或评论等,然后将这些特征合并到推荐模型中，增强解释的能力[9]。这样通过深度学习获得的用户和物品的关键特征，可以为推荐结果提供更具解释性的证据。",
    "context2": "Huang等[24]认为从交互序列中捕获细粒度的用户偏好较为困难，如用户具体喜欢某个物品的哪个属性，是价格、质量,还是性价比。此外,潜在的向量表示通常较难理解和解释。鉴于此,作者提出一种基于知识增强的序列推荐模型（Knowledge-enhanced Sequential Recommender,KSR）,将基于RNN的网络与键值对记忆网络(Key-Value MemoryNetworks,KV-MN)相结合,并进一步融入知识库信息以增强KV-MN的语义表示,使模型具有很强的可解释性(体现在对于各属性值的预测)。其中,RNN模型用于捕获序列用户偏好，基于知识增强的键值对记忆网络用于捕获属性层面的用户偏好。序列用户偏好表示和属性层面偏好表示组合为用户偏好的最终表示。该方法也存在一些不足： $\\textcircled{1}$ 只考虑结构化数据作为知识来源,未利用非结构化文本等数据提供的背景知识； $\\textcircled{2}$ 生成的解释局限于属性层面的推理,未提供对序列依赖性的深层次解释; $\\textcircled{3}$ 物品与知识库实体之间的关联是人工构建的，可能存在误匹配问题。Li等[25]认为如果不检查评论细节,仍然很难理解用户根据什么观点和程度来确定他们是否喜欢或不喜欢物品的某个方面(Aspect)。鉴于此，作者提出一种基于胶囊网络(Capsule Network)的用户评论评分预测模型CARP(CApsule network basedmodel for Rating Prediction with user reviews）。CARP模型旨在从用户评论中提取信息丰富的逻辑单元(LogicUnit,用户持有的观点和物品的某一方面构成一个逻辑单元),并推断其相应的情感。对于每个用户-物品对,CARP模型将信息量大且情感明确的逻辑单元作为用户对物品的偏好解释。该模型的不足之处在于： $\\textcircled{1}$ 逻辑单元的提取依赖于自然语言处理技术，可能存在错误; $\\textcircled{2}$ 情感推理也可能存在错误,影响解释的正确性; $\\textcircled{3}$ 可能无法处理某些类型的评论，如含有讽刺或幽默元素的评论，因而无法从中自动推断出可解释的方面。Chen等[26认为大多数现有可解释的推荐系统模型将用户偏好视为不变的,生成静态解释。然而,在现实场景中，用户的偏好总是动态变化的，可能对不同的产品特性感兴趣。解释和用户偏好之间的不匹配可能降低用户对推荐系统的满意度、信心和信任。鉴于此,作者构建了一种动态可解释推荐系统（Dynamic ExplainableRecommender,DER），用于更准确的用户建模和解释。具体来说,作者设计了一个时间感知门控循环单元(GateRecurrentUnit,GRU)建模用户动态偏好，并利用评论信息，采用语句级卷积神经网络对物品进行特征分析。根据用户当前状态适配性地学习重要的评论信息，不仅能够改进推荐性能，还能为用户当前偏好生成定制的解释。然而,在建模用户动态偏好时，并未考虑随机过程,因此无法解释在不同时间点上推荐或不推荐某些物品的原因。"
  },
  "2.3基于注意力机制的嵌入式解释": {
    "context1": "基于注意力机制解释的思想是,通过动态分配注意力得分，自适应地识别输入数据中与候选物品密切相关的潜在特征，然后通过高权重的特征增强推荐模型的可解释性[27-28]。Seo等[29]认为评论信息对于理解用户偏好和物品特性，以及增强网站的个性化推荐能力非常有用。考虑到卷积神经网络在提取复杂特征方面的优势,作者使用具有双重局部和全局注意力机制的卷积神经网络对用户偏好和物品属性进行建模。其中,局部注意力机制用于提供对用户偏好或物品属性的洞察(有意义的词语),全局注意力机制帮助卷积神经网络关注评论文本的语义含义。因此,组合的局部和全局注意力机制使用户和物品的可解释性、更优的学习表示成为可能。该模型也存在一些不足： $\\textcircled{1}$ 未结合长短期记忆网络（Long Short-Term Memory,LSTM)与注意力网络，以处理评论文本中的长程依赖关系; $\\textcircled{2}$ 对全局语义理解不够，无法产生考虑全局语句关系的解释。Chen等[30]认为评论信息对用户的网购决策起主导作用。然而,评论的质量参差不齐，质量较低的评论会损害模型的性能,而且对用户的参考意义也不大。鉴于此,作者提出一种新的注意力机制探索评论的质量,并提出一个具有评论级解释的神经注意力回归模型（Neural Attentional Regression model withReview-level Explanations,NARRE)用于推荐。该模型不仅可以预测准确的评分,还可以同时学习每个评论的质量。通过这种方式,获得的高质量评论作为评论级别的解释，可以帮助用户更好更快地做出决策。该模型在设计注意力网络时未从特征层面(Feature-Level)进行考虑。Yu等[31提出一个神经注意力可解释推荐系统（Neural Attentive InterpretableRecommendation System,NAIRS）。其中,自注意力网络(Self-Attention)根据与用户偏好相关的意图重要性,计算用户画像中历史物品的注意力权重。利用学习到的注意力权重,NAIRS根据用户的历史偏好为用户提供高质量的个性化推荐,并通过可视化学习到的注意力权重解释推荐的原因。该方法未考虑采用协同注意力机制学习用户的表示，也没有考虑计算针对特定物品的注意力得分。Guo等[32]认为如何通过评论中的特定方面有效地学习用户偏好和物品特征的潜在表示，以及如何建模它们之间的相互作用,是可解释推荐中的两个关键问题。鉴于此，作者提出一种基于三重注意力和时间卷积网络的可解释性推荐方法（Triple-AttentionalExplainableRecommendationwithTemporalConvolutionalNetwork,TAERT),旨在联合生成推荐结果和解释。具体来说,首先探索基于时间卷积网络(Temporal",
    "context2": "ConvolutionalNetwork,TCN)的特征学习方法推导出词感知(Word-Aware)和评论感知(Review-Aware)的向量表示;然后，引入三级注意力网络分别建模词贡献、评论有用性和潜在因素的重要性;最后,通过基于因子级的注意力预测层推断预测评分。此外，注意力机制也有助于识别有代表性的物品评论并突出重要词汇以生成解释。然而，该方法未探索从物品评论中生成摘要级(Summary-Level)解释。"
  },
  "2.4基于多任务学习的嵌入式解释": {
    "context1": "基于多任务学习解释的思想是,利用不同任务中所有可用数据学习对多个上下文有用的泛化表示，然后利用这些表示进行评分预测和解释生成[33]。Wang等[34开发了一种用于可解释性推荐的多任务学习解决方案（Multi-Task Explainable Recommendation,MTER),将用户偏好建模(用于推荐)和情感内容建模(用于解释)这两种相关的学习任务通过联合张量分解进行集成,其中共享用户、物品、特征和观点向量。该算法不仅可以预测用户对一组物品的偏好（即推荐）,还可以预测用户在特征级别如何评价特定的物品(即情感文本解释)。该研究的不足之处在于： $\\textcircled{1}$ 仅依赖于用户明确表述的偏好和物品属性，无法发现用户隐性的兴趣诉求； $\\textcircled{2}$ 未探索用户之间的社交网络结构和物品之间的产品分类。Lu等[33]提出一种多任务推荐模型,通过结合矩阵分解(用于评分预测)和对抗性序列到序列学习(用于解释生成)来联合学习执行评分预测和推荐解释。对抗性序列到序列学习技术的灵感来自生成对抗网络（Generative Adversarial Networks,GANs)结构[35],其中生成器生成评论，鉴别器则判断生成的评论是否真实。总之,该研究用生成评论的方式解决推荐算法的解释合理性难题，但未对生成解释的自然性和说服力进行定性评估，也未进行在线用户研究和A/B测试以验证该模型在提供解释时的有效性。Chen等[36提出一种基于互注意力机制的多任务学习模型（Co-AttentiveMulti-taskLearning,CAML)用于可解释推荐，通过充分利用推荐任务和解释任务之间的相关性提高推荐的预测准确性和可解释性。作者设计了一种受认知心理学中人类信息处理模型启发的编码器-选择器-解码器架构，提出一种层次化互注意力选择器有效地建模这两个任务之间传递的交叉知识。该模型不仅提高了推荐任务的预测准确性，还生成了流畅、实用且高度个性化的语言解释。然而,作者没有对生成的解释提供具体的定量或定性评估结果，也没有探讨用户对于解释的接受程度可能会因为解释的个性化程度不足或过于突出而有所差异。"
  },
  "3事后处理解释方法": {
    "context1": "事后处理解释方法的核心思想是，在物品被推荐之后生成解释,即在模型给出推荐结果后,再通过独立的解释模块选择或生成解释[37-39],如图2所示。其中,解释模块或从预定义的候选语句集中选择某一语句作为解释,如\"其他人还购买了\"和\"您的7个朋友喜欢这个\"等语句,或基于用户与推荐物品之间的某种内在联系生成解释。该方法将推荐过程和解释过程完全解耦,不仅使其工程实现更加简单,也使得解释更容易被用户理解和接受，因此被认为是一种模型无关(Model-Agnostic)的方法。",
    "context2": "![](images/1811e0c226c7e9a8139533221b2c3d8a8e15c4ada09b2b211790c246b6fb6793.jpg)  \n图2事后处理解释方法  \nFig.2Post-Hoc Explainability Method",
    "context3": "现有研究中基于预定义模板（Pre-DefinedTemplates）、语句检索（SentencesRetrieval）、自然语言生成（NaturalLanguage Generation）、强化学习（ReinforcementLearning）、知识图谱和众包评论的解释方法采用了事后处理这一理念。其中,预定义模板方法通过人工设计固定的解释模板，在推荐后进行匹配选择。语句检索方法在推荐后从候选集中检索匹配的解释语句，比预定义模板更加丰富和适应性强。自然语言生成方法能够自主生成新的语句作为解释，而不是从有限候选中选择，更具创造性。强化学习方法可以通过奖惩机制学习到对用户效果更好的解释语句,实现解释的优化。知识图谱方法在推荐后，从知识图谱中获取解释所需的实体关系知识,具有一定的可解释性。众包评论利用用户生成内容作为候选解释,包含丰富的语义信息。",
    "context4": "总体来说，事后处理解释方法可以避免针对每个推荐模型分别设计解释方案，能够灵活解释任何推荐模型，包括含有深度神经网络的复杂、混合模型，以及未来将要被设计出来的推荐模型,它的局限性在于： $\\textcircled{1}$ 丢失了推荐模型中包含的丰富信息; $\\textcircled{2}$ 无法反映系统的真实推荐原因，即不能保证推荐模型的解释能力，解释的可信度有限； $\\textcircled{3}$ 欠缺多样性，容易让用户觉得枯燥。"
  },
  "3.1基于预定义模板的事后处理解释": {
    "context1": "基于预定义模板解释的思想是,基于事先设定好的语句模板生成语句级的解释[39]。从技术上讲，应用预定义模板进行解释主要从两个方面开展。"
  },
  "(1)基于一般语句": {
    "context1": "基于一般语句(Generic Sentences)解释的思路是,从预定义的候选语句集合中选择某一语句作为解释。例如,电子商务网站上的\"购买了此商品的用户,还购买了\"语句[]、YouTube网站上的\"根据您的观看历史\"语句、Netflix网站上的\"用户过去评价较高的类似电影\"语句，以及社交网站上的\"您的朋友喜欢这个\"等语句[40]。这类简洁直观的语句通常易于理解,具有较高可读性和说服力,但可能无法反映模型的实际推理(没有考虑推荐模型的内在工作机制),而且解释的多样性有限(受限于定义的候选解释数量）。"
  },
  "(2)基于特征语句": {
    "context1": "基于特征语句(Features Sentences)解释的思路是，首先建立一系列预定义解释语句模板,然后用特征词语填充这些模板,使解释个性化[41]。例如,算法根据产品的属性，选择出一些特征词语，然后填充“您可能对[特征]感兴趣,这个产品在这个特征上表现很好\"[42]。Zhang等[42]提出一种显式因子模型（ExplicitFactorModel，EFM）,能够分析各种特征中哪些特征在推荐物品时起关键作用,并使用显式特征构建有效的解释,如\"您可能对这个产品表现良好/不佳的[特征]感兴趣”。该方法没有考虑引入更多的指标(如情感、流行度等),每个方面仅使用单一指标，因此未能捕获特征之间更复杂的相互作用。Chen等[43]将静态规格（Static Specifications）和从产品评论中提取的特征情感结合起来,为用户提供有关推荐结果的全面解释,例如，“这组数字相机在有效像素、重量和价格方面具有更好的价值,但在屏幕尺寸方面价值较差”。其中,静态规格是指产品的客观属性,例如价格、尺寸、重量等;特征情感是指用户对产品的看法,例如好、坏、喜欢、不喜欢等。该方法没有考虑用户可能既是信息搜索者又是贡献者（为电影、音乐撰写评论的人)这一情形，因此无法利用用户的评论推断他们的初始属性偏好，从而在初始阶段生成更相关的解释。Li等[44]认为很少有研究从用户的上下文环境角度提供解释(例如,如果推荐的是酒店,则包括同行、季节和目的地),鉴于此,作者提出一种新的基于监督注意力机制的上下文感知推荐算法（Context-Aware Explanation basedonSupervised Attention for Service Recommendations,CAESAR),将潜在特征与从用户生成的评论中提取的显式上下文特征相匹配,生成上下文感知的特征级解释,如\"这款产品推荐给您,因为它的[特征]适合您当前的[上下文]”。然而,该研究在用户偏好建模中没有考虑更多的负面特征，也没有考虑通过预训练上下文特征的嵌入进一步增强解释性能。"
  },
  "3.2基于语句检索的事后处理解释": {
    "context1": "基于语句检索解释的思想是，从评论中检索具体语句而非整篇评论作为解释[14,2]。Wang等[认为现有的解释方法要么忽略了推荐模型的工作机制,要么是针对特定的推荐模型而设计。此外，现有方法很难确保解释的呈现质量,如一致性。鉴于此,作者设计了一种强化学习框架用于可解释推荐,该框架可以解释任何推荐模型(与模型无关),并且可以根据应用场景灵活控制解释质量。具体来说,作者使用个性化注意力神经网络实例化框架中的解释生成模块,对每个语句建模用户个性化偏好并建模语句间依赖关系。该框架通过强化学习和神经网络技术，实现了对解释生成过程的控制和个性化。然而,该方法也存在一些不足： $\\textcircled{1}$ 局限于采用现有的语句,无法创造新的内容; $\\textcircled{2}$ 强化学习是一种试错方法，可能需要花费大量时间和计算资源。这对于需要快速有效地生成解释的可解释推荐系统来说是一个问题。"
  },
  "3.3基于自然语言生成模型的事后处理解释": {
    "context1": "基于自然语言生成模型解释的思想是,不使用",
    "context2": "解释模板[9.45],从用户生成的内容(如用户评论)中自动生成解释语句。从技术上讲,应用自然语言生成模型进行解释主要从三个方面开展。"
  },
  "(1)基于神经模板": {
    "context1": "基于神经模板(NeuralTemplate)解释的思想是,不仅从数据中学习出模板，且能将某个物品属性填入模板以产生解释。Li等[46认为当前生成语句解释的方法要么局限于预定义的语句模板，导致语句表达能力有限;要么选择自由式语句生成,导致语句质量控制困难。为同时兼顾语句表达力和质量，作者提出一种神经模板(NETE)解释生成框架,通过从数据中学习语句模板并生成关于特定特征的模板来控制语句。作者不仅根据传统的文本质量度量标准评价生成的解释,还根据创新的度量标准评价解释的独特性、匹配特征、特征覆盖率和特征多样性。该方法可以高度可控地生成关于给定用户、商品、情感和特征的解释，但没有考虑修饰特征的形容词以增加生成解释的表现力，也没有探讨包含多个特征的情形。"
  },
  "(2)基于生成式模型": {
    "context1": "基于生成式模型(GenerativeModel)解释的思想是，利用RNN、Transformer等技术逐字生成信息量较高的解释。Costa等[47]认为目前大部分解释方法使用结构化的语句提供推荐的描述,忽略了以评论为导向的写作方式。鉴于此,作者设计了一个字符级(Character-Level)循环神经网络模型，使用LSTM根据评论和评分生成文本评论，这些评分表达了对物品不同因素或方面的意见。其中，生成以评论为导向的解释是解释生成的关键。然而，该模型未考虑根据用户的评分、偏好方面和表达的情感进行定制解释,从而帮助用户更好地了解物品。Li等[48]提出一个基于深度学习的框架(Neural Ratingand Tips,NRT),可以同时预测精确的评分和生成具有良好语言质量的概括性提示（Abstractive Tips,通常是几个词或一句话),模拟用户体验和感受。对于概括性提示生成，作者采用基于序列到序列（Seq2Seq)[49]的门控循环神经网络[50],将用户和物品的潜在表示（Latent Representations）“翻译”为简洁的语句。概括性提示生成与评论内容摘要和可解释主题词提取不同,需要将用户和物品的潜在因素\"翻译\"为流畅的词序列,这是解释生成的关键。由于数据稀疏性，该框架生成的概括性提示可能无法完全准确地捕获用户的体验和感受，或者在表达推荐物品的关键特征方面存在一定的模糊性。Chen等[45]提出一种自动去噪的层次化序列到序列模型（Hierarchical Sequence-to-Sequence,HSS）,不仅可以给出准确的评分预测,还可以生成解释语句。其中,带有特征感知注意力机制的层次化GRU生成个性化的解释语句是解释生成的关键。具体来说，层次化生成模型能够在来自不同用户的多个语句上协作学习,进而生成解释语句。特征感知注意力模型隐式地从评论中选择解释语句进行模型学习，并进一步引入特征注意力模型增强解释的特征级个性化。然而，该模型没有考虑有关物品的具有错误描述的语句。"
  },
  "(3)基于算法与众包相结合": {
    "context1": "基于算法与众包(Crowdsourcing)相结合解释的思想是,将众包数据结合到算法过程中以生成个性化的自然语言解释。其中，众包是指通过互联网和社交媒体等渠道，邀请大量用户(即\"众包\")参与提供信息、意见或反馈。Chang等[51认为算法生成的自然语言解释可能过于简单且缺乏说服力，因为除了物品的内容之外，许多其他因素可能会影响用户是否接受推荐,如用户对系统的信任、社会影响力和过往经验等,而人类可以克服这些限制。鉴于此，作者设计了一种结合众包和计算的方法生成个性化的自然语言解释,对电影的关键主题方面进行建模，要求众包工作者根据在线电影评论中的引用内容编写解释,并根据用户的评分历史个性化地向用户呈现解释，如\"根据您的兴趣，我们推荐您观看这部电影。这部电影是一部动作片,获得了9.0分的评分。您之前看过的电影《钢铁侠》的导演也执导了这部电影。”。具体来说,作者收集每个方面的相关评论引用,然后邀请众包工作者将这些引用合成为解释，基于用户的活动对用户的偏好进行建模,并以个性化的方式呈现解释。该模型也存在一些不足： $\\textcircled{1}$ 人力成本高昂，因为需要人工编写解释; $\\textcircled{2}$ 解释质量可能受到人工编写解释者的影响; $\\textcircled{3}$ 难以扩大规模。"
  },
  "3.4基于强化学习的事后处理解释": {
    "context1": "基于强化学习解释的思想是，使用耦合代理（Couple Agents）,其中一个代理负责生成解释,另一个代理则预测生成的解释是否能满足用户的需求（评估生成的解释的质量)[14]。Wang等[14]提出一个模型无关的强化学习框架，其中耦合代理与环境(Environment)交互,智能体1根据当前状态生成语句解释,智能体2根据智能体1生成的语句解释预测用户对所有物品的评分，如果此评分与推荐模型的预测评分相似则得到奖励(Reward)。同时，如果智能体给出的语句解释满足可读性强、连贯性高、解释精炼等条件，智能体也会得到奖励。通过这两个奖励条件更新两个智能体的策略,不仅可以使其习得解释能力,也保证了事后解释的质量。其中，从物品中提取可解释的组件子集为用户提供个性化的解释是解释生成的关键。该框架的局限性在于： $\\textcircled{1}$ 没有考虑生成解释时可能存在收敛较慢的情形; $\\textcircled{2}$ 没有探讨预先设定的奖励机制是否与实际应用中希望的奖励有直接联系。类似地,McInerney等[52认为不同的用户对解释信息的反应各不相同,且随着用户所处的情况而动态变化。因此，作者提出一种名为Bart（BAndits for explaining recommendations as Treatments ）的基于多臂老虎机(Multi-ArmedBandit)的探索(Exploration,发现新的、未知的行动和策略)与开发(Exploitation,利用已知的高回报策略)平衡框架,为每个用户找到最佳的解释序列。Bart能够根据不同的用户需求提供多样化的解释内容： $\\textcircled{1}$ 基于内容的解释：推荐的物品与用户的兴趣相关,如\"这本书与您之前阅读过的书类似”; $\\textcircled{2}$ 基于评分模型的解释：推荐的物品在用户的评分模型中得分较高，如\"这部电影的评分为4.5星”。简单来说，就是用用户喜欢的评分来说明推荐结果; $\\textcircled{3}$ 基于用户行为的解释：推荐的物品与用户的过去行为一致,如\"您之前浏览过这本书”。其中，探索推荐具有不确定的预测用户参与度的内容以收集更多信息，开发推荐具有最高预测用户参与度的内容(如产品、电影和音乐播放列表)。该框架不仅可以学习到每个用户对于哪些解释信息做出何种反应，也可以学习到对于每个用户来说哪些物品是推荐的最佳物品，以及如何平衡探索与开发以应对用户满意度不确定性。然而，该框架未考虑自动化解释的生成和参数化以实现更加细致的个性化。"
  },
  "3.5基于知识图谱的事后处理解释": {
    "context1": "在事后处理解释方法中，基于知识图谱解释的思想是,通过软匹配算法（SoftMatchingAlgorithm)在知识图谱嵌入空间中找到用户和推荐物品之间的解释路径。Ai等[53]提出在知识图谱嵌入的基础上增强协同过滤方法以实现个性化推荐,并采用软匹配算法在用户和物品之间寻找解释路径。其中,在知识图谱中从用户到物品找到一个合理的逻辑推理序列是解释生成的关键。作者从用户和物品开始进行最大深度的广度优先搜索，找到潜在连接它们的解释路径，然后记住路径并通过软匹配计算路径概率,最后根据概率对解释路径进行排序，并返回最佳路径以生成自然语言解释。很显然，其中的解释组件只是在为已经选择的推荐结果寻找事后解释。这种策略存在的问题是：解释并非根据推理过程生成,而是通过用户和物品嵌入之间的经验相似性匹配后生成的。"
  },
  "4可解释性方法之间的详细比较": {
    "context1": "为实现可解释推荐，现有研究通常采用两种方法：嵌入式解释方法(解释是推荐过程的一部分)和事后处理解释方法(解释是在提供推荐之后才提供)。",
    "context2": "嵌入式解释方法通常具有较好的模型解释性，即推荐模型本身的解释能力较好，但也存在一些不足： $\\textcircled{1}$ 解释内置使得模型的灵活性较差; $\\textcircled{2}$ 受限于建模的困难,无法保证解释的质量及个性化程度,例如不能确保解释的一致性等。",
    "context3": "(1)基于知识图谱的嵌入式解释方法主要存在以下问题：知识获取的质量和规模会直接影响解释的质量;知识图谱的静态特性使得解释同样缺乏动态更新的能力；依赖专家提供的结构化知识。",
    "context4": "(2)基于深度学习的嵌入式解释方法主要存在以下问题：解释往往针对算法而不是针对用户需求;解释结果不具备很强的一致性和稳定性;学习复杂特征的内在关系困难，导致生成的解释可读性较弱。",
    "context5": "(3)基于注意力机制的嵌入式解释方法主要存在以下问题：注意力机制侧重算法角度而非用户角度，生成的解释不一定符合用户认知;注意力分布中的权重确定过程本身缺乏解释;不同任务的注意力分布解释方式可能存在差异，扩展性有限。",
    "context6": "(4)基于多任务学习的嵌入式解释方法主要存在以下问题：需要同时优化多个目标，目标之间的权重面临着精确调节的困难;当任务之间关系不密切时，强行多任务学习可能获得反效果;多任务学习中的推荐任务和解释任务目标不一致可能导致解释质量不高。",
    "context7": "事后处理解释方法可以避免针对每个推荐模型分别设计解释方案,即可以灵活解释任何推荐模型,包括含有深度神经网络的复杂、混合模型,以及未来将被设计出来的推荐模型。然而,它也存在一些不足： $\\textcircled{1}$ 丢失了推荐模型中所包含的丰富信息； $\\textcircled{2}$ 无法反映系统的真实推荐原因,存在解释准确性问题,即解释的可信度有限; $\\textcircled{3}$ 欠缺多样性,容易让用户觉得枯燥。",
    "context8": "(1)基于预定义模板的事后处理解释方法主要存在以下问题：预定义模板数量有限，生成的解释多样性不足；缺乏个性化，不能适应不同用户的特别需求；预定义模板更新困难，无法及时适应用户喜好和环境的变化。",
    "context9": "(2)基于语句检索的事后处理解释方法主要存在以下问题：候选集规模受限,生成解释的多样性不足;对用户历史行为模式利用不足,生成的解释连贯性较差;难以深入解释推荐系统的内部机制和判断逻辑;缺乏对语义信息的利用,生成的解释不够丰富。",
    "context10": "(3)基于自然语言生成模型的事后处理解释方法主要存在以下问题：需要大规模标注数据进行训练，应用中数据集较难获取;需要频繁人工评估生成解释的质量;生成长文本解释的效果较弱;模型扩展和迁移到新的场景困难较大。",
    "context11": "(4)基于强化学习的事后处理解释方法主要存在以下问题:需要设定合理的环境、状态、动作、奖励函数等，在解释生成中设计难度大;需要频繁地在线学习,实时更新解释策略困难;容易陷入局部最优解，无法获得全局最优的解释策略。",
    "context12": "(5)基于知识图谱的事后处理解释方法主要存在以下问题：侧重显性推荐的解释,对隐性推荐解释支持不足;依赖领域知识，不同领域需要构建不同的知识图谱;知识图谱的质量直接影响解释效果,构建高质量图谱存在困难;获取动态用户兴趣和环境变化的解释能力较弱。",
    "context13": "从逻辑思路、性能特点和局限性三个方面对文中所述可解释性方法进行详细比较,如表1所示。",
    "context14": "表1可解释性方法详细比较  \nTable 1Detailed Comparison of Explainability Methods",
    "context15": "<table><tr><td>可解释性方法</td><td>逻辑思路</td><td>性能特点</td><td>局限性</td></tr><tr><td>的嵌入式解释 接信息(或关联路径)提供解释。</td><td>基于知识图谱 通过在知识图谱中搜索用户和物品之间的连 2.打通物品、用户和特征这三类 利用卷积神经网络、循环神经网络等深度学</td><td>1.利用知识图谱的可推理特性， 增加推荐结果的可解释性; 媒介之间的关联,根据具体情况 灵活选择其中最合适的媒介对用 户进行推荐与解释。</td><td>1.难以根据用户反馈实时更新； 2.难以提前过滤脏数据； 3.对知识图谱的质量较为敏感; 4.无法应用用户-物品交互的动态性和演 化性。</td></tr><tr><td>基于深度学习 的嵌入式解释</td><td>习技术和表示学习技术,从输入数据中深层 次学习和表示用户与物品的非线性特征,从 而获得用户与物品的本质特征,然后将这些 特征合并到推荐模型中以增强解释性能。</td><td>解释模型; 人可解释性模块来实现。</td><td>1.深度模型的黑盒性质导致难以1.模型结构复杂,算法透明性低,因而模型 本身对输出结果不能赋予很好的可解释性; 2.自身可解释性只能通过额外引2.很难通过模型内部的数据交互以及处理 逻辑给出比较直观易懂的解释。</td></tr><tr><td>释</td><td>制的嵌入式解 数据中与候选物品密切相关的潜在特征,然后 型自身可解释性较差的问题； 通过高权重的特征增强推荐模型的可解释性。2.解释性主要来自注意力权重分配。相似性的准确性等方面仍缺乏深入考虑。</td><td>1.在多个任务之间共享知识和学</td><td>基于注意力机通过分配注意力得分，自适应地动态识别输人1.可以有效改善深度神经网络模 在交互物品之间的不同重要性、用户和物品的 潜在表示的时间依赖性、生成解释时所基于的</td></tr><tr><td>释</td><td>基于多任务学使用不同任务中的所有可用数据学习对多个 习的嵌入式解上下文有用的泛化表示,然后进行评分预测 和解释生成。</td><td>习到的表示,提高模型的泛化 能力； 2.促进任务之间的知识传递。</td><td>1.可能存在任务间的相互干扰; 2.多任务之间的相关性或共享特征的假设 并不总是成立。</td></tr></table>",
    "context16": "(续表)",
    "context17": "<table><tr><td>可解释性方法</td><td>逻辑思路</td><td>性能特点</td><td>局限性</td></tr><tr><td>基于预定义模 板的事后处理 解释</td><td>基于事先定义好的语句模板提供语句级的解1.通常具有说服力和高可读性； 释。</td><td>2.比较容易部署。</td><td>1.需要手动定义语句模板,模板的创建成 本很高； 2.展示给用户的理由过于单一，难以有效 提升用户对系统的信任度进而促进购买; 3.推荐模型的工作机制常被忽略，解释的 多样性往往受到预定义候选数量的限制。</td></tr><tr><td>基于语句检索 的事后处理解检索语句而不是整个评论作为解释。 释</td><td></td><td>提高了解释语句的表达多样性。</td><td>1.给出的解释仅限于现有的文本语句,模 型无法产生新的语句进行解释; 2.挑选出来的文本语句很难做到高度个性 化； 3.存在版权问题，如将某些用户的文本语 句用于描述其他商家的产品可能带来法律 风险。</td></tr><tr><td>基于自然语言 生成模型的事 后处理解释</td><td>从用户生成的内容(如用户评论)中自动生成 解释语句，不使用解释模板。</td><td>1.可以很容易地融入不同的应用 程序场景中; 2.生成的语句是全新的,具有高 度个性化,不需要人为设计模板够吸引人。 且不存在法律风险。</td><td>1.文本生成过程中可能存在噪声； 2.生成的语句可能存在语法错误,可能不</td></tr><tr><td>释</td><td>基于强化学习使用耦合代理,其中一个代理负责生成解释，能够根据环境的反馈和奖励信号 的事后处理解 另一个代理则预测生成的解释是否足够满足调整策略，以获得更好的长期回 用户的需求。</td><td>报。</td><td>1.选择适当的探索策略和开发策略是一个 复杂的问题; 2.需要大量的交互和试错来学习适当的策 略。</td></tr><tr><td>基于知识图谱 的事后处理解 释</td><td>通过软匹配算法在知识图谱嵌入空间中找到1.有较好的灵活性; 用户和推荐物品之间的解释路径。</td><td>2.可以融合多种关系类型。</td><td>解释并非根据推理过程生成，而是通过用 户和物品嵌入之间的经验相似性匹配后生 成的。</td></tr></table>"
  },
  "5结语": {
    "context1": "推荐系统被广泛部署在网络世界的每个角落,如电子商务平台、社交网络、视频分享平台和流媒体服务，大大提高了用户的决策效率。对给出的推荐结果进行解释不仅可以揭示推荐系统背后的推理过程和数据,还可以反过来为推荐系统的调试、优化改进提供方向。从商业角度来看,解释也被证明是有益的,可以增加用户对系统的信任,帮助用户更快地做出决定,并说服用户订阅或购买物品等。鉴于此,本文对现有代表性的可解释性方法和思路进行系统分析和探讨,这为后续研究在推荐结果解释方面取得突破性进展,真正达到\"知其然,知其所以然\"的目标奠定了基础。展望未来,随着互联网内容生态圈日益发展，个性化推荐应用场景愈发多样复杂，用户对推荐系统的可解释性需求也越来越大,如人类可读、高质量、实时、内容直观丰富和个性化的语义解释，因此本文认为在对话交互式解释（Conversational InteractiveExplanation）、因果解释(Causal Explanations)等研究中仍存在一些开放性问题值得探讨。",
    "context2": "(1)对话交互式解释",
    "context3": "对话交互式解释与传统的单向解释不同，它强调解释过程中的双向交流和反馈。对话交互式解释指通过对话的形式与用户进行交互，提供个性化、实时的解释。在系统与用户的交互过程中，用户在接收到系统提供的初始解释后，应当拥有调整及反馈的渠道,以避免一次性的解释,进而深化对系统的理解或感知[54]。尽管对话式推荐系统和交互式推荐系统已经得到广泛研究[55-56],但将交互性特征融入可解释性推荐系统的探索仍显不足，有待进一步深入研究[57]。事实上,在现有文献和实际应用系统中,仅有少数可解释性推荐系统支持交互式解释功能，目的是让用户检视推荐结果并纠正系统的推理过程中"
  }
}