{
  "original_filename": "full_4475.md",
  "基于大语言模型的问答技术研究进展综述": {
    "context1": "文森,²钱力1,2,3 胡懋地1,2 常志军1,2",
    "context2": "1（中国科学院文献情报中心北京 100190)  \n2（中国科学院大学经济与管理学院信息资源管理系北京100190)  \n3（国家新闻出版署学术期刊新型出版与知识服务重点实验室 北京 100190)",
    "context3": "摘要：【目的】全面回顾和概述基于大语言模型的问答技术发展现状、机制原理以及应用趋势。【文献范围】选取与基于大语言模型的问答技术相关的73篇文献。【方法】系统梳理大语言模型的发展现状、参数高效微调策略,分别从面向简单问题的检索增强生成问答推理以及面向复杂问题的提示工程问题推理两方面,深入解析各技术的原理机制、应用价值与存在问题。通过定性分析,全面概述基于大语言模型的问答技术研究进展,并提出未来研究方向。【结果】开源预训练大语言模型不断涌现,高效微调策略可显著提升模型垂直领域适配性。借助文本嵌入与近似最近邻检索技术,检索增强生成技术可有效提升问答可解释性与可信度。借助精心构造的提示工程,可大幅拓展大语言模型的复杂问题推理能力。【局限】大语言模型相关研究发展迅速,调研工作未全面覆盖。【结论】基于大语言模型的问答技术在语义表示、复杂推理等多个方面均取得显著进展,融合外部知识的检索增强生成技术与提示工程技术是当前大语言模型领域的主要研究热点,未来研究工作可在生成内容可控、可信等方面展开深入探索。",
    "context4": "关键词：大语言模型问答技术向量检索提示工程分类号：TP391G350DOI: 10.11925/infotech.2096-3467.2023.0839",
    "context5": "引用本文：文森，钱力，胡懋地等.基于大语言模型的问答技术研究进展综述[J].数据分析与知识发现, 2024，8(6): 16-29.(Wen Sen, Qian Li, Hu Maodi,et al. Review of Research Progress on Question-Answering Techniques Based on Large Language Models[J]. Data Analysis and Knowledge Discovery,2024,8(6): 16-29.)"
  },
  "1引言": {
    "context1": "信息技术的快速发展促进了第四范式下大数据时代的到来[1]。面对海量数据信息,如何更准确、快速地获取到所需知识却成为愈发严峻的挑战[2]。尽管现有的搜索引擎与数据库可以提供一定的帮助，但它们大多基于关键词匹配搜索,对于复杂查询或需要深入理解语义内容的问题往往效果有限。且现有的语义检索也主要停留在匹配层面，并不能高效实现语义理解和问答总结。这些限制进一步导致了知识获取效率的下降和知识利用效率不足。",
    "context2": "面对信息检索的挑战，自然语言处理技术的发展为智能问答带来了新的可能性。大语言模型(Large Language Model,LLM)可以更高效、准确地理解人类提出的复杂语义问题,为用户进行智能问答、文本生成提供技术支撑。但由于其自身解码器技术架构,生成结果存在着一定的幻觉和误差。为了减少回答幻觉现象，提供回答参考依据,研究人员更加关注通过融合外部知识提升模型回答的准确性与真实性，为生成式大语言模型提供检索增强的事实引导。因此,本文旨在通过对相关文献与开源项目进行调研，系统总结基于大语言模型的问答技术研究进展,从研究现状、基于检索增强的问答推理以及基于提示工程的问答推理三方面深入阐述基于大语言模型的问答技术相关原理机制与进展情况，并总结不足,提出未来展望,以期为该领域的相关研究提供参考和借鉴。"
  },
  "2现状研究": "",
  "2.1问答系统研究进展梳理": {
    "context1": "问答系统研究起步于20世纪50年代的图灵测试，经历了基于结构化数据库的专用问答系统、基于大规模文档集的通用问答系统、基于问题答案对的问答系统、基于知识图谱的问答系统以及最新的基于大语言模型的问答系统5个发展阶段。",
    "context2": "20世纪60\\~70年代，伴随结构化数据库技术的发展，专用于回答特定领域内事实型问题的专用问答系统应运而生，典型问答系统有Baseball3和Lunar[4]等。它们通常采用人工设计的规则模板，将自然语言问题转换为数据库查询语句，进而通过查询语句获取到查询结果。这类问答系统覆盖范围有限，但实现了计算机利用自然语言界面回答特定领域问题的初步突破。",
    "context3": "20世纪90年代，随着互联网的蓬勃发展，大量非结构化文本文档涌现,如电子邮件、网页等，处理开放领域下各类自然语言问题的通用问答系统也随之出现。该阶段的关键技术主要包括问句分类[5]、关键词提取[6]、文档检索[7]、候选答案筛选等,代表性问答系统有A.L.I.C.E.聊天机器人[8],已开始具备一定的开放域问题解答能力。",
    "context4": "21世纪以来，各类社交媒体平台上积累了大规模的问答数据，基于这些先验问答对的问答系统得到快速发展，可以通过匹配相似问句或直接生成回复解决新问题。关键技术包括文本表示学习[9、相似度计算[10]、回复生成等，主要类型包括FAQ(FrequentlyAskedQuestions,常见问题)问答系统和社区问答系统。这类系统扩大了问答范围,并开始显现一定的对话交互能力。",
    "context5": "近10年来,知识图谱作为结构化知识表达方式受到广泛关注。相应的知识图谱问答系统通过解析问题并在知识图谱中搜索答案,可以利用丰富的实体关系进行多跳推理[]。主要技术包括实体链接[12]、关系提取[13]、语义解析[14]等，代表系统有GoogleKnowledgeGraph[15]等。知识图谱为问答质量与覆盖范围的提升提供了有力支持。",
    "context6": "当前,大语言模型技术取得突破性进展,可以端到端地分析文本问题并产生自然流畅的答复，这标志着问答系统向真正智能化方向发展。综上所述，问答系统通过不断突破关键技术瓶颈，从专用系统到通用系统,从简单事实问答到复杂推理问答,其处理能力、覆盖范围与交互智能性得以不断提升，正朝着模拟人类问答能力的终极目标持续演进。"
  },
  "2.2大语言模型相关概念界定": {
    "context1": "近年来,深度学习技术、自然语言处理技术的发展为智能问答带来了新的可能性。2022年底，OpenAI公司推出了一款专注于对话问答生成的大语言模型ChatGPT,标志着问答系统进入一个全新时代[16]。基于大语言模型的问答系统可以更高效、准确地理解人类提出的复杂语义问题,并且支持多源异构知识表达和多轮语义交互，可以实现更智能化的问答体验。",
    "context2": "目前大语言模型并没有公认统一的概念定义。Carlini等指出大语言模型由具有大量参数（通常为数十亿以上)的神经网络组成[17],并使用自监督或半监督学习在大量未标记文本上进行训练,具备通用能力，可以执行广泛的自然语言处理任务，包括文本摘要、翻译、情感分析等。Zhao等指出,大语言模型指的是在海量文本语料上训练,包含至少数十亿级别参数的语言模型,如GPT-3、PaLM、LLaMA等[18]。",
    "context3": "尽管许多研究人员主要依据模型参数量与所利用训练数据规模界定与评估何为大语言模型，但迄今尚未确立一个被广泛认可的临界标准，用以明确定义何时方可将某一语言模型称为\"大语言模型”。综合考量,大语言模型的范畴受到多重因素所制约，主要涵盖模型参数规模、训练数据的广度与多样性，以及模型在实际硬件平台上运算所需计算能力。",
    "context4": "然而，大语言模型与基于大语言模型的问答技术并非毫无二致。Hoffmann等指出基于大语言模型的问答技术是一种利用大规模预训练语言模型回答自然语言问题的方法，该方法的核心思想是先用海量文本数据预训练出一个大语言模型，然后在海量问答数据集上对该模型进行微调，使其适应下游问答任务，进而提供问答服务[19]。",
    "context5": "大语言模型由纷繁复杂、类型多样的数据训练得到，天然适用于各类自然语言处理任务，而非仅适用于问答任务[20]。同时，原始训练出来的大语言模型缺乏对海量真实的人类问答数据集的学习训练，并非完全适用于人类的问答场景,往往需要进一步微调优化,方可作为问答系统投入使用,这也是目前各类大语言模型基本都配置了base基础版本与chat",
    "context6": "问答版本的原因。"
  },
  "2.3主流开源大语言模型梳理": {
    "context1": "随着GPT-3的问世，大语言模型研究掀起了热潮。由于大语言模型对于庞大的计算能力和数据资源的高度依赖，主要究力量主要来自工业界。但随着Meta公司、清华大学等发布了不同参数规模的开源大语言模型,学术界也得以在有限的计算资源下积极参与进来。本文通过文献调研与广泛的资料搜集工作对目前主流的开源大语言模型基本情况进行了归纳，如表1所示（按照发布时间升序排列)。",
    "context2": "表1开源大语言模型研究现状  \nTable 1Current Status of Research on Open Source Large Language Models",
    "context3": "<table><tr><td>名称</td><td>发布时间</td><td>发布主体</td><td>参数规模/B</td><td>模型架构</td><td>训练数据</td></tr><tr><td>PaLM[21]</td><td>2022-04</td><td>Google</td><td>8/62/540</td><td>解码器架构，使用 SwiGLU激活函 公开可用文本数据集,包括英语CommonCrawl、 数、并行层技术、多查询注意力机C4、Wikipedia、Books1、Books2TB等,以及多 制、旋转向量嵌入等技术进行优化</td><td>种编程语言的源代码，共计7800亿个标记</td></tr><tr><td>Flan T5[22]</td><td>2022-10</td><td></td><td>Google0.06/0.22/0.77/3/11</td><td>基于T5的自回归语言模型,使用了 指令微调和思维链技术进行优化</td><td>使用了超过1800个不同类型的自然语言处理 任务进行微调,包括问答、文本生成、文本分 类、文本摘要等</td></tr><tr><td>LLaMA[23]</td><td>2023-02</td><td>Meta</td><td>7/13/33/65</td><td>解码器架构,使用预正则化、SwiG- LU激活函数和旋转向量嵌入等技 术进行优化</td><td>Common Crawl、Github、Wikipedia Books、 arXiv、StackExchange等近4.7TB数据</td></tr><tr><td>Falcon[24]</td><td>2023-05</td><td>Technology innovation institute</td><td>1/7/40</td><td>解码器架构,使用旋转位置编码、多 查询注意力、Flash Attention等技术</td><td>公开可用文本数据集,包括英语和欧洲语言的 RefinedWeb、Books Conversations 、Code、 Technical等,共计约2.8TB</td></tr><tr><td></td><td>ChatGLM22023-06清华大学</td><td></td><td>6/12/32/ 66/130</td><td>GLM架构,结合FlashAttention算法、 多查询注意力机制、混合目标函数、人 类偏好对齐训练等技术进行优化 解码器架构,采用了和LLaMA一样</td><td>6B级训练数据为1.4TB 中英标识符的预训 练,中英文比约为1:1</td></tr><tr><td>BaiChuan</td><td>2023-07百川智能</td><td></td><td>7/13</td><td>的模型设计,使用FlashAttention算 法、多查询注意力机制、RMSNorm、 混合目标函数等技术进行优化</td><td>公开可用的中英文数据和自行抓取的中文互 联网数据,以及部分高质量知识性数据,共计 约1.4万亿个标记</td></tr><tr><td>LLaMA2[25]2023-07Meta</td><td></td><td></td><td>7/13/70</td><td>解码器架构，在LLaM技术框架技 术上,采用了Ghost Attention算法，在2万亿个数据标记上训练 以改善模型的多轮对话一致性</td><td></td></tr></table>",
    "context4": "大语言模型领域知名度最高的当属是OpenAI公司开发的GPT系列模型。其中,基于Transformer的解码器架构训练而来的GPT-1是一个单向语言模型,发布于2018年,模型参数规模仅为1.17亿[26]。在此基础上,GPT-2于次年发布,该模型有近15亿个参数[27]。在此期间,基于编码器架构的语言模型、基于解码器架构的语言模型与基于编码器-解码器双向语言模型三分天下。突破性地,Brown等于2020年发布了GPT-3模型,该模型参数达到1750亿规模[28],研究人员在实验过程中发现,随着模型参数量以及训练数据规模量的增加,大语言模型的表现也往往随之提升,甚至出现了“涌现\"能力[29],涌现能力可以理解为参数量达到一定量级后,模型具有了复杂的推理能力。",
    "context5": "在此发展过程中，解码器架构逐渐主导了大语言模型的发展。基于解码器架构的\"大数据 $^ +$ 大参数\"大语言模型出现的涌现能力,将编码器以及编码器-解码器架构的领域任务适配优势拉平甚至有所超越，这也反映出海量文本数据天然蕴含着大量的自然语言处理任务，进行更充分的学习训练后的解码器架构大语言模型往往具备更强、更通用的零样本拟合性能。然而,仅通过增加参数数量和训练数据规模也不能确保模型效果得到提升。以2021年Google推出的SwitchTransformer模型为例,其参数规模高达约1.6万亿，是首个万亿参数级别的大语言模型[30],但其生成效果却远未达到预期水平。相比于参数量巨大但训练不充分、算法不合理、数据质量低的模型,一个参数量适中但结构设计合理并针对特定任务进行了优化的模型,往往能够带来更好的效果。"
  },
  "2.4大语言模型参数高效微调策略研究": {
    "context1": "大语言模型由海量文本数据训练而来,天然适用于各种自然语言处理任务。然而，直接将原始训练而来的大语言模型用于问答任务,效果往往不尽如人意。为了更好地适应特定领域的问答任务，需要进一步在大规模、高质量的聊天对话数据中进行训练,将外部知识内化融合到大语言模型参数中，使其进一步提升问答能力。与传统的语言模型微调不同,微调大语言模型则需要使用更为海量、高质量的数据,以及更高的计算资源方可实现大语言模型微调。Chen等讨论了微调大语言模型可能会导致严重的灾难性遗忘[31],因为具有复杂架构的大语言模型在微调过程中更有可能忘记之前学到的知识，并过度适应目标领域、特定任务。而重新开始训练大语言模型需要大量高性能图形处理器(GraphicsProcessingUnit,GPU),这对于研究人员来说难度极大。",
    "context2": "为了应对上述问题，研究人员开始探索参数高效 微调（Parameter-Efficient Fine-Tuning,PEFT）技术，如表2所示。PEFT技术的目标是通过最小化微调参数和计算复杂度,提高预训练模型在新任务上的性能的同时，也不出现严重的灾难性遗忘问题,以实现在提升模型效果的同时显著缩短模型训练时间和降低计算成本的目标。",
    "context3": "目前，主流的参数高效微调策略技术基本可以分为三大类，分别是Adapter、PrefixTuning以及LoRA类，它们在模型结构中所嵌入的位置也有所不同,具体情况如图1所示。在这些技术中,Adapter类的参数高效微调技术在预训练模型的各层之间插入较小的神经网络模块，这些新增的神经模块被称为“适配器”,在进行下游任务的微调时，只需对适配器参数进行训练便能实现高效微调的目标，在此基础上衍生出了AdapterP[40]、Parallel[41]等高效微调技术;PrefixTuning类参数高效微调技术通过在模型的输入层或隐层添加 $k$ 个额外可训练的前缀标记,模型微调时只训练这些前缀参数便能实现高效微调的目标,在此基础上衍生出了P-Tuning、P-Tuning v2 等高效微调技术;LoRA类参数高效微调技术则通过学习小参数的低秩矩阵近似模型权重矩阵W的参数更新，微调训练时只需优化低秩矩阵参数便能实现高效微调的目标，在此基础上衍生出AdaLoRA、QLoRA等高效微调技术。",
    "context4": "表2参数高效微调技术总结  \nTable 2Summary of Parameter-Eficient Fine-Tuning Technology",
    "context5": "<table><tr><td>名称</td><td>提出时间</td><td>技术简介</td></tr><tr><td>Adapter Tuning[(32]</td><td>2019</td><td>在预训练模型内部的网络层之间添加新的网络层或模块适配下游任务</td></tr><tr><td>Prefix-Tuning[33]</td><td>2021-01</td><td>在模型输入前添加一个连续的且任务特定的向量序列,固定预训练语言模型的所有参数,只更新优化 特定任务的前缀</td></tr><tr><td>P-Tuning[34]</td><td>2021-03</td><td>固定模型参数,利用多层感知机和LSTM对提示词进行编码,编码之后与其他向量进行拼接之后正常 输入模型中</td></tr><tr><td>Prompt Tuning[3 [35]</td><td>2021-04</td><td>固定整个预训练模型参数，只允许将每个下游任务的额外k个可更新的标记前置到输入文本中,也没有 使用额外的编码层或任务特定的输出层</td></tr><tr><td>LoRA(Low-Rank Adaptation）[36]</td><td>2021-06</td><td>冻结预训练模型的权重,并在每个Transformer块中注入可训练层(称为秩分解矩阵),通过学习小参数 的低秩矩阵近似模型权重矩阵W的参数更新，训练时只优化低秩矩阵参数</td></tr><tr><td>P-Tuning v2[37]</td><td>2021-10</td><td>在多层加人了提示词标记作为输入,使得Prompt Tuning能够在不同参数规模的预训练模型、针对不同 下游任务的结果上都达到类似精调的结果</td></tr><tr><td>AdaLoRA[38]</td><td>2023-03</td><td>对LoRA的一种改进，根据重要性评分动态分配参数预算给权重矩阵,AdaLoRA将关键的增量矩阵分 配高秩以捕捉更精细和任务特定的信息,而将较不重要的矩阵的秩降低</td></tr><tr><td>QLoRA[39]</td><td>2023-05</td><td>使用一种新颖的高精度技术将预训练模型量化为4bit,然后添加一小组可学习的低秩适配器权重,这 些权重通过量化权重的反向传播梯度进行微调</td></tr></table>",
    "context6": "![](images/bb8cef5686a3f8685e179cd4bf93f5221c2f4e9bd94671af948ce3d6251f636b.jpg)  \n图1三类参数高效微调策略图析 Fig.1Diagram Analysis of Efficient-Parameter FineTuning Strategies"
  },
  "3基于检索增强生成的问答推理": {
    "context1": "生成式语言模型通常依赖大量的参数存储知识,面对没有训练学习过的问题时，往往会给出虚构、编造的回答，出现\"幻觉”。为解决该问题,提升知识回答的可信度与准确度,检索增强生成技术应运而生。检索增强生成（Retrieval-AugmentedGeneration,RAG)技术[42]由MetaAI于2020年提出,检索方法特指向量检索，检索增强生成技术利用文本嵌入模型和生成模型之间的协同作用，将带有参数记忆的隐性知识Seq2Seq模型与带有非参数记忆的外部知识库结合起来，利用嵌入模型将输入问题向量化,然后结合知识向量数据库以及高效、准确的近似最近邻搜索算法进行搜索，将相关检索结果与用户提问共同输入大语言模型,以期获得更准确的文本生成结果。",
    "context2": "检索增强生成技术通过结合文本嵌人模型与生成模型实现语义匹配检索与自然语言生成的有机融合，提高了问答系统的可解释性与可信度。该技术呈现出两大显著优势：首先,知识无须隐式嵌入模型参数,而是以即插即用的方式显式引入,具备良好的可扩展性;其次,对比从零开始生成文本,基于检索增强的文本生成以检索得到的文档作为参考依据，便于溯源查证。",
    "context3": "为使检索增强生成技术发挥最大效用,选择合适的文本嵌入模型与快速准确的语义检索方法至关重要。以下将重点阐述文本嵌入模型的发展现状、文本嵌入在检索式问答任务中的应用价值,以及近似最近邻搜索等语义检索技术，以更深入地解析检索增强生成所需的语义表示与检索技术基础。这有助于全面理解检索增强生成技术内涵,为优化基于大语言模型的问答系统提供理论支撑。"
  },
  "3.1语义表示技术": {
    "context1": "语义表示是一种广泛的概念，指的是将文本信息转换为更丰富、更有意义的表示形式，以便计算机能够理解文本的含义[43]。文本嵌入技术作为实现语义表示的重要方式,通过将文本投影到向量空间进而测量出文本之间语义距离，是自然语言处理领域最新的成果之一[44]。对于向量检索而言，文本嵌入模型的好坏、合适与否更是直接决定了检索匹配程度的高低，所以选择合适的文本嵌入模型，对于提升问答准确度与可信度至关重要。",
    "context2": "文本嵌入技术的发展可以追溯到20世纪50\\~60年代的计算语言学研究,其中最著名的是Harris在1954年提出的分布式语义理论[45]。该理论认为单词的语义可以通过它们在上下文中的分布来表示，即单词含义可从其周围的词语中推断出来。在计算机科学领域,最早的文本嵌入技术可以追溯到20世纪$8 0 { \\sim } 9 0$ 年代的神经网络研究。其中最著名的是Bengio在2003提出的神经语言模型[46],该模型通过单词的上下文预测下一个单词,用以生成大规模文本的嵌入向量表示。随着深度学习技术不断发展,重要的文本嵌入算法被提出，如Word2Vec、GloVe和FastText等。这些算法通过训练神经网络、进行矩阵分解等方法学习文本的静态嵌入表示，目前这些方法已在各种自然语言处理任务中得到广泛应用。",
    "context3": "近年来，随着自然语言处理技术的进一步发展，尤其是Transformer与注意力机制出现,文本嵌入技术得到了更进一步发展。ELMo(Embeddings fromLanguage Models）、BERT（Bidirectional EncoderRepresentationsfromTransformers）以 及 GPT(Generative Pre-trained Transformer)等语言模型依托于注意力机制生成动态的上下文相关嵌入向量表示,这些模型往往能更有效地捕捉文本的语义信息和上下文特征。在如今的大语言模型时代,文本嵌入技术焕发出新的价值，主要体现在解决大语言模型所面临的标记输入限制方面。长篇文本难以一次性输入语言模型中,因此需要将文本分割成较小的块或片段。然而，这种分段策略可能引发不一致性和连贯性问题,从而进一步降低所生成文本的质量。而文本嵌入技术则可以将单词和短语表示为高维向量,进而以紧凑、高效的方式编码输入文本的上下文信息,语言模型便可以使用上下文编码信息生成更连贯和适当的输出文本。目前公开的文本嵌入模型调研归纳如表3所示。",
    "context4": "表3文本嵌入模型调研  \nTable3Research on Text Embedding Models",
    "context5": "<table><tr><td>名称</td><td>提出时间</td><td>模型特点</td><td>预训练语料库</td></tr><tr><td>Word2Vec[47]</td><td>2013</td><td>两种基本模型(Skip-Gram 和CBOW),通过最大化词与其上下文词 之间的共现概率,学习词嵌入表示</td><td>谷歌新闻数据集等</td></tr><tr><td>GloVe[48]</td><td>2014</td><td>基于全局词频统计,充分利用全局统计信息,融合了局部窗口方法和 全局矩阵分解方法</td><td>维基百科、新闻文章、书籍等</td></tr><tr><td>CoVe[49]</td><td>2017 信息</td><td>使用序列到序列模型的编码器预训练词嵌入,能获取丰富的上下文 英文维基百科、书籍语料、WMT机器翻译数</td><td>据集</td></tr><tr><td>ELMo[50]</td><td>2018</td><td>考虑了词的上下文信息,每个词的嵌入是所有层的加权和,可以生成 更丰富的词嵌入</td><td>维基百科等数据</td></tr><tr><td>BERT[51]</td><td>2018</td><td>通过Transformer架构,能够获取句子的双向上下文信息</td><td>BooksCorpus 维基百科等</td></tr><tr><td>GPT</td><td>2018</td><td>基于Transformer架构,通过预测下一个词的方式学习词嵌入 通过自回归方法,解决了BERT中的掩蔽问题,能获取更全面的上下BooksCorpus、Wikipedia、Giga5、</td><td>BooksCorpus、维基百科等</td></tr><tr><td>XLNet[52]</td><td>2019</td><td>文信息</td><td>ClueWeb2012-B、CommonCrawl等</td></tr><tr><td>ERNIE 3.0[53]</td><td>2021</td><td>融合了各种知识图谱,支持多模态输入,增强了模型的理解能力</td><td>中文百科、百度百科、百度新闻、百度贴吧等</td></tr><tr><td>Instructor[54]</td><td>2022</td><td>一种指令微调文本嵌入模型,可以通过简单地提供任务指令生成针 包含330个来自 Super NaturalInstructions的 对任何任务(如分类、检索、聚类、文本评估等)和领域(如科学、金融 数据集与30个用于句向量训练的数据集的 等)的文本嵌入,而无需任何微调</td><td>数据集集合MEDI</td></tr><tr><td>E5[55]</td><td>2022</td><td>采用两阶段的训练方式,基于弱监督预训练和对比学习训练而成</td><td>2.7亿文本对数据集CCPairs</td></tr></table>",
    "context6": "总体而言，随着时间的不断迭代,模型预训练语料规模以及文本嵌入模型参数规模的不断增大，文本嵌入模型的向量化表示效果呈现出越来越好的趋势。对于检索任务而言，由于问题与答案的匹配不能仅靠文本相似度进行判断，且回答可能是单词、短语、句子、段落乃至长文本,回答长度不一，传统依靠句子相似度、单词相似度或文档相似度匹配的方法并不能直接适用于问答检索领域的文本嵌入表示。根据HuggingFace公布的大规模文本嵌入基准排行榜[56],在多个检索任务上,E5文本嵌入模型都位居前三名，该模型凭借着高质量的训练数据、两段式的训练策略、精心设计的微调方法、一致性过滤等技术手段在开源文本嵌入模型中处于领先地位。"
  },
  "3.2语义检索技术": {
    "context1": "语义搜索技术是指在搜索过程中，利用自然语言处理技术分析查询文本和文档的语义信息，实现查询意图与文档语义内容之间的语义匹配，从而返回更加相关和符合查询意图的搜索结果[57]。最大内积搜索作为重要的语义搜索技术,通过最大化查询向量与文档向量的内积实现文档排序和语义匹配[58],为检索增强生成技术提供了底层搜索技术支撑,用于在海量知识向量库中快速、精准地匹配到与用户提问相关的文本段落。然而，最大内积搜索需要计算查询与所有文档的向量相似度,计算量过大，时间成本高昂。为了加速检索过程,平衡检索效率与检索速度，近似最近邻搜索成为目前向量数据库所使用的主流检索方法，诸如FAISS、Chroma、Milvus等向量数据库都已支持高速的近似最近邻搜索。该方法运用分而治之的思想，将原始数据经过映射方法划分到多个不同的向量空间,在较低的准确性损失下，返回前k个最近邻，从而换取搜索速度提升。根据不同的算法基本思想,近似最近邻搜索可以分为四大类别,分别是基于局部敏感哈希的方法、基于空间划分的方法、基于向量压缩的方法以及基于图的方法，4类方法的原理与特点对比如表4所示。",
    "context2": "表44类近似最近邻搜索方法的对比  \nTable 4Comparison of Four Approximate Nearest Neighbor Search Algorithms",
    "context3": "<table><tr><td>类别</td><td>典型算法</td><td>基本原理</td><td>优点</td><td>缺点</td><td>检索速度</td><td>检索准确率</td></tr><tr><td>基于局部敏 感哈希[59]</td><td>敏感哈希等</td><td>符号随机投影、使用哈希函数将高维数据 在高维空间中具有较 查询感知局部 映射到桶中,相似数据更 好的效果,适用于图 可能被映射到同一个桶</td><td>像、音频等数据</td><td>据检索效果较差</td><td>数百毫秒</td><td>对于维度较低的数几毫秒到 60%~80%,受到哈希函数、 数据规模等因素的影响</td></tr><tr><td>基于空间 划分[60]</td><td>随机k维树、k 均值树、倒排索 引等</td><td>将数据空间通过树形结 对于维度较低的数据 构递归划分,以在有限范 和均匀分布的数据点 围内进行搜索 使用向量量化或标量量</td><td>效果良好</td><td>在高维空间中存在 几毫秒到 “维数灾难&quot;问题</td><td>几百毫秒</td><td>检索准确率在低维数据上 可达95%以上。但在高维 数据上可能下降到30~50%</td></tr><tr><td>基于向量 压缩[61]</td><td>正交乘积量化、 标量量化等</td><td>化方法对数据进行降维，能够在一定程度上减 以减少索引的大小并节 少存储和计算成本 省内存</td><td>检索召回率和速度最</td><td>压缩后的向量可能 几百毫秒 会失去一些信息，到秒级范 从而影响准确性</td><td>围</td><td>50%~85%,但随着维数的 增加,检索准确率会降低</td></tr><tr><td>基于图[62]</td><td>导航小世界、近 邻图搜索等</td><td>构建数据之间的图结构， 沿着图中相似节点的路 径搜索</td><td>优,能够捕捉数据点 构造图难度较大， 之间的复杂关系,适 搜索速度较慢 用于结构化数据</td><td></td><td>以上</td><td>几百毫秒 70%~90%,但随着数据量 到秒级及 的增加,检索速度和检索 准确率都有可能下降</td></tr></table>",
    "context4": "由表4可知，近似最近邻搜索领域内涵盖的每类方法均呈现出独特的优势和限制,以适应不同数据特征和搜索需求。具体而言，基于局部敏感哈希的方法在高维数据中展现出显著的效能,支持多样的相似性度量，但在低维情境下表现相对欠佳，且其应用需要复杂的参数调整与哈希函数设计。基于空间划分的方法在低维且数据均匀分布的场景下表现突出，然而在高维空间中可能受制于“维数灾难\"问题,对数据分布的先验信息要求较高。基于向量压缩的方法在存储和计算成本上具备优势，但压缩过程可能引发信息损失，因此需在压缩率和搜索质量之间做权衡。基于图的方法可有效处理数据点之间的复杂关系，特别适合处理结构化数据,但也面临着图构建难度较大、搜索时间复杂度高等挑战。"
  },
  "3.3小结": {
    "context1": "通过对文本嵌入模型与近似最近邻搜索技术的系统梳理，可以看出语义表示与语义检索技术在基于检索增强生成的问答推理中发挥着关键作用。文本嵌入模型在语料规模与模型设计等方面不断优化，已能较好地表达语义信息；近似最近邻搜索作为快速精准的向量检索方法,也日趋成熟,基于图的方法正在兴起。综上可知,检索增强生成所需的语义表示与检索技术基础较为成熟,结合大语言模型的生成能力，检索增强生成技术可以发挥各自技术优势,实现知识表达与知识检索的有效融合。这为问答系统提供了外部知识的即插即用特性，能够显著提升问答的可解释性和可信度,有助于问答系统获取更丰富相关的外部知识，从而生成更符合事实的回答。但是，目前仍存在着文本嵌入模型表达能力有限、语义匹配精度不高等问题,未来可继续探索设计更优文本嵌入模型和多源异构知识表示以提升问答语义理解能力，设计更高效的检索方法与优化近似最近邻搜索算法以实现高效准确的向量检索等。"
  },
  "22 数据分析与知识发现": "",
  "4基于提示工程的问答推理": {
    "context1": "对于简单问题问答任务而言,大语言模型借助检索增强生成技术便已能够通过向量检索查询获取到相应信息，从而生成更准确可信的答案。然而，对于复杂问题问答任务而言，传统检索方法的优势逐渐减弱,语义理解不足的弊端也显现出来,此时则需要运用提示工程技术进一步发挥大语言模型的语义理解能力。众多研究人员也已发现，利用提示工程技术将简洁而高效的自然语言描述纳入模型中，便能够实现对复杂问题的知识推导，从而有效提升复杂问题回答的完整性和准确性。目前常见的提示工程技术主要可分为任务规划型和反思评估型两类，该技术通过将复杂问题分解、反思评估优化,将推理过程纳入其中，从而获得更全面准确的答案。"
  },
  "4.1任务规划型提示工程技术": {
    "context1": "面对较复杂的任务和需求，往往需要在明确问题的情况下，对复杂问题进行分解与规划。大语言模型提示工程技术的核心在于用户通过编写提示词,引导大语言模型按照提示词的逻辑和思考方式进行问题分析和解决。在任务规划领域,提示工程技术目前主要包括以下类型。",
    "context2": "（1)思维链(ChainofThought,CoT）。该方法通过生成一系列短句逐步描述推理逻辑,让大语言模型进行逐步思考，从而利用更多的测试时间计算将困难任务分解为更小、更简单的步骤,将大任务转化为多个可处理的任务，进而得到最终答案[63]。该方法通过逐步推理引导的方式，显著提高了数学单词问题和符号操作等复杂任务的回答准确率。然而，该方法在提示词中强加线性思考逻辑，从根本上限制了大语言模型的推理能力。",
    "context3": "（2）自我一致性思维链（Self-Consistency withCoT,CoT-SC)。该方法在思维链的基础上进行改进优化，通过先采样生成多条不同的推理路径，然后汇总得到一致性最高的答案，而不是仅采用贪心解码得到单一推理路径[64]。该方法允许探索不同的推理路径，通过生成多个独立的思维链并选择输出效果最好的一个，但是仍然无法在推理路径中进行局部探索，如进行回溯等。",
    "context4": "(3)LLM+PDDL规划器。该方法主要依赖于外部的经典规划器来进行长期规划,通过使用规划领域定义语言（Planning Domain Definition Language,PDDL)作为中间接口，用于描述规划问题。在这个过程中，大语言模型首先将问题转化为“问题PDDL”,然后请求经典规划器基于现有领域PDDL生成PDDL规划,然后将PDDL规划转化回自然语言,输入大语言模型中得到解答[5]。该方法将保证逻辑正确的经典规划器优势与大语言模型强大的语义理解能力相结合，使代理能够解决对话描述的复杂规划任务。但是,该方法严重依赖领域专业知识提供动态规划，并且需要一个示例问题指导语言模型,实现难度较高，自主性较低。",
    "context5": "（4)思维树（TreeofThoughts,ToT）。该方法的核心思想在于通过在每个步骤探索多种推理可能性，从而扩展思维链。首先将问题分解为多个思考步骤,并在每个步骤生成多个思考，从而形成一个树状结构[6。搜索过程可以采用广度优先搜索或深度优先搜索，而每个状态都通过分类器或多数投票进行评估。具体而言,该提示工程技术使得大语言模型遵循4个步骤：将中间过程分解为思维步骤;通过思维生成器从每个状态生成候选想法;通过状态求值器启发性地评估状态;根据深度搜索和广度搜索算法选择最优方案进行回答。该方法将推理过程建模为一棵树,可以实现回、路径择优等功能,但是树结构仍然存在着严格的逻辑结构限制，一定程度上仍限制了复杂问题的推理能力。",
    "context6": "（5）思维骨架（Skeleton of Thought,SoT）。该方法的核心思想在于利用提示引导大语言模型先生成答案框架，然后并行调用API（ApplicationProgrammingInterface,应用程序接口）或批量解码并行扩展每个框架点的内容,从而加速答案生成[67]。该方法采用编号列表的形式,从框架中提取出编号及简要内容作为各个框架点，然后并行地提示语言模型扩展每个框架点的内容，最后将所有框架点的扩展内容拼接起来，形成最终完整的答案。该方法通过并行生成答案显著加快大语言模型的推理速度，还通过鼓励模型考虑多个角度，提高某些问题类型的答案多样性和相关性。但是,该方法很难解决需要逐步推理的问题，如数学问题等。",
    "context7": "（6)思维图(Graphof Thoughts,GoT）。该方法的核心思想是将大语言模型的推理过程建模为任意图形，将思想看作顶点，思想之间的依赖关系看作边[8]。该技术使得大语言模型遵循以下步骤进行推理回答：将任务分解为子任务，每个子任务由一个顶点表示;分别解决子任务，添加代表子任务解决方案的新顶点；评分和排序各个子任务的解决方案;选择性地聚合顶点，即合并子任务的解决方案,形成新的顶点;重复上述步骤，直到形成完整的解决方案图；从解决方案图中选择得分最高的解决方案作为最终结果。该方法将推理建模为任意图，可以实现聚合、回溯等推理，是目前最灵活的提示结构,但是实现难度较大，需要更仔细的人工设计。",
    "context8": "任务规划型提示工程技术主要包括思维链、自我一致性思维链、LLM $+$ PDDL规划器、思维树、思维骨架和思维图等。这些方法从线性推理逐步发展到树状和图状结构推理,灵活性与自主性不断提升,实现难度也随之增加。总体而言，此类提示工程技术正朝着更灵活、更自主、更人类化的方向发展,以便应对更复杂的任务和场景。当前的关键挑战在于如何在保证推理质量的同时，提高提示工程技术的自主性和人机协作性。未来此类提示工程技术则需要进一步模拟人类思维,实现更自然、更智能的人机互动。"
  },
  "4.2反思评估型提示工程技术": {
    "context1": "当前大语言模型在解决复杂问题时，单纯依靠一次性生成的回答往往难以做到准确完整，往往需要通过自我反思与评估纠正使大语言模型不断改进过去的行动决策来不断优化回答准确性与完整性。在反思评估领域,提示工程技术目前主要有以下类型。",
    "context2": "（1)ReAct(Reasoning and Acting）。该方法通过将动作空间扩展为任务特定的离散动作和语言空间的组合,将推理和行动融合到大语言模型中[9]。任务特定的离散动作使大语言模型能够与环境进行交互，而语言空间的组合则可以促使大语言模型生成自然语言的推理轨迹。ReAct提示模板包含了明确的大语言模型思考步骤,分别是\"思考、行动、观察”，并不断循环直至得到最优解。该方法通过整合与维基百科等外部环境互动的行动和观察反馈,生成了更扎实和事实上更准确的推理。然而，ReAct方法中交错推理、动作和观察的约束也降低了推理公式的灵活性。",
    "context3": "(2）Self-Ask。该方法主要包含Follow-Up、ImmediateAnswer步骤,通过让大语言模型自行判断决定是否需要将问题以及回答步骤进行拆分，整体上保持着\"总-分-总\"的简单回答逻辑[70]。该方法与思维链类似,都属于启发性提示方法,区别在于该方法在回答复杂问题之前将其显式分解为简单的子问题,而思维链允许模型在输出最终答案之前\"通盘讨论”,没有明确地将问题分解与子问题的回答分开。因此,该方法使得模型更容易以简洁、可解析的格式陈述最终答案。该方法的缺点在于计算步骤和时间较长，在复杂的构图问题上表现仍然远远低于人类水平。",
    "context4": "(3)Reflexion。该推理框架采用标准的强化学习设置,其中奖励模型提供简单的二元奖励,行动空间则遵循ReAct方法的设置。在这个设置下，任务特定的动作空间通过语言扩展以实现更复杂的推理步骤。在每个动作执行之后，代理计算启发式函数用于决定行动轨迹是否低效或是否包含幻觉，并根据自我反思的结果选择性地决定是否重置环境以开始新的试验[71]。该方法允许语言模型代理通过自我反思和持久记忆快速有效地从试错中学习，而不像传统的强化学习方法那样需要从大量的训练样本进行昂贵的模型微调。但该方法在很大程度上仍严重依赖于语言模型的自我评估能力且与其他策略优化技术一样,局部极小问题仍持续存在。",
    "context5": "(4)Plan-and-Solve。该方法本质上是先计划再执行,即先根据用户的问题,让大语言模型设计出执行计划，把用户的问题分解成一个个子任务，然后再执行各个子任务，最后合并输出得到结果[72]。该方法通过详细的分解提示指引,使语言模型能够显式地设计解决给定问题的计划，更好地发挥语言模型的多步推理能力,但需精心设计提示语句,且无法避免原有的语义理解错误情况的出现。",
    "context6": "(5)DTG(Deliberate then Generate）。与现有提示方法不同的是,该方法不仅提供正确信息，而且还会提供包含错误的信息引导模型进行自我思考判断。该方法通过加入反馈机制让模型自主发现文本中的错误并进行改正,进而提高语言使用能力[73]。具体来说，该方法可以分为三个步骤：清晰明确地说明所需的任务，并提供生成论证的指导;提供一个合成文本作为候选输出；通过鼓励模型发现潜在的错误并经过自我思考判断改进输出，推动训练过程。该方法是一种简单且有效的提示工程技术，可以使语言模型在最终文本生成之前进行检测和更正错误,能够有效地避免错误的发生。然而,该方法正确与否的判断标准可能来源于预训练大语言模型固有偏见。",
    "context7": "反思评估型提示工程技术主要包括ReAct、Self-Ask、Reflexion、Plan-and-Solve以及DTG 等。其中,ReAct通过行动空间扩展实现推理与交互的融合；Self-Ask明确将问题分解为子问题后逐一回答；Reflexion利用试错学习和启发式函数进行自我检验;Plan-and-Solve先计划再解决的策略明确分解问题;DTG则提供包含错误信息的反馈引导模型自我检验。这些方法都在一定程度上提高了模型的推理能力,但各有优劣。总体上,反思评估型提示工程技术因其简洁高效已成为大语言模型研究热点，实现了用户与模型的自然交互，同时引导模型进行逻辑推理,从而显著提高了大语言模型在复杂问题上的准确性和完整性。未来此类提示工程技术则需要向更好地模拟人类思维、实现开放域的复杂推理、规避模型自身的偏见等方向探索。"
  },
  "4.3小结": {
    "context1": "本节系统梳理了基于大语言模型的复杂问题问答推理的提示工程技术。提示工程技术通过设计引导性的自然语言描述，可以有效地发挥大语言模型的复杂推理能力，显著提高复杂问题的回答质量。提示工程技术可以分为任务规划型和反思评估型，任务规划型提示工程技术从线性推理的思维链发展到树状结构的思维树，以及任意图形结构的思维图，实现了复杂任务分解与推理路径的探索。反思评估型提示工程技术提示引入了反馈机制,通过自我检验思考纠正推理偏差,实现回答的不断优化。这些提示工程技术的提出极大拓展了大语言模型的推理边界。当前提示工程技术面临的主要挑战在于如何在保证推理质量的同时,提高提示工程技术的自主性、开放性以及可控性。未来研究可继续深化对复杂推理过程的模拟,建立标准化的数据集，开发通用的提示搜索与优化算法等。"
  },
  "5总结与展望": "",
  "5.1研究总结": {
    "context1": "本文通过对基于大语言模型的问答技术研究进展的系统梳理，总结了当前该领域的核心技术内容与发展现状。在大语言模型方面，目前已有众多不同规模的开源预训练语言模型问世，为学术研究提供了有力支撑，这些模型大多采用了优化的解码器架构,并在模型设计上进行了创新,如采用旋转词向量嵌入、并行层技术、多头查询注意力等，但在拓展输入长度、提升推理速度等方面仍需进一步研究模型架构的创新设计。参数高效微调策略已成为优化模型垂直领域、特定任务适配性的重要手段，LoRA类参数高效微调技术已较为成熟，这为面向问答服务的大语言模型效果提升和应用落地提供了技术支撑。在基于检索增强生成的问答推理方面,文本嵌入模型发展日趋成熟,能表达丰富的文本语义信息，其中面向检索问答式任务,E5、Instructor等文本嵌入模型效果较优。近似最近邻搜索作为向量检索方法也取得长足发展,尤其是基于图方法的检索，已有大量实验证明将大语言模型与检索增强生成技术相结合可以显著提升问答可解释性和可信度，但目前向量检索的匹配精度仍有待提高，检索结果损耗问题依然存在。在基于提示工程的问答推理方面，任务规划型提示工程技术实现了从线性推理到树状和图状结构推理的进步,推理灵活性不断增强;反思评估型提示工程技术通过嵌入检测纠错提示词便能明显提升复杂问题回答的完整性和准确性，目前提示工程技术尚处于探索阶段，其稳定性与可控性等仍面临着巨大挑战。",
    "context2": "总体而言，基于大语言模型的问答技术取得了飞速发展,在模型设计、参数高效微调、语义表达、复杂推理等多个方面均取得突破。检索增强生成技术作为当前研究热点，能显著提升问答的可解释性和可信度；任务规划型与反思评估型提示工程技术也大幅拓展了大语言模型的推理边界。但是,知识表达能力与生成内容可信度的提升仍是当下的技术瓶颈,多源异构知识的表示与控制也面临挑战，问答技术的应用范围与场景亦有待扩展。本文通过较全面、系统的文献调研和技术梳理,深入解析了基于大"
  }
}