{
  "original_filename": "full_2529.md",
  "·研究综述与前沿进展·": "",
  "可视化应用的评估方法研究综述": {
    "context1": "刘合翔",
    "context2": "（杭州电子科技大学人文艺术与数字媒体学院，浙江 杭州310018)",
    "context3": "摘要：回的/意义可视化是新形势下信息服务的一种普遍技术需求和应用趋势，而如何更好地运用这一技术就涉及对技术本身及其用户效用的考察与评估。仿法/过程本文通过回顾相关研究及实践中对其可视化应用的评估方法，分别从理论运用、方法路径、评估任务、评估维度及指标等几个方面进行了梳理与分析。结果/结论研究发现，目前可视化应用评估中采用的方法已比较多元，但多数评估仍主要以实验方法为主，评估的效用多以认知为主，可视化的情感效用与社会效用少有探讨及测度，缺少评估的通用框架和用于评估的标准（任务、数据）测试集。而这些方面也相应地为可视化应用的评估提供了进一步创新实践的机会。",
    "context4": "关键词：可视化；应用；评估方法；评估任务；评估维度；评估指标；综述 DOI: 10.3969/j.issn.1008-0821.2020.04.017 （中图分类号）G252（文献标识码）A（文章编号）1008-0821（2020）04-0148-11"
  },
  "Research Review on Evaluation Methods of Visualization Application": {
    "context1": "Liu Hexiang (School of Media and Design，Hangzhou Dianzi University，Hangzhou 310018，China)",
    "context2": "AbstractPurpose/Significancel]Visualization isa general technical requirement and atrend of information services inthenew situation，andbeterutilizingofthis technologydependsoninvestigationandevaluationof itsutility.Method/ Process]From aspects of theory employing，assessing ways，task，dimensionand index of evaluation byreviewing relevant researches and practices.Result/Conclusion]The methods used in the assessment of visualized applications were quite diverse，but most of theevaluations were mainlybased on experimental methods.Theutilityof the assessment was mostlyaboutcognition.Emotionalandsocial utilitywererarelydiscusedand measured.There wasalack of anassessment generic frameworkand standard test set（oftask，data)for evaluation.Corrsponding to this，there were manyopportunities for innovative practices in the evaluation of information visualization applications.",
    "context3": "Key words:visualization；application；evaluation method；evaluation task；evaluation dimension；evaluation indicator；review",
    "context4": "在当前信息服务的主体越来越多元化，信息服务开始高度竞争化的形势下，信息用户对信息服务提出了有别于和高于传统信息服务的要求。而其中的一点就是在信息简单传递和信息的定题研究之间，延伸出一类新的服务方式：尊重用户自身的信息研判能力，通过提供有效的信息解读辅助，提升用户自身解读信息的效能。而对于信息的辅助解读最有效的莫过于利用人类高效的视觉认知通道和视觉处理能力来进行信息的可视化。而要确保这一技术手段能得到有效的运用，就需要通过对可视化工具能力的评估和可视化应用后的用户效用评价来为可视化应用的持续改进提供依据。",
    "context5": "本文首先通过检索主题相关的研究，选择研究的样本集，在相关研究变量中选择理论运用、方法路径、评估任务、评估维度及指标4个方面来考察、梳理与比较，最后形成一个以这4个变量为主体的信息可视化应用评估方法的参考框架。"
  },
  "1可视化评估相关研究概述": {
    "context1": "国内相关研究中，通过对“可视化AND（评估OR评测OR评价）”等关键词组合的文献检视，结果文献中大多数是研究如何将可视化作为评估的手段应用在特定领域，而将可视化工具本身及其用户效用作为评估对象的研究则较少。国内针对这方面的典型研究有：陈为对可视化测评的相关研究中评测流程、方法和影响因素的梳理总结;杨峰等对评价信息可视化技术的指标研究和信息可视化显示效果优化的研究；吴莎莎等对信息可视化技术可用性分析的框架设计与案例检验；冯小琴对信息检索可视化的评价标准研究;张会平对可视化的知识转化效用假设的实证研究；王辉等对图书馆可视化系统效用的测试研究；邱玲玲对可视化教学效果的实证研究等。",
    "context2": "而通过“Visualization AND（Evaluate OR As-sess OR Measure）”检索的外文文献相比较而言，针对可视化本身及其效用进行评估的研究则较为普遍。评估类研究也被归结为可视化研究中几大类别中的一类。由于可视化连接的是以机器为中心的过程（如算法和技术）和以人为本的过程（如人类认知、交互、情感和沟通)，因此可视化的评估通常围绕这两个方面来展开。此前的研究中以可视化技术和算法的评估居多，而近期针对用户的表现与反馈，评估可视化对其工作实践和分析推理能力改进的研究已有稳步的增长[i 。",
    "context3": "尽管已经有各种各样日渐成熟的可视化技术，但针对不同的应用情境还需要对其进行特定的评估，以确定所选技术在特定场景下是否有用、可用与实用。相比评估结论的差异性与多样性，其评估的方法更值得归纳与总结。不同的评估方法也反映了不同研究者的偏好和对不同情境下方法适用度的判断。"
  },
  "2可视化应用评估中的理论运用": {
    "context1": "理论与模型的借鉴是开展各领域研究的一种常用方法，站在成熟的理论与模型的基础上，可以让相关研究能够快速构建思考框架，同时对原有的理论模型做适应性检验和进一步地提炼与延展。",
    "context2": "在涉及可视化应用评估的研究中，部分研究运用了一些理论模型来思考研究主题并构建研究框架，涉及的相关研究及其要点按其发表年份列举如下:",
    "context3": "表1可视化评估研究中的理论运用",
    "context4": "<table><tr><td>研究者</td><td>引述理论</td><td>理论用途</td></tr><tr><td>Juarez 0等</td><td>“信息处理理论”</td><td>评价不同可视化形式对不同任务类型的适用关系</td></tr><tr><td rowspan=\"2\">Valiati E R A等</td><td>“诺曼行动理论”</td><td>提供可视化任务中行为的阶段性参考框架</td></tr><tr><td>“人机交互任务模型”</td><td>设计可视化效用的实验任务</td></tr><tr><td>Cawthon N等[</td><td>“Wundt 的审美接受曲线”、 “情感设计理论”</td><td>分析可视化中的美学效用</td></tr><tr><td>Hoffler TN等[4</td><td>“认知负荷理论”、“双编码理论”、 “工作记忆理论”、“多媒体学习的 认知理论”</td><td>分析动画在可视化中的意义以及与静态图的效用比较</td></tr><tr><td>Moere AV等</td><td>“信息美学模型”</td><td>引用其9个标准用例分析“风格”对可视化效用影响 参考其模型将可视化的评估分为4个层面：问题表征、数据</td></tr><tr><td>Isenberg T等[</td><td>“Munzner嵌套模型”</td><td>与任务抽象、视觉编码/交互设计、算法设计</td></tr><tr><td>Demiralp C等[d</td><td>“视觉嵌入理论”</td><td>作为可视化构建和评估的理论模型，将可视化定义为从数据 点映射到一系列视觉要素的函数，而将视觉嵌入定义为在嵌 入的感知空间（范围）内保留数据（域）中结构的函数 视觉嵌入理论中，可视化被认为是针对数据结构的感性绘画</td></tr><tr><td>Bresciani S 等[1</td><td>“技术接受模型’、“创新扩散理论”、 “技术接受与使用统一模型”</td><td>评估影响可视化接受与使用的要素</td></tr><tr><td>Reda K等[1</td><td>“基于马尔科夫链的行为模型”</td><td>评估探索性可视分析中的用户行为及效用</td></tr><tr><td>Dimara E等[g</td><td>“理性选择理论”</td><td>分析评估可视化中视觉诱饵所产生的吸引力对用户认知的影 响</td></tr></table>",
    "context5": "由以上提及的理论可以看出，可视化应用评估的相关研究中运用的理论来自于HCI、认知科学、设计学等多个领域。然而，相比涉及可视化应用评估研究的数量，运用相关理论来开展评估的仍然属于少数。事实上，在目前相关研究中，理论的运用并未成为一种自觉，运用理论的范围与深度也很有限，很多有价值的可参考理论，如信息空间理论、信息服务理论等未能被应用到可视化应用的评估上来，而这也为未来相关研究提供了进一步开展理论探索的空间。"
  },
  "3可视化应用评估中的方法路径": {
    "context1": "关于可视化应用评估的方法，有一些研究总结了其不同的方向类别。例如 MunznerT总结的可视化应用评估股方法主要有两类：1）在实验室环境中进行用户研究，使用明确或抽象的任务，在时间和准确性方面进行定量测量，运用统计学方法进行分析，发现显著影响因素和因素之间的相互作用；2）开展现场研究，将可视化应用在目标用户的现实环境中，时间跨度通常为数周或数月，开展定性与定量测量。Plaisant $\\mathrm { ~ C ~ } ^ { \\mathsf { L } \\mathbb { d } }$ 则是将可视化应用的评估方法分为4类：控制实验比较设计元素，工具的可用性评估，比较两个或更多工具的控制实验，以及在现实环境中对工具进行案例研究。而Carpendale $\\mathrm { S } ^ { [ 2 \\mathbb { I } ] }$ 将可视化应用的评估方法从精准性、通用性和现实性的3个维度进一步扩展为：实地研究、实地实验、实验室实验、实验性模拟、判别研究、抽样调查、理论研究、计算机模拟8个类别。",
    "context2": "基于对相关研究中方法选型的梳理以及参考前述的这些研究中的分类，本文认为可视化应用评估的方法总体可划分为3个大类：客观评估、主观评估与综合评估。",
    "context3": "在客观评估的有关方法上，最常用的就是对照实验及观察法。Saraiya $\\mathrm { P }$ 等认为可视化应用的评估就主要是在对照实验研究中评估用户对预定任务中的表现。而对于实验的方法，其基本的逻辑是：控制独立变量如工具、任务、数据和参与者等，依赖变量主要是准确性和效率。其中准确性包括精确度、错误率、正确和不正确响应的数量，而效率包括完成预定义基准测试任务的时间。而在实验和传统的观察方法之外，目前已有研究延伸了可视化应用评估的技术手段和方法形式，例如ChenX等设计和实践了一个量化可视化所形成心理负载，并对用户偏好持续考察的在线评估。评估运用基于脑电波、眼动跟踪及日志等手段记录的数据进行了融合性的分析。",
    "context4": "在主观评估的有关方法上，有很多研究者采用了有别于客观评估的形式与方法，以弥补可视化应用评估中客观评估方法的不足。例如ToryM等4就反思总结实验评估方法的弊端和不足，例如实验评估的成本、深度以及所适用的阶段等，通过与实验评估的比较，论证了专家评估方法的有效性。ZukT等认为启发式评估（基于既有经验规则的探索式评估）在可视化应用评估上使用得还不够，并总结整理了有关研究中可供参考的经验规则。KennedyH等则是在其可视化应用评估中尝试运用了多焦点小组访谈的形式，将小组分为对视觉表达有兴趣组、对数据有兴趣组、外地域组、特定族裔组、主题兴趣组、主题涉及组、无兴趣组、各组代表构成组等多类小组。",
    "context5": "随着前述一些主流的评估方法的运用，不少研究者对传统评估方法的有效性提出了疑问与批判，认为可视化应用的评估应当结合客观与主观评估的方法，参考借鉴有关的理论，综合更多的一些现实因素进行考量。",
    "context6": "偏综合性评估的相关研究中，Shneiderman B等在回顾可视化应用的评估方法后提出评估可视化需要引入“多维深度长期案例研究”。其中多维是指使用观察、访谈、调查以及自动记录来评估用户绩效和界面效能；深入是指将专家用户转变成合作伙伴，深入、互动地开展研究；长期是指纵向研究，从特定工具的使用培训开始，观察用户的策略变化；案例研究是指在正常的环境中对用户向工作人员详细报告的有关问题进行分析。而Reda K等[发现大多数的可视化实证评估都是针对结果，而极少评估其中的用户过程，并针对这一问题以探索性可视分析为例构建了一套强调过程评估的评估方法。IsenbergP等还认为可视化应用的评估在可视化产品开发生命周期的前期就应该进行，进而提出一种质性评估方法，判断可视化是否符合预期的使用情境，并通过评估来促进可视化设计的优化迭代。",
    "context7": "而针对当前的可视化应用评估方法中存在的一些普遍问题，有关研究提出了方法批判，例如 An-drews $\\mathrm { ~ K ~ } ^ { [ 2 9 ] }$ 认为：评估的新的可视化界面不够成熟，在与成熟可视化界面比较时存在成熟度上的劣势；选择的测试用户不具有代表性；测试用户对可视化界面学习周期影响的考虑不够；测试任务设置简单，缺少对洞察和探索性任务的测试评估等。",
    "context8": "更多涉及到以上划分的评估方法，及其应用到的有关研究可参见表2。",
    "context9": "表2可视化评估中的评估方法路径类别",
    "context10": "<table><tr><td>客观评估</td><td>主观评估</td><td>综合评估</td></tr><tr><td>对照试验[8,27-2</td><td>专家评估</td><td>案例分析[4,282g</td></tr><tr><td>观察法[</td><td>后发式评估</td><td>理论研究[17,30</td></tr><tr><td>脑电波分析[23,3]</td><td>抽样问卷调查[6-7,21]</td><td>强调过程性评估[，强调前期评估</td></tr><tr><td>眼动分析[23,3</td><td>焦点小组访谈2</td><td>强调多维、深度、长期评估[2]</td></tr><tr><td>日志分析[23,3</td><td>民族志调查</td><td>研究方法批判</td></tr></table>"
  },
  "4可视化应用评估中的评估任务设计": {
    "context1": "尽管可视化应用评估中采用的方法已较多元化，但目前主流的评估方法仍是以实验方法为主。而由于可视化应用评估的实验针对的具体目标往往不同，评估的对象各异，场景也不一，因此实验设计中的样本规划与流程安排必然是高度情境依赖的，相比较而言，评估实验中的任务设计则借鉴意义更大，因而本节重点考察的是相关评估实验中的任务设置。",
    "context2": "表3是对涉及相关任务的研究梳理，可以看到在KellerP等列举了可视化应用评估的一些常见任务后，后续许多研究在其基础上提出了扩展的评估任务，进一步细化和丰富了可视化应用评估可选的任务集。而这里列举的各研究涉及的具体任务主要是其有别于其他研究创新或特色的任务。而在陆续提出众多的评估任务后，相关研究也对可视化评估中的任务设计做出了类别与方向的划分，如予知（Inform）任务和使能（Enable）任务b，基于理性的任务和基于世界观的任务，目标性任务和支持性任务[等，同时也为结构化地任务设计提供了思路。",
    "context3": "而在多元的实验设计与实验任务背后，这类实验性的评估存在一个显著的问题，即实验与实验之间难于做参照和比较，而这一问题又源于：待评估应用面向场景与问题的差异、实验设计与设置的差异、受试者的差异等。这就导致了我们难于回答：目前的实证研究在多大程度上一致，是否能形成共识，对于特定的任务是否存在最优的可视化形式等。",
    "context4": "在ChenC等4对35项信息可视化的比较研究发现，虽然多数的实验任务是基于检索的情境设置，评估也主要是针对可视化环境下任务的完成度和完成效率，相关影响因素的相关性及其显著性也通常会考察，但在实验的可对照性和实验结果的可比较性上仍然存在很大的障碍。为克服这一类问题，作者提出了相关的实验设计建议，建议相关的研究尽量使用标准的测试数据（如TREC的NIST会议数据集）和标准的任务分类，重点考察常见的任务一形式的组合，尽量使用标准的认知能力测试（如认知因子参考测试工具箱），并且细化统计结果报告等。而上述的问题和建议无疑为相关的可视化应用的实验性评估提供了借鉴和参考。所幸越来越多的个人与机构意识到这一问题而做出了评估标准化的努力，例如IEEEInfoVis自2003年起就创建了可视化评估的基准数据集和任务[4] 。",
    "context5": "表3可视化评估中的评估任务设计",
    "context6": "<table><tr><td colspan=\"2\">研究者</td><td>设计的评估任务</td></tr><tr><td rowspan=\"3\"></td><td>Zhou M等</td><td>予知（Inform）任务：陈述、概览 使能（Enable）任务：探索、计算</td></tr><tr><td>Amar R等</td><td>基于理性的任务：揭示不确定性，关系具体化，判断因果关系 基于世界观的任务：确定领域参数，多变量解释和确认假设</td></tr><tr><td>Valiati E R A等</td><td>目标性任务：识别，确认，比较，推断和定位 支持性任务：视觉化、配置</td></tr><tr><td rowspan=\"10\">西</td><td>Keller P等</td><td>辨别、定位、区分、归类、分簇、分布、排名、比较、联系、关联</td></tr><tr><td>Zhou M等</td><td>在Keller等提出任务的基础上进一步扩展：划界、强调、揭示、追踪、标 签、量化、标记、切换、描摹、概括等任务</td></tr><tr><td>Morse E等[4</td><td>判断是否有更多满足条件的元素、哪些可以划归、哪些元素最相似、哪类元</td></tr><tr><td>North C等[</td><td>素最多、哪个节点联系最多、哪些判断为真、以及视图的总体特征 判断是否涵盖所有要素</td></tr><tr><td>IEEE Infovis Contest 4</td><td>提出可视化评估的基准任务集及数据集</td></tr><tr><td>Koua EL等4</td><td>识别数据间可能的关系、找出特定值域的数据、通过差异区分不同的实体、 对不同元素类别确认边界、通过空间的间隔划分不同的簇、描述整体的布局 特征、识别最好和最差、比较不同空间位置的值、识别数据元素间的形式关 系、识别具有共同属性的数据元素；在空间近似点中找到兴趣点位置、确认 兴趣点密度、查看不同属性不同值域的空间分布、识别数据中的不同簇以及 簇间的关系、比较不同位置的值并区分值域、明确值、位置与形状的关系、</td></tr><tr><td>I. -C. Wu等</td><td>分析提取信息的相关性等 发现直接关系、发现间接关系、找到主题知识</td></tr><tr><td>GhorbelF等</td><td>最大值点识别、属性枚举</td></tr><tr><td>Chen等21</td><td>自由探索，找到特定要求元素，统计特定要求元素</td></tr></table>"
  },
  "5可视化应用评估中的评估维度及指标": {
    "context1": "可视化应用的评估最终是通过对评估项及其对应评估指标的跟踪、记录与量化来实现的，因而明确评估项及其评估指标是这类研究的一项核心工作，其科学性与完整性很大程度上决定了可视化应用评估结论的效度。",
    "context2": "在可视化应用的过程中存在许多变量，例如源数据集、目标任务、显示与交互设备、人类观察者的知识和经验、交互行为、应用环境等。其中一些变量的测量是容易的，例如数据规模、精度以及时间。而其他的一些变量如信息、知识、认知则相对复杂。尽管如此，越来越多的研究者开始尝试来测量与评估特定可视化形式下，人类在洞察、理解、知识学习、创造、认知负荷、信心等方面的表现。",
    "context3": "从大的评估类别来看，可视化应用的评估可分为其用户效用的评估与其工具能力的评估两大方面。在一些具体的评估中，有的侧重于前者，例如用户的任务完成时间与成功率；有的侧重后者，如工具的易用性、易学性和满意度，有的则两者兼顾，如 Mackinlay $\\mathrm { ~ J ~ D ~ } ^ { \\mathsf { H } \\vec { \\mathbb { I } } }$ 认为良好的可视化应符合“表达性”和“有效性”的标准，前者关注（工具）如何真实地呈现，而后者则关注（用户）如何准确地看。HalimZ等4提出评估可视化应分别考察：（针对用户的）有效性、（工具的）表现力、可读性与交互性等方面，并且认为有必要分别给出权重进行组合评估。",
    "context4": "由于相关研究中引用的评估指标众多，更多的评估指标我们放在表4中总结和归类。需要注意的是一些新型评估指标的出现，例如在Jansen Y等[49对数据物理性可视化—数据雕刻的效用评估中，引入了“是否触摸”的评估维度。而这类指标对于越来越多基于触摸屏展示的可视化的评估是一个重要的提示。",
    "context5": "1）我们从用户表现、用户评价及反馈与专家评价几个评估维度来梳理相关的评估指标。其中评估用户表现的主要是些客观的量化指标，而用户评价及反馈、专家评价则主要基于的是偏主观的量化指标，具体的指标归集如表4所示。",
    "context6": "表4可视化应用评估中的评估维度及指标",
    "context7": "<table><tr><td rowspan=\"2\">评估维度</td><td>评估类别</td></tr><tr><td>用户效用评估 工具能力评估</td></tr><tr><td rowspan=\"2\">用户表现</td><td>效率效果——任务完成时长与成功率平均值[4,43,48.51；这两个指 标的最大值、最小值以及标准差[；用户测试得分,34,47,50 发现洞察一发现数、花费时间、发现的重要性、正确性、深 用户反应时长（正确、错误、平 度、意外性、发现类别；洞察的复杂度、深度、洞察的不完 均）b1、用户认知障碍与困惑点的个 全性与层次性、意外性以及关联性；洞察种类（分布、趋势、</td></tr><tr><td>数、操作时反馈的问题数[；学习时 边界、关联等)、洞察深度（确信度、难易度、深浅度）和洞察 长、用户用于查阅帮助、外部文档 缘起（推理、分析、情感等）的占比 搜索访问、寻求指导和支持的花费时 讨论参与一是否激发问题并意愿参与讨论、是否促成对主题正 间；过程节点数[；工具功能使用 式或批判性地讨论、沟通效率、参与率、对话频数</td></tr><tr><td>注意力分布一一兴趣区注视时间、注视次数、眼动轨迹、眼跳距 离、平均注视时间、兴趣区百分比；脑电波波长、熵；鼠 标轨迹；是否触摸</td><td>率[47,34,71 可识别度（可识别点的比例）5、感知</td></tr><tr><td rowspan=\"2\">用户评价 及反馈</td><td>认知反馈一—可视化在知识转化（社会化、外化、组合化、内 化）方面效用假设认同度评价；是否有收获、是否强化已有 知识、是否说服或改变认知、是否学到新知识、是否找到自己觉 得有用的数据[2；用户心智投入的评分b、认知负荷[55,7]</td><td>效率、感知正确性、可理解性与可学习 性；功能的缺失及好恶；用户对 工具结果和流程的理解及解释水平、用 户对于灵活性，兼容性（工具看起来 的工作方式与用户期望之间）和对工</td></tr><tr><td>反应态度—用户是否喜欢，是否产生对数据背后人或事的情 感、是否产生好奇心、是否惊讶、是否有愉快的经历或感觉愉 悦、是否感觉到视觉享受、是否激起强烈的情绪反应、是否增强 了对阅读可视化及数据的信心；初始使用意愿、再次使用 意愿、用户给出的综合评分5；有用性[50,59,34、可用 性[50,6]、实用性[31,62,60</td><td>具的功能和适用性的主观观点、工 具的满意度[48,44、易用性与易学 性[47-4d；用户对视觉特性以及配套的 听觉特性的评价4；使用不适（如心 理压力、视疲劳等）5</td></tr><tr><td>专家评价</td><td>基于成功度（轻松/困难、完全/部分）和任务完成时间的效用 专家评分61</td><td>界面可用性专家评价[23,63</td></tr></table>",
    "context8": "2）考虑到不同的评估场景，例如不同的用户特性、信息特性、工具特性、设计特性与情境，都会直接影响到评估测试的效果及相关结论的判定，因此还需要针对上述的场景维度做评估指标的梳理。",
    "context9": "相关研究中，BertiniE等将影响可视化视觉质量的因子分为3类：规模因子、视觉效果因子、特征保存度因子，其中规模因子对应的是可视化应用中的“信息特性”，而后两者则对应的是其“设计特性”。FreitasCMDS等从可用性的角度对信息可视化技术进行评价，将可视化的可用性分为3个主要的类别：视觉表现可用性，界面交互可用性以及数据可用性，分别对应可视化应用的“设计特性”、“工具特性”和“信息特性”。而",
    "context10": "Kennedy H等的评估涉及了“用户特性”的用户信念与观点、情绪、自信与技能，以及涉及“情境”的应用领域和任务时间等。",
    "context11": "更多基于不同特性及情境的评估指标归集详见表5。",
    "context12": "表5可视化应用评估中的评估维度及指标",
    "context13": "<table><tr><td rowspan=\"2\">评估维度</td><td>评估类别</td></tr><tr><td>用户效用评估 工具能力评估</td></tr><tr><td>用户特性 入</td><td>样本人群特征[48,1、参与者动机、期望、构成[1、用户偏 好4、用户对术语的理解[4、信念与观点、情绪、自信与技 能、专家用户2、任务准备状态、领域认知层级、时间投</td></tr><tr><td>信息特性</td><td>信息抽象度，1、数据规模；数据点数与数据密度；数据维 度数[、数据类型（布尔、向量)[；数据结构（一维、二维、 大数据集处理能力、对多样性数据的处 三维、时态、多维、层次结构、网络结构）、信息的碎片度理能力、对问题数据的处理能力，对疑 （离散度与不连续性）、信息的内向度（信息潜藏度）、信息的视问数据的处理能力、对多尺度数据的处 域（信息在空间及时间维度的可达的尺度）、数据来源[2、理能力[6 有无提示、有无文字、数据完整性46、不确定性</td></tr><tr><td>工具特性</td><td>状态转换、属性呈现、过滤筛选、 浏览器显示兼容性、可协作性、可交流性、对多样化用户的搜索查询、视域缩放[4,4；选择、缩 支持、图例、成图时长、交互性 放、旋转、摇摄、清除、浏览；定 位、帮助、撤销、集簇、裁剪[4,6</td></tr><tr><td>设计特性</td><td>空间组织、相关信息的视觉编码； 可视化类型（图标、图表、文字、表格、拟态）[43,6；表现 特征保存度[6；可读性、图形的 力、隐喻、秩序和复杂性；对称性、均匀度、统一边界 闭塞率（模糊的或被遮挡的数据点数/ 长度和交叉度；布局与风格（清晰度、美感）,、视觉诱 总数据点数)；可识别的关联数据点的 饵[2、展现的动态性；动画类型（电脑、视频)、动画的抽 比例（与其他可见数据点相关的数据 象度、动画的功用类型（展示、装饰）；逻辑顺序</td></tr><tr><td>所需知识类型（问题解决、陈述、程式化)[、任务、时 情境 间、应用领域[5,2a、工作环境、安慰剂[</td><td>点数量/可见数据点数2）[5g 隐私保护[</td></tr></table>",
    "context14": "事实上，场景的评估要素很难在某一项研究中完全穷尽考虑，需要结合实际的应用场景和评估条件来选择较佳的因子组合进行考察。",
    "context15": "举例来说，如按场景类别划分，在认知加工类场景中，可视化应用评估侧重的指标主要是可识别性、可交互性、洞察力等。知识获取类的场景中则相对偏重对保真度、可理解性、目标达成度等指标的评价，而交流协作类场景则侧重视觉吸引力、可交互性、沟通力等指标。如果是具体到某个特定场景，比如在虚拟现实环境下的可视化，其效用需要考察诸如可穿戴性、抗眩晕等针对性的指标。而对于图书馆馆藏资源可视化检索与浏览的场景，其效用评价的重点之一就在于可视化条件下用户对资源适用性的判别效率。",
    "context16": "通过对上述几类特性的识别与相关场景评估指标的梳理，可以帮助具体的评估中相关评估要素的选择，同时也能为可视化应用的设计与应用提供参考。"
  },
  "6归纳与总结": {
    "context1": "综合现有的可视化应用评估的各种实践及探索，对其中涉及方法进行总结归纳，通过对方法要素的摘选、合并与归类，本文整理了一个可视化应用评估方法的参考框架，如图1所示，包含可视化评估可参考利用的理论、评估方法类别、评估任务、评估项、主客观的评估指标、评估的影响因素等，为相关可视化应用的评估提供了一个较为完整的方法视图和要素设计的参考索引。",
    "context2": "![](images/a8c26dc63460dc76d112789542256dfe36e129e27643dba193aa60ebd0c852d6.jpg)  \n图1可视化应用评估方法参考框架",
    "context3": "基于该参考框架，在具体设计可视化应用的评估方案时，可依次从理论工具集中选取可参考的理论模型，选择合适的评估方法路径，参考和选用框架中涉及的评估任务类别及细项，基于框架中列及的影响因素选择需要重点控制的有关变量，选择框架中涉及的评估项目类别与评估指标来做评估的具体执行。",
    "context4": "目前可视化应用评估中采用的方法已比较多元，但多数评估仍主要以实验方法为主。各评估的侧重有所不同，有的偏重评估工具能力，有的更重视评测用户效用。当前可视化应用评估存在的一些典型问题有：评估中运用理论较少；评估的效用多以认知为主，可视化的情感效用与社会效用少有探讨及测度；缺少一个评估的通用框架和用于评估的标准（任务、数据）测试集。而与此同时，可视化应用评估的研究与实践也存在着很多的创新机会，例如，对视觉、认知等领域众多有价值理论的引入；针对新兴媒介（VR、AR等）可视化效果的评估；构建基于用户交互行为数据的可视化效用测算模型，通过二维码等方式实现线下线上互动的户外可视化应用的公共效用研究等。",
    "context5": "随着可视化应用的日益广泛和投入的加大，针对可视化应用的评估势必会为相关的工作实务所重视，进而对评估方法提出更高的要求。而本文通过对相关研究中可视化应用评估方法相关要素的总结梳理，为相关的研究及评估实务提供了一定的方法论参考。"
  },
  "参考文献": {
    "context1": "[]陈为．数据可视化M．北京：电子工业出版社，2013.   \n杨峰，李蔚．评价信息可视化技术的指标研究．图书情报 知识，2007，（4)：80-84.   \n杨峰，李蔚．信息可视化显示效果的优化方法研究[．现代 情报，2012，32(5)：34-36.   \n吴莎莎，刘正捷，张海昕．信息可视化技术的可用性[]／/ 建立和谐人机环境联合学术会议，2005.   \n冯小琴．信息检索可视化的评价标准．知识经济，2009, (3):126-127.   \n张会平．可视化技术对知识转化效果影响的实证研究[．情 报探索，2014，(1)：1-5.   \n王辉，吴鸣，肖永红，等．图书馆系统的用户测试组织案例研 —156— 究山．图书情报工作，2012，56（3)：71-74.   \n图邱玲玲，可视化与非可视化教学方法对不同文章背景知识的教 学效果的实证性研究[．贵阳：贵州大学，2008.   \nMunzner T.Processand Pitfalls in Writing Information Visualization Research Papers[]．Information Visualization:Human - centered Issues and Perspectives，2008:134-153.   \n[1d]Isenberg T， Isenberg P， Chen J，et al. A Systematic Review on the Practice of Evaluating Visualization[．IEEE Transactions on Visualization and Computer Graphics，2013，19（12)：2818- 2827.   \n[1]Juarez O,Hendrickson C，Garrett JH.Evaluating Visualizations Based o thePerformed Task[c]//Information Visualization, 2000.Procedings.IEEE International Conference on.IEEE, 2000: 135-142.   \n[12]Valiati ERA，Pimenta M S，Freitas C MD S.A Taxonomy of Tasks for Guiding the Evaluation of Multidimensional Visualizations [d//Proceedings of the2006 AVI Workshop on Beyond Time and Errors:Novel Evaluation Methods for Information Visualization. ACM，2006:1-6.   \n[13]Cawthon N，Moere A V.A Conceptual Model forEvaluating Aesthetic Effect Within the User Experience of Information Visualization [//Information Visualization，2006.IV 2006．Tenth International Conference on. IEEE，2006:374-382.   \n[14]Hoffler T N，Leutner D. Instructional Animation Versus Static Pictures:A Meta-analysis[.Learning and Instruction，2007, 17 (6): 722-738.   \n[15]Moere A V，Tomitsch M，Wimmer C，et al.Evaluating the Effect of Stylein Information VisualizationIEEETransactions on Visualization and Computer Graphics，2012，18（12)：2739- 2748.   \n[16Demiralp C，Scheidegger C E，Kindlmann G L，et al．Visual Embeding:A Model forVisualization[J].IEEE Computer Graphics and Applications，2014，34(1)：10-15.   \n[17]Bresciani S，Eppler MJ.Extending Tam to Information Visualization:AFramework for Evaluation[.Electronic Journal of Information System Evaluation，2015，18(1)：46-58.   \n[18]Reda K,Johnson AE，Papka ME，et al. Modeling and Evaluating User Behavior in Exploratory Visual Analysis[．Information Visualization，2016，15(4)：325-339.   \n[19]Dimara E，Bezerianos A，Dragicevic P.The Atraction Effect in Information Visualization[].IEEE Transactions on Visualization and Computer Graphics，2017，23（1)：471-480.   \n[0]Plaisant C.The Challenge of Information Visualization Evaluation [d//Proceedings of the Working Conference on Advanced Visual Interfaces.ACM，2004:109-116.   \n[1Carpendale S.Evaluating Information Visualizations[]．Information Visualization，2008:19-45.   \n[2]SaraiyaP，North C，Duca K.An Insight-based Methodology for Evaluating BioinformaticsVisualiations[]．IEETransactions on Visualization and Computer Graphics，2005，11（4)：443- 456.   \n[3]Chen X，Ran J.Statistical Modeling for Visualization Evaluation Through Data Fusion [．Applied Ergonomics，2017.   \nb4Tory M，Moller T.Evaluating Visualizations:Do Expert Reviews Work[]．IEEE Computer Graphicsand Applications，2005, 25 (5): 8-11.   \n[5] Zuk T，SchlesierL，Neumann P，etal.Heuristics for Information Visualization Evaluation[C]//Proceedings of the 2006 Avi Workshop on Beyond Time and Errors:Novel Evaluation Methods for Information Visualization.ACM，2006:1-6.   \nKennedy H，HilR L，Allen W，et al.Engaging with（Big) Data Visualizations:Factors that Afect Engagement and Resulting New Definitions of Effctiveness[]．First Monday，2016，21 (11).   \n[7]Shneiderman B，Plaisant C.Strategies for Evaluating Information Visualization Tools:Multi-dimensional In-depth Long-term Case Studies[//Proceedings of the 2006 Avi Workshop on Beyond Time and Erors:Novel Evaluation Methods for Information Visualization．ACM，2006:1-7.   \n[28]Isenberg P，Zuk T，Collins C，et al.Grounded Evaluation of Information Visualizations//Proceedingsofthe2o08 Workshop on Beyond Time and Errors: Novel Evaluation Methods for Information Visualization．ACM，2008：6.   \n[9]AndrewsK.Evaluating Information Visualisations[C]//Proceedings of the 2006 Avi Workshop on Beyond Time and Errors: Novel Evaluation Methods for Information Visualization.ACM, 2006:1-5.   \nb0]Latorre Postigo JM，Hernández-Viadel JV，Trives JJR. Efficacy of a Group Memory Training Method for Older Adults Based on Visualization and Association Techniques:A Randomized，Controlled Trial WithaPlacebo Group．Aplied Cognitive Psychology，2010，24（7):956-968.   \nB1lKim S，JooK H，Han S，et al. The Efectiveness of Visualization System for Virtual Reality Learning．International Journal of u-and $_ \\mathrm { e } -$ Service，Science and Technology，2015，8（12): 151-160.   \nB2]Ramasubramanian T.Evaluating theEfetivenessof Widely Available 3-D Visualization Tools in Support of Public Participation Journalof Public Administration SpecialIssue，2009.   \nB3Erfan Mirzabeiki，Lili.Nurliyana.EfectivenessofTextVisual ization as a Learning Facility（A Review)[．International Journal of Interactive Digital Media，1（3)：41-48.   \nB4Anderson EW，Potter KC，Matzen L E，et al.A User Study of Visualization Effectiveness Using EEG and Cognitive Load[]// Computer Graphics Forum．Blackwell Publishing Ltd，2011，30 (3):791-800.   \nB5]Brus J，Popelka S，Svobodova J.Exploring Effectiveness of Uncertainty Visualization Methods by Eye-Tracking[C]//Proceeding of the 1Oth International Symposium on Spatial Accurancy Assessment In Natural Resources and Enviromental Sciences，Brazil. 2012.   \nB6Schimke D，Stoeger H，Ziegler A.Evaluating the Effectiveness of Social Visualization Within Virtual Communities[]．Virtual CommunityPracticesand SocialInteractive Media:Technology Lifecycle and Workflow Analysis:Technology Lifecycle and Workflow Analysis，2009.   \nB7]Zhou M,Feiner S K.Visual Task Characterization for Automated Visual Discourse Synthesis[C]//Proc. CHI98 Conference, ACM Press，1998:392-399.   \nB8Amar R，Stasko J.Knowledge Task-Based Framework for Design and Evaluation of Information Visualizations[c]//Proc.IEEE Symposium on Information Visualization，2004:143-149.   \nB9]Keller P，Keller M.Visual clues:Practical Data Visualization Ml．Los Alamitos，CA，IEEE Computer Scociety Press，1992.   \n[40] Morse E，Lewis M，Olsen K A.Evaluating Visualizations:Using a Taxonomic Guide[]．International Journal of HumanComputer Studies，2000，53（5)：637-662.   \n[41]North C，Shneiderman BEN. Snap-together Visualization:Can Users Construct and Operate Coordinated Visualizations[.International Journal of Human-Computer Studies，2Ooo，53（5)： 715-739.   \n[42]J.-D.Fekete，Plaisant C.Infovis 2003 Contest[EB]．http:// www.cs.umd.edu/hcil/iv03contest/，2003.   \n[43]Koua EL，Maceachren A，Kraak MJ.Evaluating the Usability of Visualization MethodsinanExploratory GeovisualizationEnvironment[].International Journal of Geographical Information Science，2006，20（4):425-448.   \n[4L-C.Wu，Vakkari P.Supporting Navigationin Wikipedia By Information Visualization:Extended Evaluation Measures[．Journal of Documentation，2014，70（3)：392-424.   \n[45]Ghorbel F，Elouze N，Metais E，et al. MEMO GRAPH:An Ontology Visualization Tool for Everyone[．Procedia Computer Science，2016，96: 265-274.   \n[46]Chen C，YuYUE.Empirical Studies of Information Visualizatin:AMetaanalveisllIntermatinal Iumalnf Hman-Com"
  }
}