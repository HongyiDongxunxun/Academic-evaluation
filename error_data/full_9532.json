{
  "original_filename": "full_9532.md",
  "词汇表示学习研究进展": {
    "context1": "潘俊1,²，吴宗大²",
    "context2": "(1．浙江科技学院理学院，杭州310023；2．温州市波普大数据研究院，温州325035）",
    "context3": "摘要词汇语义表示是自然语言理解的基础。传统的基于语义词典的编码表示构建成本高昂，而独热表示又存在高维稀疏等缺点。词汇的分布式表示将词汇映射为低维稠密的实值向量，能有效捕捉词汇间的语义关联，是当前主流的表示技术。本文从数据特征、学习目标和优化算法三个方面，对现有的词汇表示学习方法进行了全面深入的分析，重点介绍了这些方法的理论基础、关键技术、评价指标及应用领域。此外，本文还总结了该方向面临的主要挑战以及最新研究进展，并对词汇表示学习未来的发展方向做了展望。",
    "context4": "关键词词汇表示；表示学习；词向量；分布式表示；深度学习"
  },
  "A Review of Word Representation Learning": {
    "context1": "Pan Jun1² and Wu Zongda²",
    "context2": "(1.School of Science, Zhejiang University of Science and Technology, Hangzhou310023; 2.Wenzhou Popper Big Data Research,Wenzhou 325035)",
    "context3": "Abstract:Word representation thatreflects semantic meaning is fundamental to natural language understanding tasks.The traditional methodof encodinga word through a semantic dictionary is impractical due to the high construction cost,and one-hotrepresentation sufers from various defects,such as high dimension anddata sparsity.Distributed word representation,which projects the words into vectors inalow-dimensional real-valued space,cancapture the semanticrelatedness between the words and has been widelyused in many NLPtasks.In this paper, we present an in-depth study of word representation learning methods fromthe perspectives of input data,learning objectives,and optimization algorithms,focusing onthetheoretical basis,keytechniques,evaluation methods,andapplication fields.We then summarize the main challenges and the latest advances in this research field,and we finally discuss possible future work in the field.",
    "context4": "Key words: word representation; representation learning; word vector; distributed representation; deep learning"
  },
  "1引言": {
    "context1": "可表示成 $[ 0 , \\cdots , 1 , 0 , \\cdots , 0 ]$ 及 $[ 0 , \\cdots , 0 , 1 , \\cdots , 0 ]$ 。独热编码的词向量两两正交，无法表达词汇间的语义关系。为此可以借助语义词典，如基于《同义词词林》,将“苹果”与“梨”表示成“Bh07A14”和“Bh07A15”，通过层次编码计算词汇的语义相关度。然而，语义词典在构建成本、覆盖范围和信息更新等方面存在诸多局限，基于树结构的语义计算",
    "context2": "词汇是具有明确语义的基本语言单元，如何将词汇表示成适合机器处理的形式，是自然语言理解的核心问题。一种直观的形式是独热（one-hot）表示，即将词汇表示成词表长度的向量，词汇序号所在维的值为1，其余维为0，如“苹果”和“梨”",
    "context3": "复杂度也较高[。另一种方式是使用低维实向量，如将“苹果”和“梨”表示成 $[ 0 . 2 2 , \\cdots , 0 . 6 3 , \\cdots , 0 . 0 1 ]$ 及 $[ 0 . 2 1 , \\cdots , 0 . 7 4 , \\cdots , 0 ]$ 的形式。这种表示又称词汇的分布式表示，其优点是可直接计算语义相关度，并具有一定的泛化能力[2]。图1是在《人民日报》语料上使用 SGNS（skip-gram with negative sampling）模型[3训练所得的词向量，经PCA降维后的可视化结果，可直观看出“情报学”与近邻词的语义相关性。此外，利用代数运算或神经网络，还可将词向量合成为不同粒度的语言单位[4-10]。",
    "context4": "分布式表示是当前主流的词汇表示形式，但其他两种表示方式并非已经过时。实际上，在构建分布式表示的学习模型时，就常把词汇表示成独热向量作为输入；同时，一些模型会在目标函数中引入语义知识作为先验信息[1I-17]，或在输出层借助语义词典构建二叉树[18-19]以加快训练速度。",
    "context5": "本文主要讨论词汇的分布式表示。从发展脉络来看，用向量表示词汇的思想最早可追溯到Salton等[20]于1975 年提出的向量空间模型（vector spacemodel，VSM），他们构建了一个词汇与文档的共现矩阵，并使用词频和逆向文档频率（TF-IDF）计算词汇与文档的语义相关度。由于共现矩阵过于高维稀疏，后来陆续提出了以隐语义分析（latent semanticanalysis，LSA）为代表的各种矩阵分解模型[21-25]。",
    "context6": "但LSA缺乏概率意义上的解释，Hoffmann[26]和Blei等[27]为此又先后提出了概率隐语义分析（probabilis-tic latent semantic analysis，pLSA）及其贝叶斯版本LDA（latentDirichletallocation）等主题模型。矩阵分解模型和主题模型的本质都是寻求共现矩阵的低维隐主题空间，有研究表明二者之间存在某种等价性[28-29]。不同于上述工作，Bengio等[30]提出的神经网络概率语言模型（neural probabilistic languagemodel，NPLM）采用了预测的方式，将词向量作为模型参数来迭代学习，近年来提出的诸多模型如C&W[31]、LBL[32]、RNNLM[3]、CBOW[2-3]、Skip-Gram[2-3]、Glove[34]等，都在一定程度上借鉴了NPLM的思想。综观现有的词汇表示学习方法，基本上可归为两类：基于共现矩阵的统计或基于神经网络的预测。这两类方法的某些具体模型在特定条件下已被证明是等价的[35-36]，但是否存在一个通用数学框架能统一这两类方法，目前尚缺乏相关洞察。",
    "context7": "近年来，随着深度学习在自然语言处理领域的广泛应用[37]，词汇表示学习的研究方兴未艾，在融合先验知识[1I-17]、含义表示[38-47]、融合语言学特征[48-54]、多语言与多模态学习[55-61]、概率密度表示[62-67]、通用词嵌入表示[69-72]等方向取得了众多进展；此外，这些词汇表示也在语义关系挖掘[73-78]、计算社会学[79-87]、文献分析的相关任务如主题挖掘[88]、内容抽取[89-90]、语义查询[91]中得到广泛应用。然而，该领域也面临着诸多挑战，仍有许多问题需要继续探索。本文将从理论基础、关键技术、应用领域等方面对现有工作进行一次全面深入的分析，总结该领域的最新进展和未来的研究方向，以期为进一步研究提供参考。",
    "context8": "![](images/f0549cc6717314ce44da25c494273de4ab5dfcf7fd0f4f0f27bbdf179249e847.jpg)  \n图1在《人民日报》语料上训练的词向量可视化"
  },
  "2词汇表示学习的主要内容": {
    "context1": "关于词汇语义的本质，代表性学说主要有指称说、观念说、关系说、使用说等[92]，其中使用说主张：词汇的用法即词义，建立在使用说基础上的分布式假设[93-94]据此认为，词汇的语义可以由其所在的上下文来推断。研究者基于此假设提出了众多词汇表示学习的方法，通常包含3项内容：数据输入、学习目标以及优化方法。"
  },
  "2.1数据输入与特征": {
    "context1": "词汇表示学习的输入主要是词汇的上下文，各类模型对上下文有不同的定义：矩阵分解模型[21-25]和主题模型[26-27]的上下文通常指整个文档，关注全局信息；NPLM[30]等神经网络模型的上下文一般指滑动窗口内的词，关注局部信息；Glove[34]、Huang等[95]模型的上下文则同时关注全局和局部的统计信息。此外，还有多种特征可作为数据输入： $\\textcircled{1}$ 形态学特征[96-103]，如前后缀、词元、词干、偏旁部首等； $\\textcircled{2}$ 语言学特征[104-106]，如词性、语法成分树、语义角色、句法依存树、回指等； $\\textcircled{3}$ 语义关系特征[1-17]，如同义、反义、上下位关系等； $\\textcircled{4}$ 其他如词序、多语种、字符上下文等特征[107-114]。",
    "context2": "数据输入不同，训练得到的词向量的性质也不同[115]。范围较广的上下文，较易得到具有主题相关性的词向量（如“北京”的近邻词为“烤鸭”、“故宫”等），句法依存上下文容易得到的具有功能相似性的词向量（如“北京”的近邻词为“上海”、“杭州”等)，范围较小的上下文，生成的词向量则介于二者之间。因此，选择什么样的特征及组合作为数据输入，往往要视具体的任务而定[7]。"
  },
  "2.2学习模型与目标": {
    "context1": "有了输入，还需设定学习目标来判定训练效果。依据目标函数的类型，可将学习模型大致分为三类。 $\\textcircled{1}$ 数据降维：给定词与上下文的共现矩阵，目标是学得对应的低维空间，相关方法有奇异值分解[21]（singular value decomposition，SVD）、典型关联分析[23-24]（canonical correlation analysis，CCA）、非负矩阵分解[25]（non negative matrix factorization,NMF）等。 $\\textcircled{2}$ 密度估计：给定 $n$ 个观测值$x _ { 1 } , x _ { 2 } , \\cdots x _ { n }$ ，目标是学得密度函数 $p ( x )$ 。若此时已知 $p ( x )$ 所在的分布族 $p ( x | \\theta )$ ，而对模型参数 $\\theta$ 一无所知，可通过最大似然估计得到 $\\theta$ ，典型如pLSA模型[26]；若知道 $\\theta$ 服从某个分布，则可通过贝叶斯估计得到 $\\theta$ ，典型如LDA模型[27]。 $\\textcircled{3}$ 对数线性分类：给定中心词及其上下文的 $n$ 个观测值，目标是学得一个Softmax多分类预测函数，使该函数对上下文（或中心词）的预测损失最小，相关方法包括NPLM[30]、CBOW[2-3]、Skip-Gram[2-3]、LBLM[32]等。由于学习任务的词表通常很大，在构建目标函数时，应适当缩小假设空间，并尽量减小算法的搜索空间。"
  },
  "2.3模型优化与算法": {
    "context1": "学习模型定义了从假设空间搜索最优表示的目标函数，为了尽量得到最优的词汇表示，需要设计或选择合适的优化方法。典型算法有梯度下降法、牛顿迭代法、共轭梯度法、拉格朗日乘子法、随机逼近法等。为了在性能和效率之间取得平衡，在选择优化算法时，要同时考虑算法的理论保证和工程实现。例如，pLSA采用期望最大算法（expectationmaximization，EM）来估计模型的参数[26]，LDA可选用变分贝叶斯推断[27]、期望传播[116]或吉布斯采样[17]来估计参数；基于神经网络的预测模型通常使用误差反向传播随机梯度下降算法及其变种[30]。此外，参数初始化、加速技术以及超参设定等，都有可能对训练的结果产生影响[115]。"
  },
  "3词汇表示学习的主要方法": "",
  "3.1基于共现统计的表示学习": {
    "context1": "基于共现统计的表示学习的基本思想，是通过构建词-上下文共现矩阵，将词汇映射为语义空间的一个点。由于共现矩阵的高维稀疏，又利用矩阵分解技术或概率图模型，以求得该矩阵的低维表示。形式化地，令 $V _ { w }$ 表示词表， $V _ { c }$ 表示上下文集合， $f ( w _ { i } , c _ { j } )$ 表示词 $w _ { i }$ 与上下文 $c _ { j }$ 的相关度，$\\pmb { M } \\in \\mathbb { R } ^ { | V _ { w } | \\times | V _ { c } | }$ 为共现矩阵，其元素由函数 $f$ 确定， $M ^ { \\prime }$ 为降维后的矩阵。依据对上下文集合 $V _ { c }$ 、相关度函数 $f _ { \\nu }$ 降维方法 $g \\colon M \\to M ^ { \\prime }$ 的不同定义，有以下几种",
    "context2": "代表性模型。"
  },
  "3.1.1向量空间模型": {
    "context1": "Salton等[20]针对文本检索应用提出了向量空间模型，该模型构建一个查询项-文档共现矩阵，其中查询项指能表征文档的字、词、短语等基本语言单位，文档则泛指整篇文档或描述该文档的元数据，如标题、摘要、简介等，相关度函数采用TF-IDF 及其变形公式。向量空间模型的优点是可以通过余弦公式直接计算查询项和文档的相关度。其局限性在于，共现矩阵是基于文档词频构建的，主要针对文档检索，对词汇语义的刻画有所不足。",
    "context2": "Lund 等[119]提出的 HAL（hyperspace analogue tolanguage）模型依据语料构建了一个词-词共现矩阵，词汇相关度定义为滑动窗口的长度减去词的间隔，其含义是两个词在给定窗口下关联程度。该模型得到的词向量的维度为整个词表的大小，存储和处理这样的高维向量非常困难，这也是此类早期模型的主要缺陷。"
  },
  "3.1.2矩阵分解模型": {
    "context1": "Deerwester等[2I]提出的隐语义分析（LSA）试图对共现矩阵进行奇异值分解，构建一个低维隐语义空间，并将文档和词汇都映射到该空间。奇异值分解的过程如图2所示，高秩共现矩阵 $M$ 被分解为3个 $k$ 秩矩阵，即 $M \\approx U \\sum _ { k } V ^ { \\mathrm { T } }$ ，其中 $U$ 和 $V$ 为正交单位阵， $\\sum _ { k }$ 为前 $k$ 个最大奇异值降序排列的对角阵。分解后可得低维文档向量矩阵 $\\sum _ { k } V ^ { \\mathrm { T } }$ ，以及词向量矩阵 $U \\sum _ { k }$ 。",
    "context2": "![](images/24c7c571cb1768c14e52d8aef122a44208d0f34148f122f4ba6f13fb04d51130.jpg)  \n图2LSA对共现矩阵的奇异值分解",
    "context3": "Schutze等[22]随后提出的WordSpace模型同样使用了奇异值分解，但他们以四元字符为单位构造共现矩阵。该方法先对共现矩阵作奇异值分解并得到四元字符的向量表示，然后统计每个单词在四元字符上的分布情况，最后用四元字符向量的加权平均作为每个词的向量表示。",
    "context4": "Dhillon等[23-24]使用典型关联分析（CCA）来对共现矩阵降维。直观地，目标词的上文和下文在语义上应紧密相关，可考虑在表示空间中为其建立关联。为此构造两个共现矩阵，词-上文矩阵和词-下文矩阵，再最大化它们投影后的协方差。具体地，设语料中词语个数为 $n$ ，滑动窗口大小为 $2 h$ ，词向量维度为 $k$ ， $\\boldsymbol { W } \\in \\mathbb { R } ^ { n \\times | V _ { w } | }$ 表示词-词共现矩阵，$\\boldsymbol { L } , \\boldsymbol { R } \\in \\mathbb { R } ^ { n \\times | V _ { w } | h }$ 表示词-上文和词-下文共现矩阵。 $\\phi _ { L } , \\phi _ { R } \\in$ $\\mathbb { R } ^ { | V _ { w } | h \\times k }$ 表示投影矩阵，令 $\\pmb { L } ^ { \\prime } = \\phi _ { L } \\pmb { L } , \\pmb { R } ^ { \\prime } = \\phi _ { R } \\pmb { R }$ ，CCA$^ { ( L , R ) }$ 的目标是最大化 $\\pmb { L ^ { \\prime } }$ 和 $\\pmb { R } ^ { \\prime }$ 的相关系数",
    "context5": "$$\n\\operatorname * { a r g m a x } _ { \\phi _ { L } , \\phi _ { R } } \\frac { \\operatorname { c o v } \\left( L ^ { \\prime } , R ^ { \\prime } \\right) } { \\sqrt { D \\left( L ^ { \\prime } \\right) } \\sqrt { D \\left( R ^ { \\prime } \\right) } }\n$$",
    "context6": "其中， $\\mathrm { c o v } \\left( { \\cal L } ^ { \\prime } , { \\cal R } ^ { \\prime } \\right)$ 为 $\\pmb { L ^ { \\prime } }$ 和 $\\pmb { R } ^ { \\prime }$ 的协方差， $D ( \\pmb { L } ^ { \\prime } )$ 和$D ( R ^ { \\prime } )$ 为 $L ^ { \\prime }$ 和 $\\pmb { R } ^ { \\prime }$ 的方差。求得 $\\phi _ { L }$ 和 $\\phi _ { R }$ 后，令 $s$ 为 $\\pmb { L } ^ { \\prime }$ 和 $\\pmb { R } ^ { \\prime }$ 的拼接矩阵，再次计算 $\\mathrm { C C A } ( S , W )$ ，得投影矩阵 $\\phi _ { s }$ 和 $\\phi _ { \\scriptscriptstyle W }$ ，其中 $\\phi _ { w } \\in \\mathbb { R } ^ { | V _ { w } | \\times k }$ 即为词向量矩阵。",
    "context7": "Lee等[25]则采用非负矩阵分解对词-文档共现矩阵进行降维。分解公式为 $M = W T$ ，其中矩阵 $T$ 为主题词频矩阵，矩阵 $W$ 为主题权重矩阵， $W$ 的维数值 $k$ 表示隐主题的个数。分解得到的 $W$ 的每一行表示一个文档，可以理解为该文档在每个主题下的权重，而主题词频矩阵 $T$ 的每一列表示一个词，即该词在 $k$ 个主题下的词频。",
    "context8": "总体而言，矩阵分解方法通过寻求高秩共现矩阵的低秩逼近，能同时得到文档和词汇的向量表示，缺点是分解过程缺乏概率意义上的解释。"
  },
  "3.1.3概率图主题模型": {
    "context1": "LSA的思想为主题模型的发展奠定了基础。主题模型假设文档具有主题概率分布，各个主题又具有词汇概率分布，通过参数估计可确定这两个分布。Hoffman[2提出的pLSA被认为是第一个真正意义上的主题模型。令 $d$ 表示文档， $w$ 表示词， $z$ 表示隐主题，pLSA生成文档的过程是：以概率 $p ( d )$ 选择一个文档 $d$ ，对 $d$ 中的每个词 $w$ ，重复以下过程：$\\textcircled{1}$ 以概率 $p ( z | d )$ 选择一个隐主题 $z$ ； $\\textcircled{2}$ 以概率$p ( w | z )$ 选择一个词 $w$ 。 $( d , w ~ )$ 的联合概率为",
    "context2": "$$\np \\left( d , w \\right) = p \\left( d \\right) \\sum _ { z \\in Z } p \\left( w | z \\right) p \\left( z | d \\right)\n$$",
    "context3": "其中， $w$ 和 $d$ 为观测值， $p ( z | d )$ 和 $p ( w | z )$ 是待估计的参数，分别为文档向量矩阵和词向量矩阵。pLSA通过EM算法估计参数：E步骤估计隐变量 $z$ 的后验概率，M步骤利用上一步的结果更新参数。",
    "context4": "pLSA的思想属于频率学派，即假定存在确定但未知的参数，通过观测值对参数作最大似然估计，容易造成过拟合。Blei等[27提出的LDA可视作plSA的贝叶斯化。令 $\\theta$ 为文档主题分布，且服从参数为 $\\alpha$ 的Dirichlet分布， $\\varphi$ 为主题词汇分布，LDA生成文档的过程为：采样一个主题分布$\\theta { } _ { \\mathbf { \\Gamma } }$ Dirichlet $( a )$ ，按以下步骤生成文档 $d$ 中的 $N _ { d }$ 个词。 $\\textcircled{1}$ 从主题分布中为词指派一个主题 $z _ { d , n }$ ； $\\textcircled{2}$ 以概率 $p \\left( w _ { d , n } | z _ { d , n } , \\varphi \\right)$ 生成这个词。则整个语料的概率$p ( D | a , \\varphi )$ 为",
    "context5": "$$\n\\prod _ { d = 1 } ^ { D } \\int p \\left( \\theta _ { d } | \\alpha \\right) ( \\prod _ { n = 1 } ^ { N _ { d } } \\sum _ { z _ { d , n } } p \\left( z _ { d , n } | \\theta _ { d } \\right) p \\left( w _ { d , n } | z _ { d , n } , \\varphi \\right) ) \\mathrm { d } \\theta _ { d }\n$$",
    "context6": "式(3)难以直接求解，实践中常采用变分法[27]、期望传播法[116]来近似估计。在文献[27]的基础上，Griffiths等[117]又对参数 $\\varphi$ 施加Dirichlet先验，构建了更为完整的LDA文档生成模型，如图3所示。",
    "context7": "![](images/ea1312bba13986eee42a8e0d6c6b13cc74242ada06254a1e50f25b9f40a938fd.jpg)  \n图3LDA图盘模型"
  },
  "3.1.4Glove模型": {
    "context1": "上述模型构建的词-文档共现矩阵充分利用了文本的全局特征。但Pennington 等[34]认为由于共现矩阵没有考虑词汇的局部上下文，所得词向量在语义类比任务中表现不佳，为此他们提出同时考虑全局和局部特征的Glove方法。首先利用滑动窗口构建词-词共现矩阵，元素为词 $w _ { i }$ 和 $w _ { j }$ 在彼此上下文的共现次数；接着构建共现概率矩阵，直观地，词$w _ { i }$ 和 $w _ { j }$ 的相关性，可通过其与第三词 $w _ { k }$ 的共现概率之比 $p _ { i , k } / p _ { j , k }$ 来度量：若比值趋于1，则 $w _ { i }$ 和 $w _ { j }$ 要么都与 $w _ { k }$ 相关，要么都与 $w _ { k }$ 无关；若比值很小，则 $w _ { i }$ 与 $w _ { k }$ 无关而 $w _ { j }$ 与 $w _ { k }$ 相关，若比值很大则相反。Glove使用比值与距离的差平方作为损失函数：",
    "context2": "$$\nJ = \\sum _ { i , j , k } ^ { N } \\left( \\frac { p _ { i , k } } { p _ { j , k } } - \\exp { ( ( w _ { i } - w _ { j } ) ^ { \\mathrm { T } } w _ { k } ) } \\right) ^ { 2 }\n$$",
    "context3": "化简后通过随机梯度下降法优化，最终将词表示为其作为中心词和背景词的向量之和。"
  },
  "3.1.5模型的讨论": {
    "context1": "矩阵分解模型和主题模型的区别在于：前者的目标是对共现矩阵的最优低秩逼近，后者的目标是对观测值的最大似然；前者缺乏概率意义上解释，难以扩展，后者具备坚实的概率基础，模拟了文档的生成过程，也容易扩展。理论上，LSA的奇异值分解能得到非凸函数的全局最优解，而pLSA的参数估计只能得到局部最优解。工程上，pLSA使用的EM算法可纳入高效的MapReduce框架[I20]，更容易实现。然而从降维的角度看，这两类模型又都是在寻求一个低维隐语义空间，它们之间存在着某种联系：若令 $U ^ { \\prime } = p \\left( d _ { i } | z _ { k } \\right) _ { i , k }$ ， $V ^ { \\prime } = p \\left( w _ { j } \\right) \\left( w _ { j } | \\boldsymbol { z } _ { k } \\right) _ { j , k } , \\Sigma ^ { \\prime } =$ dia $\\mathrm { g } \\left( \\boldsymbol { p } \\left( \\boldsymbol { z } _ { k } \\right) \\right) _ { k }$ ，则pLSA的式(2)可表示成 $U ^ { \\prime } \\Sigma ^ { \\prime } V ^ { \\prime }$ ，分别对应于奇异值分解后的3个矩阵[26]。Gaussier等[28]证明，基于KL散度的非负矩阵分解与pLSA,均为多项式PCA框架[121]的一种特例。Ding等[29]进一步指出，pLSA和NMF的目标函数存在等价性：NMF分解的两个权重矩阵归一化后，可视作文档在主题上的分布 $p ( z | d )$ ，以及主题在词上的分布$p ( w | z )$ ，同样可用EM算法估计，只是会收敛于不同的局部最优点。他们由此提出融合NMF和pLSA的方法：当NMF收敛于某个局部最优点时，用pLSA跳出这个点，反之亦然，从而求得更优的局部解。从已有的工作来看，目前的研究还是仅限于pLSA与矩阵分解模型的联系，对于更一般的LDA模型，其与矩阵分解模型存在何种联系，尚未见到相关结论。",
    "context2": "总体而言，基于共现统计的方法理论上已较为完备，近来出现了许多关注模型性能的工作，牛奉高等[1I8]基于LSA的思想，在对共现矩阵降维的同时合并同义词。张晓娟[9利用主题嵌入获得每个主题下的上下文词分布，使用图模型对候选查询进行排序。蔡永明等[88]则在LDA模型中引入词汇社交网络，提升短文本主题划分效果，这些工作表明共现统计方法的实用性已经得到了认可，表1对部分有代表性的方法作了比较。"
  },
  "3.2基于神经网络预测的表示学习": {
    "context1": "基于神经网络的方法采用预测的方式，将词向量作为参数来训练。形式化地，令 $E$ 为嵌入矩阵，$w _ { i }$ 为独热编码向量， $\\pmb { { \\cal E } } \\left( \\pmb { { w } } _ { i } \\right) \\in \\mathbb { R } ^ { k }$ 为 $k$ 维词向量，将$\\pmb { { \\cal E } } \\left( { \\pmb { w } } _ { i } \\right)$ 作为有监督的学习任务 $f ( x \\to y$ 的输入，当 $f$ 训练完毕后，即可输出参数 $E$ 。训练语料为 $W =$ $\\left\\{ { \\pmb w } _ { 1 } \\cdots { \\pmb w } _ { T } \\right\\}$ ，通常选择目标词 $w _ { i }$ 及其上下文 $c _ { w _ { i } }$ 作为训练样本，可以由目标词预测上下文，或者相反。这里任务 $f$ 的得分函数是关键，该函数要可微分，以便通过梯度下降法来反向传播损失，同时还要使正样本的得分高于其他随机样本。依据对样本数据$( x , y )$ 、任务 $f$ 以及得分函数的不同定义，有以下几种代表性方法。",
    "context2": "表1部分有代表性的基于共现统计的学习方法比较",
    "context3": "<table><tr><td>类型</td><td>方法</td><td>上下文集合</td><td>相关性计算</td><td>目标函数</td><td>优化方法</td></tr><tr><td rowspan=\"2\">向量空间</td><td>VSM[20]</td><td>词-文档矩阵</td><td>TF-IDF</td><td>无</td><td>无</td></tr><tr><td>HAL[119]</td><td>词-词矩阵</td><td>窗口长度减去词间隔无</td><td></td><td>无</td></tr><tr><td rowspan=\"3\">矩阵分解</td><td>LSA[21]</td><td>词-文档矩阵</td><td>TF-IDF</td><td>共现矩阵主成分相关性降维</td><td>SVD矩阵分解</td></tr><tr><td>NMF[25]</td><td>词-文档矩阵</td><td>TF-IDF(非负)</td><td>共现矩阵主成分相关性降维</td><td>NMF非负矩阵分解</td></tr><tr><td>CCA[23][24]</td><td>词-左右窗口词双矩阵</td><td>TF-IDF</td><td>词-上文与词-下文矩阵相关性降维</td><td>协方差矩阵特征值分解</td></tr><tr><td rowspan=\"2\">主题模型</td><td>pLSA[26]</td><td>词-文档矩阵</td><td>TF-IDF(非负)</td><td>词-文档联合概率的最大似然</td><td>EM算法参数估计</td></tr><tr><td>LDA[27]</td><td>词-文档矩阵</td><td>TF-IDF(非负）</td><td>词-文档联合概率的最大后验</td><td>变分贝叶斯推断或吉布斯采样</td></tr><tr><td>Glove</td><td></td><td></td><td></td><td>Glove[34]词-词全局共现概率矩阵词-词共现概率比值 词-词相关度与共现概率的最小二乘回归 随机梯度下降</td><td></td></tr></table>"
  },
  "3.2.1面向语言模型的方法": {
    "context1": "这一类方法以Bengio的工作[30]为代表，目标是学得一个语言模型，得分函数以降低语言模型的困惑度（perplexity）为目标。",
    "context2": "（1）神经概率语言模型（NPLM）[30]。定义目标词的上下文为其前 $n$ 个词（ $n$ -Gram），由上下文$c _ { w _ { i } }$ 预测目标词 $w _ { t ^ { \\bigcirc } }$ 如图4所示，输入层将上下文的每个词映射为词向量并拼接，得到 $\\pmb { x } = [ E ( \\pmb { w } _ { t - n + 1 } ) ; \\cdots ;$ $E \\left( \\boldsymbol { w } _ { t - 1 } \\right) \\big ]$ ； $x$ 由矩阵 $\\pmb { H }$ 转换并经函数 $\\operatorname { t a n h } ( \\cdot )$ 激活后，生成隐向量 $\\pmb { h }$ ； $\\pmb { h }$ 再经矩阵 $\\pmb { U }$ 转换，并与跨层转换的向量 $\\varrho x$ 相加；最后通过Softmax函数进行归一化，则词表中每个词为下一词的概率为$p \\left( \\pmb { w } _ { t } | \\pmb { c } _ { w _ { i } } \\right) = \\mathrm { S o f t m a x } \\left( b + \\pmb { Q x } + U \\mathrm { t a n h } \\left( d + H \\pmb { x } \\right) \\right.$ )，模型通过梯度下降法迭代计算参数 $\\theta = \\left\\{ b , Q , U , d , H , E \\right\\}$ D",
    "context3": "![](images/88fe5aaea0985b87a2a9434364e1c9399eb271a162085d8f2525dadf35d5a01e.jpg)  \n图4概率神经网络语言模型",
    "context4": "在图4中，输出层将隐向量 $\\pmb { h }$ 与权重矩阵 $\\pmb { U }$ 点积，以预测每个词作为下一词的概率。由于相似的输出将导致相似的输入，因此 $x$ 中的词在嵌入空间会趋于接近，这个过程是网络通过自我参数调整得到的。值得注意的是，作为一种通用的表示学习框架，NPLM的输入层、隐藏层和输出层都可根据具体情况定制，这为后续的一些工作奠定了重要基础。",
    "context5": "（2）能量函数语言模型[32]。NPLM模型属于判别式模型，输出层的Softmax函数直接对条件概率建模，Bengio等[30]同时也从生成式的角度，提出了基于能量最小化的模型，其思想是先对目标词和上下文的联合概率建模，再求其边缘概率的最大似然。Mnih等[32]进一步提出三个能量函数语言模型，以其中的受限玻尔兹曼机为例，观测值 $\\left( \\pmb { w } _ { 1 : n - 1 } , \\pmb { w } _ { n } \\right)$ 的能量函数 $E \\left( \\boldsymbol { w } _ { n } , \\boldsymbol { h } ; \\boldsymbol { w } _ { 1 : n - 1 } \\right)$ 为",
    "context6": "$$\n- \\Bigg ( \\sum _ { i = 1 } ^ { n } E \\left( \\pmb { w } _ { i } \\right) \\pmb { V } _ { i } \\Bigg ) \\pmb { h } - \\pmb { b } ^ { \\operatorname { T } } \\pmb { h } - \\pmb { d } ^ { \\operatorname { T } } E ^ { \\operatorname { T } } \\left( \\pmb { w } _ { n } \\right) - \\pmb { r } ^ { \\operatorname { T } } \\pmb { w } _ { n }\n$$",
    "context7": "其中向量 $\\pmb { b }$ 、d、 $r$ 分别为隐变量 $\\pmb { h }$ 、词向量 $E ^ { \\mathrm { { T } } } { \\left( { \\boldsymbol { w } } _ { n } \\right) }$ 和可见变量 $w _ { n }$ 的偏置，矩阵 $V _ { i }$ 为词向量 $E \\left( w _ { i } \\right)$ 与隐变量 $\\pmb { h }$ 的交互，表示每个词对要预测的词 $w _ { n }$ 的贡献，参数为 $\\pmb { \\theta } = \\{ E , V , b , d , r \\}$ 。能量函数经归一化后即为目标词与上下文的联合分布，进而可求语言模型的边缘分布，通过在语料上求该分布的最大似然可得到 $\\pmb \\theta$ 。",
    "context8": "受限玻尔兹曼机的训练较慢，文献[32]为此移除隐变量层，提出对数双线性模型（LBLM），式(5)简化为",
    "context9": "$$\n- \\Bigg ( \\sum _ { i = 1 } ^ { n - 1 } E \\left( \\pmb { w } _ { i } \\right) \\pmb { V } _ { i } \\Bigg ) E ^ { \\operatorname { T } } \\left( \\pmb { w } _ { n } \\right) - d ^ { \\operatorname { T } } E ^ { \\operatorname { T } } \\left( \\pmb { w } _ { n } \\right) - r ^ { \\operatorname { T } } \\pmb { w } _ { n }\n$$",
    "context10": "式(6)将前 $( n - 1 )$ 个词依次用矩阵 $V _ { i }$ 转换，得到上下文向量，再与目标词 $E ^ { \\mathrm { { T } } } \\left( \\boldsymbol { w } _ { n } \\right)$ 作点积来计算相关度，物理意义清晰，计算复杂度大幅降低。这种简化的思路也启发了Mikolov等[2-3]的工作，后者进一步移除了矩阵 $V _ { i }$ ，直接计算上下文与目标词的相关度。",
    "context11": "（3）循环神经网络语言模型（RNNLM）[33,122]。",
    "context12": "观察NPLM的输入层可知，其长度由 $n$ -Gram的 $n$ 值固定，模型复杂度和 $n$ 值呈指数关系。为了更有效地利用前文信息，Mnih等[32]提出了时序受限玻尔兹曼机模型，滑动窗口的大小不再固定。受此启发，Mikolov等[33,122]将NPLM隐层的前馈神经网络替换为循环神经网络[123]，隐层按公式 $\\pmb { h } _ { t } =$ sigmoid $( E \\left( \\pmb { w } _ { t } \\right) + \\pmb { H } \\pmb { h } _ { t - 1 } )$ 迭代，每一时刻的隐向量$\\pmb { h } _ { t }$ ，由当前词向量与上一时刻隐向量 $\\pmb { h } _ { t - 1 }$ 经非线性激活后得到，最终得到一个编码了历史输入信息的定长向量，该模型采用BPTT算法[124]来训练。从编码-解码的角度看，RNNLM能将不定长的输入编码成定长的状态向量并解码，架构上最为完整，在得到词汇表示的同时，还可得到句子级表示，然而RNN会面临梯度消失和梯度爆炸问题[125]。目前这方面的最新进展是Jozefowic 等[l]的工作，他们引入字符卷积、LSTM、CNN-Softmax等多项技术来提升RNNLM的性能。"
  },
  "3.2.2面向词向量模型的方法": {
    "context1": "面向语言模型的目标是预测一个词出现在给定词序列之后的概率，受到的约束有两个方面： $\\textcircled{1}$ 训练数据需为 $\\left( \\pmb { w } _ { 1 : n - 1 } , \\pmb { w } _ { n } \\right)$ 的格式； $\\textcircled{2}$ 输出层需用Soft-max 函数计算整个词表的归一化概率。面向词向量模型对此约束作了不同程度的松弛。",
    "context2": "(1）C&W排序模型[31]。训练数据为目标词 $w _ { i }$ 及其左右窗口内的词 $c _ { w _ { i } }$ ，输出层只计算训练数据$( w _ { i } , c _ { w _ { i } } )$ 的得分。该模型先用嵌入矩阵映射 $( w _ { i } , c _ { w _ { i } } )$ 并拼接成向量 $_ { x }$ ， $_ { x }$ 经权重矩阵 $U$ 转换后由非线性函数激活，最后与向量 $\\nu$ 作点积，其得分表示目标词与上下文的契合程度，损失函数为",
    "context3": "$$\n\\sum _ { ( w , c ) \\in D w ^ { \\prime } \\in V _ { w } } \\operatorname* { m a x } \\left\\{ 0 , 1 - s _ { \\theta } \\bigl ( w , c \\bigr ) + s _ { \\theta } \\bigl ( w ^ { \\prime } , c \\bigr ) \\right\\}\n$$",
    "context4": "其中 $\\scriptstyle ( w , c )$ 为正样本， $\\left( w ^ { \\prime } , c \\right)$ 为负样本， $w ^ { \\prime }$ 为从词表中随机取的词， $s = \\operatorname { t a n h } \\left( x U \\right) \\cdot \\nu$ 为得分函数，要学习的参数为 $\\pmb { \\theta } = \\left\\{ E , U , \\pmb { \\nu } \\right\\}$ 。",
    "context5": "Huang等[95]进一步引人文档的全局信息（全部词向量的加权平均）。如图5所示，以目标词左窗口内的词作为局部上下文，以所在文档作为全局上下文，分别计算目标词与两个上下文的得分。实验表明生成的词向量具有更为明确的语义。",
    "context6": "（2）CBOW和 Skip-Gram[2-3]。NPLM在输入层和输出层之间加了直连边（图4)，目的是在反向传播误差时，部分误差可以不经隐层直达输入层。Mikolov 等[2-3]提出CBOW和Skip-Gram模型移除整个非线性隐层，只保留直连边。这两个模型对上下文的定义与C&W相同，但CBOW由上下文预测目标词，Skip-Gram则相反。如图6所示，CBOW对目标词的上下文直接求和，由权重矩阵 $\\pmb { U }$ 计算词表中每个词的得分再作归一化，即 $p \\left( w | h \\right) { = } \\mathrm { S o f t m a x } \\left( U h \\right)$ ，其中 $\\pmb { { h } } = \\sum _ { - n \\leqslant j \\leqslant n , \\neq 0 } \\pmb { { E } } \\left( \\boldsymbol { w } _ { j } \\right)$ 。Skip-Gram假设上下文各个词相互独立，其目标函数为目标词 ${ \\pmb w } _ { t }$ 左右 $n$ 个词的概率之和,",
    "context7": "![](images/dec64936633e86418b915819e3ef9aa8337a568b441577cca3884b13b96c0646.jpg)  \n图5Eric Huang模型的结构",
    "context8": "$$\nJ _ { \\theta } = \\frac { 1 } { T } \\sum _ { t = 1 } ^ { T } \\sum _ { - n \\leqslant j \\leqslant n , \\neq 0 } \\log p \\left( \\pmb { w } _ { t + j } \\middle | \\pmb { w } _ { t } \\right)\n$$",
    "context9": "这两个模型结构对称，而且和LBLM[32]很类似，都属于对数线性模型，在构造logits时都直接对两个向量作了点积。LBLM认为计算两个向量相关度时每一维的权重不同，为此对每个输入施加了一个$V _ { i }$ 矩阵转换（式(6)），而CBOW和Skip-Gram直接移除了矩阵 $V _ { i }$ ，计算速度大幅提升，得以在更大规模的语料上训练。此外，向量点积的物理意义即为余弦夹角，这种简化的线性模型，反而使得生成的词向量具有了可线性运算的性质。",
    "context10": "![](images/38ebeb98e9f503af377bd0eec89b64f08f1e466c5d1c9abd522a1e6ae39fe0e6.jpg)  \n图6CBOW和 Skip-Gram模型"
  },
  "3.2.3对上下文的扩展": {
    "context1": "上下文的选择对生成的词向量的特性有着重要影响[1I5]，近年来出现的许多改进工作聚焦于对上下文的扩展，主要有两种思路： $\\textcircled{1}$ 将线性上下文改为非线性； $\\textcircled{2}$ 构建不同粒度的上下文。",
    "context2": "滑动窗口上下文是线性的，改变线性结构的一种方法引入依存句法信息，Levy等[104]在构建训练数据时，用句法树中的邻近词及其句法关系作为上下文，缓解了噪声和信息丢失现象。刘永彬等[105]在句法依存关系的基础上，通过共指消解建立句间关联，使目标词的上下文具备跨句取词的能力。Bansal[10]同样使用句法上下文，实验发现较大的窗口易得到主题相关的词向量，较小的窗口易得到语法语义相近的词向量。另一种改进方式是考虑词序，如Wang等[107]针对CBOW，通过拼接上下文来代替求和上下文，针对Skip-Gram，为每个上下文都分别定义一个词嵌入矩阵，从而保留了词序信息。",
    "context3": "词汇可看作是字或字符的序列，而序列中蕴含着的词素信息和拼写信息有助于推断词的语义[108]。Santos等[109]使用卷积神经网络学习字符向量，并在C&W的框架下与词向量合并后使用。Kim等[110]将词分解为字符后输入神经网络语言模型，通过不同的卷积核得到不同的词向量，拼接成长向量后依次经Highway层和LSTM层，最后由Softmax函数预测下一词，该模型缓解了未登录词的表示问题。Cao等[102]基于 Skip-Gram模型，将词视作字符序列，通过双向LSTM将词编码为隐向量，并基于注意力机制训练词向量。Jozefowicz等[1i]进一步在Softmax层和隐层内积中使用字符卷积神经网络，减小了存储压力。",
    "context4": "由于单个字符一般不具语义，另一些工作尝试将词分解为字符 $n$ -Gram[22]，如Wieting等[112]、Bo-janowski等[13]将词表示为一组字符 $n$ -Gram向量的和。Chen等[114]提出了针对中文词向量的字词联合学习，同时通过上下文与构成该词的汉字来学习语义。还有一种思路是依据形态学知识将词分解为词缀和词元组合，词元部分刻画词汇的语义相近性，词缀部分刻画词汇的形态相近性，这类工作采用不同方式对词汇进行分解和组合，并应用于不同的模型如LBLM[98]、CBOW[99]、Skip-Gram[100-101,113]等。此外，中文的偏旁部首与英文的词缀词根很有相似之处，可以利用偏旁部首等形态学信息训练词向量[49-52]，但中文偏旁部首演化变化很大，且存在象形、指事、形声、会意等多种构字方式，目前仍面临不少挑战。"
  },
  "3.2.4对输出层的扩展": {
    "context1": "神经网络语言模型的开销主要在输出层。Soft-max归一化时隐向量要与整个词表作运算，实践中会对输出层作一些扩展以提升训练速度，主要有两种思路： $\\textcircled{1}$ 重构输出层； $\\textcircled{2}$ 利用采样近似求解。",
    "context2": "Goodman[126]将词表中的词分成 $k$ 类，设词 $w$ 的类别为 $c ( w )$ ，则 $p \\left( \\pmb { w } | \\pmb { h } \\right) ) = p \\left( c \\left( \\pmb { w } \\right) | \\pmb { h } \\right) \\times p \\left( \\pmb { w } | c \\left( \\pmb { w } \\right) , \\pmb { h } \\right) \\circ$ 以两层为例，若将词表按 $k = \\sqrt { | V _ { w } | }$ 均分，速度可提升 $\\sqrt { \\vert V _ { \\scriptscriptstyle w } \\vert } / 2$ 倍。随后提出的一些模型在训练时都采用了层次化Softmax 的思想，如Mikolov 等[122]在RNNLM的输出层中就引入分类先验，依据词频将词表中的词分到各类中。Morin等[19]则借助WordNet的上下位关系为词表构建平衡二叉树，令叶子节点表示词，非叶子节点表示类别隐变量，则 $p \\left( w | h \\right)$ 为从根节点到叶子节点 $\\boldsymbol w$ 这条路径的概率，多分类计算于是转换为 $\\log \\lvert V _ { w } \\rvert$ 个二分类问题，考虑到 Soft-max函数是Logistic 函数的扩展，每一层可用Logis-tic 函数进行二分类计算，计算复杂度因此从$O ( | V _ { w } | )$ 降到了 $O ( \\log | V _ { w } | )$ 。然而，人工构建二叉树成本很高，Mnih等[127]针对LBL提出了HLBL方法，采用bootstrapping方式构建二叉树：先为词表随机生成一棵二叉树并训练词向量，再对词向量进行层次聚类，重新生成二叉树并训练。Mikolov等[128]则为 CBOW和 Skip-Gram 模型构建了哈夫曼二叉树，进一步提升了训练效率。",
    "context3": "Softmax函数的计算集中在分母，上述工作的思路是通过树状分层来降低分母的计算复杂度。Chen 等[129]提出的分区 Softmax（differentiated soft-max），则采用了扁平化的重构思路，其直觉是低频词没必要采用与高频词同样多的参数来拟合，他们将词表依据词频分块，为输出层构建一个稀疏嵌入矩阵，在Softmax计算时，隐向量 $\\pmb { h }$ 只与嵌入矩阵的非零维作点积，虽然仍要与整个词表作运算，但由于参数减少，计算代价大幅降低。",
    "context4": "利用采样技术也可逼近Softmax计算，观察神经网络模型的训练过程可知，每次更新产生的损失主要有两项：目标词的得分以及整个词表的梯度期望，计算主要发生在后一项，这可以通过采样技术来估计。Bengio等[130]使用重要性采样来估计，选取一个容易采样的分布 $\\mathcal { Q }$ 来逼近待估计的分布 $P$ ，他们使用蒙特卡罗方法来估计梯度期望。由于重要性采样的效果依赖于分布 $\\boldsymbol { \\mathcal { Q } }$ 的选取，文献[131]提出了自适应采样，先不固定 $n$ -Gram的 $n$ 值，而是在训练中自适应调整，其目标是最小化建议分布 $\\boldsymbol { \\mathcal { Q } }$ 与目标分布 $P$ 之间的KL散度。噪声对比估计[132]（NCE)是另一种逼近方法，其思想是将密度估计问题转为二分类问题。Minh等[133]引入噪声分布，为每个真实样本采样 $k$ 个噪声数据，训练时要求模型正确辨别真实样本和噪声数据， $k$ 越大，理论上越能逼近原 Softmax函数。实际上，C&W模型的思想与此类似，都是将正确目标和随机生成的候选目标混合，通过对比和纠错来训练模型。此外，Mikolov等[2]提出的负采样（negative sampling，NEG）方法也可视作噪声对比估计的简化版，因其引入的噪声分布只用于采样，不参与目标函数计算，加速效果更明显。噪声对比估计和重要性采样联系紧密，前者可视作使用Logistic损失函数的二分类问题，后者可视作使用Softmax和交叉熵损失函数的多分类问题。"
  },
  "3.2.5模型的讨论": {
    "context1": "Bengio等[30]提出的神经网络概率语言模型事实上开启了一个新的方向，后续的许多工作可追溯到这项工作。表2对部分代表性的方法做了对比，省略了其他衍生性的工作（如对上下文或输出层的扩展）。",
    "context2": "从模型复杂性来看，NPLM、RNNLM、受限玻尔兹曼机等神经网络模型的复杂度最高，从输入层到隐藏层都要先经线性变换，再经激活函数的非线性变换，从隐藏层到输出层还要再作线性变换，最后由Softmax函数作归一化计算。对于语言模型来说，非线性的转换提升了模型的表达能力，但随之而来的是极度增长的复杂性；而LBLM、CBOW、Skip-Gram等模型的思想则是追求简化，放弃了非线性变换，其中CBOW和 Skip-Gram最为彻底，将输入层到隐层的线性变换一并移除，直接采用向量点积来预测，物理意义上也更为直观。此外，不同模型根据实际需要，或对上下文作了不同的扩展，或对输出层作了重构或近似。需要指出的是，模型性能和词向量质量受多种因素制约，从语料规模、应用领域、上下文的定义到超参的设定等，都可能会对最终的结果产生影响[15]。"
  },
  "3.3评估指标": {
    "context1": "词向量的质量评估主要有两种方式[134]： $\\textcircled{1}$ 内源性评估计算词汇的语义关系； $\\textcircled{2}$ 外源性评估考察词向量作为下游NLP任务输入的有效性。",
    "context2": "内源性评估的任务主要有语义相关性计算和语义类比。前者计算两个词向量的余弦距离并与人工标注比较，主要数据集有WS-353[135]（以及利用WordNet分割成的两个子集[136]）、低频词数据集RareWord[137]、动词数据集 $\\mathrm { S i m V e r b } { - } 3 5 0 0 ^ { [ 1 3 8 ] }$ 。后者由Mikolov等[2]引人，数据形如(King:Quenn:Man:Wom-an)，含义是King类比于Queen，正如Man类比于Woman。此外还有类属数据集 SuperSenses[139]以及基于 SemCor数据集构建的语言学特征数据集[140]。评估数据集以英语为主，其他语种数据集多是翻译而成，Li等[14I]最近提供了针对中文的类比评测数据集CA8。",
    "context3": "内源性评估数据集通常人工构建，规模较小,且难免存在主观性因素。外源性评估根据对具体任务性能的提升效果来评价词向量。一种方式是在有监督的任务中引入词向量作为额外特征[142]，例如,Miller[143]在命名实体识别、Koo等[144]在句法分析任务中将聚类后的词向量作为辅助特征引入模型；另一种方式是将词向量作为模型的输入，例如，C&W将预训练的词向量作为神经网络的输入来完成词性标注、语义角色标注等一系列任务[31]。",
    "context4": "需要注意的是，内源性评估与外源性评估结果并非总是相关的[140,145]，相同的词向量在不同的外源性评估任务中的表现也存在不一致现象[134]，如何客观评价词向量的质量，仍是一个有待解决的问题。"
  },
  "3.4两类学习方法的比较": {
    "context1": "无论是基于共现统计还是基于神经网络的预测，以上两类方法的本质，都是通过词汇与所在上下文的相关性来捕捉词汇语义。不同的是，前者通常使用文档作为上下文，后者通常使用滑动窗口内的词作为上下文。两类方法都以分布式假设为基础，而分布式假说的思想渊源于索绪尔的语言符号学说[146]，索绪尔将语言要素的关系归为组合（syn-tagmatic）和聚合（paradigmatic）两类，组合关系体现在词汇方面就是组字成词、连词成句、聚句为段，强调相似的词会在同一个语境中共现；聚合关系又称替换关系，强调相似的词具有相似的语境而不同时出现，具有聚合关系的语言要素一般可以互换。从这个角度看，基于词-文档共现矩阵的模型，如LSA、LDA等，可以看作是对组合关系的建模，而基于滑动窗口局部上下文的模型，如NPLM、LBL、CBOW等，更多是对聚合关系的建模。",
    "context2": "表2部分有代表性的基于神经网络预测的词汇表示学习方法比较",
    "context3": "<table><tr><td>类型</td><td>模型</td><td>输人层</td><td>隐层</td><td>输出层</td><td>优化目标</td></tr><tr><td rowspan=\"4\">面向语言模型</td><td>NPLM[30]</td><td>目标词前n-Gram词向量拼接</td><td>非线性单隐层</td><td>词表 Softmax 回归</td><td>语言模型困惑度</td></tr><tr><td>RNNLM[33,122]</td><td>目标词前n-Gram词向量(n不定）非线性循环隐层词表 Softmax 回归</td><td></td><td></td><td>语言模型困惑度</td></tr><tr><td>受限玻尔兹曼机[32]</td><td>目标词前n-Gram词向量</td><td>非线性单隐层</td><td>词表 Softmax 回归</td><td>语言模型困惑度</td></tr><tr><td>LBLM[32]</td><td>目标词前n-Gram词向量</td><td>线性单隐层</td><td>词表 Softmax 回归</td><td>语言模型困惑度</td></tr><tr><td rowspan=\"3\">面向词向量模型 CBOW[2-3]</td><td>C&amp;W[31]</td><td>目标词左右窗口词向量拼接</td><td>非线性单隐层</td><td>输入样本得分</td><td>目标词与上下文对的最大似然</td></tr><tr><td></td><td>目标词左右窗口词向量平均</td><td>无</td><td></td><td>词表Softmax回归目标词与上下文对的最大似然</td></tr><tr><td>Skip-Gram[2-3]</td><td>目标词左右窗口词向量</td><td>无</td><td></td><td>词表 Softmax回归目标词与上下文对的最大似然</td></tr></table>",
    "context4": "同时，这两类方法之间也有紧密的联系。Levy等[147]构建一个高维稀疏的词-上下文共现矩阵，其上下文定义为目标词左右窗口内的词，并直接使用PPMI统计作为词汇表示。实验表明这种显式表示得到的词向量与SGNS得到的词向量性质相似，在类比任务中表现良好，说明PPMI矩阵与SGNS之间应该存在某种关联。观察图6可知，word2vec模型学习两个词嵌入矩阵：作为中心词的词嵌入矩阵$\\boldsymbol { E } ^ { \\scriptscriptstyle W } \\in \\mathbb { R } ^ { | V _ { \\scriptscriptstyle w } | \\times k }$ 和作为上下文的词嵌入矩阵 $\\boldsymbol { E } ^ { c } \\in \\mathbb { R } ^ { c \\times k }$ ，令 $E ^ { { \\boldsymbol { w } } } \\times E ^ { \\mathrm { c } } = M ^ { \\prime } \\in \\mathbb { R } ^ { | V _ { w } | \\times c }$ ，则矩阵 $M ^ { \\prime }$ 的元素是词向量和上下文向量的点积，word2vec正是依据这两个向量的点积作Softmax分类预测，即word2vec虽然没有明确构造矩阵 $M ^ { \\prime }$ ，但实质就是对PPMI矩阵的隐式分解。Levy等[34]进一步证明了在不限制词向量维度的情况下，SGNS模型的解析解就是shifted-PPMI矩阵。Li等[3也从数学上证明了词共现矩阵分解与 Skip-Gram的等价性。PMI矩阵与SGNS之间的联系，使得一些研究者开始通过对PMI矩阵的各种变形进行分解来学习词向量，Glove[34]模型亦是受此思想的启发。",
    "context5": "总体而言，基于共现统计的方法可解释性比较好，如Levy等[147]提出的显式向量空间，每个维度都表示确切的上下文，而主题模型中的每个主题也可通过该主题下的词汇分布来解释。基于神经网络的预测方法则具有强大的拟合能力，不过其非线性转换缺乏可解释性。一些工作为此尝试去提高模型的可解释性，如Li等[148]通过擦除输入词向量的某些维、移除隐层的某些神经元、删除部分输入词，来分析比较擦除前后模型评估结果的差异，以解释神经网络的决策过程。",
    "context6": "关于两类模型的性能表现，Baroni等[149]表明，基于神经网络的预测模型几乎在所有任务中都优于基于共现统计的计数模型，而Levy 等[II5]则提出，当对超参调整优化后，基于计数的模型的性能接近于基于预测的模型，且在某些任务上有更佳表现。"
  },
  "4词汇表示的应用": {
    "context1": "词汇表示学习得到的词向量，一方面基于其语义表达能力，可用于词汇相关度计算、词聚类、词类比、词映射等应用；另一方面基于其组合性，可以作为下游自然语言处理任务（如情感分析、命名实体识别、序列标注等）的初始化输人。随着自然语言处理研究的不断深人，词汇表示学习还为我们深刻理解语言机理和社会机制提供了新的研究途径。"
  },
  "4.1语义关系挖掘": {
    "context1": "语义知识库中包含大量命名实体以及实体关系信息，是实现开放域问答、查询理解、知识推理的重要基础。然而人工构建语义知识库的方式代价高昂，自动挖掘实体间的语义关系就成了一个重要课题。Mikolov等[2]的工作表明词汇间的关系可通过向量偏移得到，通过简单的代数运算就能获得大量的句法或语义关系。Fu等[73]进一步提出了一种基于分布式表示的语义层次化方法：给定训练样本 $\\left( { \\boldsymbol { x } } , { \\boldsymbol { y } } \\right)$ ，其中， $_ { y }$ 为 $x$ 的上位词，目标是学习一个线性映射矩阵 $\\phi$ 使得 $y = \\varPhi ( x )$ ，更进一步地，由于上下位关系是多对多的关系，先对训练样本聚类，再为每一个聚类分别学习一个映射矩阵 $\\boldsymbol { \\varPhi } _ { k }$ ，即可通过 $\\boldsymbol { \\varPhi } _ { k }$ 来判断新样本对 $\\mathbf { \\Psi } ( x , y ~ )$ 之间是否存在上下位关系，以《同义词词林》为骨架，他们构建了《大词林》系统。",
    "context2": "Shwartz等[74]提出了融合依存句法路径和分布式词向量的上下文关系识别方法HypeNet。对于训练样本 $\\mathbf { \\Psi } ( x , y ~ )$ ，首先从句法依存树提取从 $x$ 到y的关联边序列，依次输入LSTM并编码为路径向量，对于从 $x$ 到 $_ { y }$ 的路径向量集合作加权平均池化操作后，首尾再简单连接上 $x$ 到y的词向量，形成一个长向量输入到Softmax作二分类。该方法将依存句法与分布式表示的优点有效结合，取得的了较好的结果。此外，Nguyen 等[75]基于 Skip-Gram提出Hyper-Vec 模型，以识别实体的上下位关系，Xie等[7则通过词向量对知网的义原进行识别与扩充，Zeng等[77]利用词向量和知网对LIWC词典中文版进行了扩充。除了上下位关系，知识表示学习的代表性方法TransE[78]，也是以词向量的语义计算和现有的语义知识库为基础的，该方向是目前研究的一个热点。"
  }
}