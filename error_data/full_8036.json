{
  "original_filename": "full_8036.md",
  "nothing": {
    "context1": "·前沿与热点·"
  },
  "生成式人工智能研究进展\\*": {
    "context1": "王芳1.2朱学坤1.2刘清民1.²岳之楠1² 王美权1² 张馨月1.² 杨天德1.2马 鑫1.2张超1,2(1.南开大学商学院信息资源管理系天津300071)(2.南开大学网络社会治理研究中心天津300071)",
    "context2": "摘要：快速地技术迭代与广泛应用表明生成式人工智能在驱动经济社会发展方面具有巨大潜力,但同时它也可能带来多种挑战与风险。在系统回顾相关学术文献的基础上,文章梳理了生成式人工智能的关键技术、技术发展历程及应用场景,分析了生成式人工智能在数据训练、算法模型、内容利用等方面存在的风险,总结了生成式人工智能风险治理的策略,包括实施全生命周期的数据质量控制,提高AI风险治理的技术能力,对组织结构进行适应性优化,强化问责机制和标准约束，同时提出未来生成式人工智能发展应坚持发展创新与风险治理并重的原则,完善政策法规与技术标准体系，建立多元包容的国际合作共治体系,充分激发企业活力。本文可以为生成式人工智能领域的学术研究、技术开发和政策制定提供参考借鉴。",
    "context3": "关键词：生成式人工智能；大语言模型;风险治理;AIGC；AI幻觉;政策分析中图分类号：G354.5 文献标识码：A DOI:10.11968/tsyqb.1003-6938.2024045"
  },
  "Progress in Generative Artificial Intelligence Research": {
    "context1": "Abstract Therapid iteration and wide application of generative artificial inteligence(GAI) technology have demonstrated its immense potential in driving economicand social development.Atthesame time,italso brings challenges andrisks.Basedon acomprehensive review of relevantliterature,this paper identifies the key technologies,development history,and applicationscenariosofGAI,analyzes therisks in data training,model building,andcontent generation,summarizes therisk governance strategies for GAI,suchas implementing dataqualitycontrol throughout theentire lifecycleofGAI,enhancing technicalcapabilities to manage AIrisks,optimizingorganizational structures foradaptability，and strengthening theconstraints by accountabilitymechanismand technical standards.Additionally，this paper proposes that the future development of generative artificial inteligenceshouldadhere tothe principle of balancing innovation withrisk management,improvethesystemofpolicies,regulations,and technical standards,establishadiverse and inclusive framework for internationalcooperationand governance,and fullstimulate thevitalityof enterprises. This study is expected to provide reference for GAI related research, technology development,and policy making.",
    "context2": "Key words generative artificial inteligence; Large Language Model (LLM); risk governance; AIGC; AI halluination; policy analysis",
    "context3": "生成式人工智能正迅速应用于教育、医疗、金融、法律、制造业、气象预测、广告创作、新媒体、游戏等经济社会的各个领域，对促进新质生产力发展、完善现代产业体系、形成国际竞争新优势具有重要意义。伴随着生成式人工智能的快速发展,其潜在的风险与挑战也引发了广泛关注。生成内容的真实性、原创性和伦理合规性成为公众和学术界讨论的焦点。同时,生成式人工智能的应用可能造成隐私泄露、算法歧视、生成内容幻觉和虚假信息泛滥等社会问题,有效规制的缺失又可能会使虚假信息在社会层面广泛扩散,进而引发更大的社会风险。因此,既要支持负责任、可信赖的AI创新,又要管理其潜在的风险,成为中、美、欧等国家和地区共同关注的课题。学术界也在短时间内发表了大量的研究成果,就相关问题展开探讨。为了更加系统地理解当前生成式人工智能领域的学术研究成果和各国政策关注的核心议题,本文将全面梳理生成式人工智能的关键技术、发展历程和应用场景，总结生成式人工智能存在应用的伦理、法律与社会风险,并在此基础上分析生成式人工智能的风险治理与创新发展策略,为促进我国生成式人工智能更好地赋能经济社会发展提供参考。"
  },
  "1生成式人工智能的关键技术、发展历程与应用场景": "",
  "1.1生成式人工智能的关键技术": {
    "context1": "人工智能（Artificial Intelligence）根据用途可分为决策式人工智能(Discriminant AI)和生成式人工智能(Generative AI)[1]。决策式AI专注于数据洞察并做出决策，应用于自动驾驶、智能推荐、人脸识别等自动化决策领域。生成式AI则能够通过模型训练生成新颖、有意义的内容,如文本、图像代码或视频等[2]。这些由人工智能技术自动生成而非由人类创作的内容被称为人工智能生成内容(Artificial IntelligenceGeneratedContent,AIGC)[3],而之与相对应的人类创作内容则被称为 HGC(Human Generated Content）。",
    "context2": "生成式AI的关键技术包括生成对抗网络(Gen-erativeAdversarialNetworks,GAN)[4]、变分自编码器(Variational Autoencoder,VAE)[5]、扩散模型(Denois-ing Diffusion Probabilistic Models,DDPM)[6]、强化学习（Reinforcement Learning,RL）[7]和 Transformer[8]等。GAN 通过生成器和判别器的对抗训练生成接近真实的数据,广泛应用于艺术创作和图像生成;VAE通过编码器将输入数据映射到低维空间,再通过解码器生成新的数据;DDPM则通过逐步向数据中添加噪声并学习逆向去噪过程生成高分辨率图像,已在图像生成任务中展现出优异的性能;RL根据试错学习和环境反馈信号来优化生成策略,通过与环境的交互评估生成内容的质量,进而调整生成策略,提高生成结果的多样性和适应性;Transformer通过其自注意力机制,能够高效捕捉和生成序列数据的上下文信息,为生成式人工智能在文本生成任务中的成功奠定基础。此外,Transformer 的并行计算能力可以大大提高训练效率,使得在大规模数据上预训练和微调这些模型成为可能,进而推动生成式人工智能在文本生成、问答系统、机器翻译等多个领域的应用和发展。",
    "context3": "大语言预训练模型(Large Language Models,LLMs)是基于Transformer架构的生成式人工智能模型的典型代表，如BERT和GPT系列模型。BERT作为基于双向Transformer架构的代表性模型,通过同时考虑上下文信息实现了更加准确和全面的文本理解和生成能力[9]。经过预训练和微调后，BERT在问答系统、文本分类等多种自然语言处理(NLP)任务上表现优异。GPT系列模型采用Transformer 的解码器结构，运用自回归方法进行语言建模。GPT-1开创了基于Transformer的大规模预训练方法，使用无监督预训练加有监督微调模式,能够生成高质量、连贯且上下文相关的文本[10]。GPT-2 提出了“Zero-shot\"学习的理念，即在没有额外微调的情况下，仅凭预训练模型就能执行特定任务[]。GPT-3引入了上下文学习(In-contextLearning)模式,使模型能够通过分析提供的文本上下文进行学习，从而在无需额外训练的情况下,灵活地适应和执行各种任务[12]。InstructGPT通过从人类反馈中进行强化学习（ReinforcementLearning from Human Feedback,RLHF),使模型生成的内容更加符合用户的具体指令和需求，从而提升了模型的实用性和可靠性[13]。而ChatGPT,作为In-structGPT的进一步优化版本，通过更深入的强化学习和微调策略,不仅保持了强大的语言生成能力，还显著提升了与用户交互的自然度和流畅度。Sora 是OpenAI推出的一项尖端的生成式AI技术，结合了DDPM Spacetime Patch 和 Transformer 架构,能够根据文本指令创造出逼真且富有想象力的视频，代表了“文生视频\"领域的一大飞跃[14]。"
  },
  "1.2生成式人工智能的技术发展历程": {
    "context1": "生成式人工智能的技术发展共经历了三个阶段。第一阶段为二十世纪五六十年代的起源阶段，主要基于规则驱动的方法，通过编写一系列规则来生成新的内容[15]。第二阶段为早期探索阶段,生成式技术以机器学习为基础逐渐发展起来。这个时期的研究者们开始尝试采用统计方法来生成新的数据，如文本、图像等。其代表技术包含隐马尔可夫模型（HiddenMarkov Model,HMM)[16]、高斯混合模型（GaussianMixture Models,GMM)[17]等,但在文本与图像的生成内容中，缺乏连贯性与创造性。第三阶段为成熟阶段,随着深度学习算法的快速发展,生成模型的性能得到了显著提升。生成式对抗网络(GAN)变分自编码器(VAE）、扩散模型(DDPM)等技术的出现，极大地推动了生成式技术的发展。这三类技术被广泛应用于生成文本、图像、音视频等领域。同时,生成模型在各个领域的发展虽遵循了不同路径,但最终出现了交集：Transformer 架构[8]。Transformer于 2017 年首次用于NLP任务，随后成为各领域许多生成模型的主要支柱。基于以上各项技术的提出,近年来,多模态生成模型DALL-E、CLIP与大语言模型ChatGPT更进一步推动了生成式人工智能的发展,显著提升了生成内容的质量与用户体验。Sora 结合ChatGPT系列的技术优势,能够模拟现实世界中的物理交互与动态变化,生成复杂的视频场景[18],成为生成式人工智能的重大突破。张亚勤提出,AI大模型的五个未来发展方向包括多模态智能、自主智能、边缘智能、物理智能和生物智能[19]。李彦宏指出,在大模型基础上衍生出的智能体,能够通过自然语言处理技术理解和响应用户的自然语言指令，并通过不断学习用户交互数据提升自身的服务能力，通过动态交互对话、任务自动化和智能推荐满足用户的复杂需求[20]。"
  },
  "1.3生成式人工智能在经济社会中的应用场景": {
    "context1": "作为一项迅速发展的前沿技术,生成式人工智能推动了经济社会诸多行业的创新，显著提升了生产力[21]。人工智能生成内容(AIGC)包括文本、图像、视频、语音、代码和三维场景等,具有高效性、创造性、多样性和广泛适用性[22]。文本生成主要用于聊天机器人、内容创作等[23]；图像生成用于创作艺术作品和视觉内容[24];视频生成广泛应用于娱乐和广告领域[25];语音生成则利用WaveNet等工具生成人类语音，用于虚拟助手、有声读物和自动客服[26-27]；三维场景生成技术则在游戏、虚拟现实和建筑可视化中得到应用[28]。",
    "context2": "在传媒和出版领域,生成式人工智能被用于自动撰写新闻和生成音视频内容[29-30]。如 NVIDIA的PYoCo模型运用时间注意力机制、级联生成架构等技术,通过文本描述生成高质量的视频,并确保视频内容的时间一致性和高分辨率，改变了内容生产和传播的方式[31-32]。",
    "context3": "在产品设计和广告创意领域,生成式人工智能正在重塑商业模式[33]。如Adobe Sensei 利用机器学习和生成技术自动生成设计元素、编辑图像和优化广告内容,帮助设计师更快地对接客户需求,生成个性化的广告[34]",
    "context4": "在公共服务领域，生成式人工智能嵌入敏捷政府的治理过程[35-36],推动公共服务自动化，提升政府的服务能力[37-38]。如北京市昌平区政务服务管理局的智能问答机器人“平平\"通过理解和生成人类语言，实现了全天候在线服务和智能问答[39]；清华大学图书馆采用AI导航助手依托数据库导航实现AI增强问答，采用AI阅读助手依托水木搜索实现AI增强阅读[40]。",
    "context5": "在司法领域,生成式AI被用来提升审判效率和质量。如深圳中院的人工智能辅助审判系统通过立案智审、智能阅卷、智能庭审和智能文书生成等功能模块，在提升审判效率的同时保留了法官的自主决策权[41]。",
    "context6": "在医疗领域，生成式人工智能被用于医疗影像分析[42]。如北京天坛医院的\"龙影\"大模型结合深度学习和图像处理技术,通过生成对抗网络(GANs)精确提取和分析医学图像特征,辅助医生进行诊断[43]。",
    "context7": "在教育领域,生成式人工智能应用于教育实践[4445],辅助生成个性化的教育内容[46]。如清华大学开发的AI助教系统运用生成式预训练模型(GPT)分析学生的学习数据,生成个性化的教育内容和反馈[47-48]。",
    "context8": "在科学研究领域,生成式人工智能被用来加速新材料、新疗法和新药物的发现。如Insilico Medicine利用生成对抗网络(GANs)和强化学习(RL)技术，进行新靶点发现和药物分子结构的设计[49]。",
    "context9": "在金融领域,生成式人工智能被用于市场预测、个性化营销内容生成[50-51]。如摩根大通的LOXM系统通过生成多样化的投资场景，更准确地预测市场动向并优化投资策略[52]。",
    "context10": "在制造业领域,生成式AI的应用能够推动产业转型[53],有助于生产效率的提升和产品质量控制。如Ford公司通过AI系统监控车辆制造过程中的质量问题,提前检测到缺陷并进行修复,从而减少废品率,提升产品质量[54]。",
    "context11": "在网络安全领域,人工智能大模型在风险识别、数据安全、垃圾邮件和钓鱼攻击过滤等方面有巨大潜力，同时也存在误报率、数据质量、模型泛化能力、可解释性问题和实时性能等方面的挑战[55]。"
  },
  "2生成式人工智能风险研究": "",
  "2.1各技术环节存在的风险": {
    "context1": "风险是指事件发生概率和相应事件后果程度的综合衡量。生成式人工智能从准备、运算到生成内容[56],涉及数据、算法和算力三个基本要素[57],在数据训练[58]、模型运算、内容输出、内容利用各个环节上存在风险,需要进行识别和治理。"
  },
  "2.1.1数据风险": {
    "context1": "生成式AI需要依靠对大量训练数据的学习才能生成新内容,如百度\"文心一言\"的训练数据包括万亿级网页数据、数十亿的搜索数据和图片数据、百亿级的语音日均调用数据,以及5500亿事实的知识图谱等[59]。在生成式AI对海量数据进行采集、存储、标注、利用和销毁过程中，可能发生如下风险：",
    "context2": "一是训练数据采集风险。包括数据采集手段非法、采集范围不当、数据含有偏见或故意投毒、篡改，以及模型训练过多依赖生成数据等[60-61]。含有偏见、受到污染、篡改的数据将严重影响输出内容的质量，产生AIGC 的幻觉问题[62-63]。如美国训练数据中存在的偏见使语言模型对非裔美国人等群体的判断存在偏见[64],而目前缓解这一偏见的人类偏好对齐做法，可能会在表面上掩盖语言模型更深层次上的种族主义，从而加剧隐性和显性刻板印象之间的差异[65]。Shumailov 等指出使用AI生成的数据训练AI模型可能导致“模型崩溃”,即模型在多代迭代后出现性能退化和输出质量下降的现象[6]。",
    "context3": "二是数据存储风险。主要包括大聚集数据的内部泄露[67]、通过外部攻击进行数据篡改、投毒、删除等[68],基于有毒数据生成的模型可能输出有害或错误的结果,进而侵犯个人隐私或商业秘密,甚至危害国家安全[9]。",
    "context4": "三是数据标注风险。主要是由标注者的能力水平、价值偏见以及监管不力等造成的训练数据质量低下问题。在面部识别技术的训练中,标注不当或数据集中某些种族群体的代表性不足，可能导致模型在识别少数群体时出现显著误差[70]。",
    "context5": "四是数据选择性利用风险。主要包括有选择地利用数据进行训练导致生成结果的偏见。如果开发者在训练过程中有意或无意地排除某类数据,模型的输出将难以反映现实情况。如在自动化招聘工具中，有模型因使用偏向男性的数据集,导致系统在筛选简历时对女性求职者产生偏见[71]。",
    "context6": "五是数据删除阶段的风险。包括删除不彻底、数据进入大量生成内容后难以完全删除等，进而造成侵权问题。尽管法律或公司政策要求删除数据,但是许多企业发现难以完全删除在多个系统中复制的用户数据,如果数据已经被AI模型使用生成内容，即便删除原始数据也无法完全抹除其影响[72]。"
  },
  "2.1.2 算法与模型风险": {
    "context1": "算法是生成式人工智能的核心技术，隐含着技术开发人员的观念与思维模式,在设计、开发和应用过程中可能隐藏如下风险：",
    "context2": "一是算法歧视破坏社会公平。算法偏见是指由于初始算法、样本数据或其他原因所形成的思维惯性而导致生成式人工智能系统在运行过程中出现有偏向性的举措或选择[73],如基于种族、性别、宗教等显性特征,在输出内容中对条件相同的人实施差别待遇[74-75]。",
    "context3": "二是算法的不可解释性和追责难问题。生成式人工智能的内部运行机制被称为“算法黑箱”,即输出内容或做出决策的原理难以被解释和监控[76]。这导致对生成式人工智能参与的行为追责困难[7],阻碍了生成式人工智能参与重要决策。"
  },
  "2.1.3内容生成风险": {
    "context1": "由大模型生成的内容可能会偏离真实世界的事实或不能准确反映用户指令，由此产生幻觉(Hallu-cination)风险。AI幻觉主要分为两类：",
    "context2": "一是事实性幻觉。AI生成的内容有可能与现实世界的事实不一致,如捏造不存在的人物、事件或数据等。造成这种问题的原因通常是模型触及知识边界,数据利用不足,或者训练数据中特定领域知识不准确、不充足[78]。",
    "context3": "二是忠实性幻觉。AI生成的内容有可能没有忠实地反映用户的指令或对话背景。这通常是因为 AI模型训练中能力不对齐和架构缺陷[79]、解码策略随机性和表示能力不足[80]等问题导致AI模型在理解复杂指令和多层次上下文推理中表现欠佳。"
  },
  "2.2利用不当导致的风险": {
    "context1": "生成式人工智能不仅在各个技术环节存在隐患，若应用不当且生成内容传播失控时,还可能会引发一系列交织的伦理、法律、政治、社会和经济风险。"
  },
  "2.2.1伦理风险": {
    "context1": "伦理风险主要指利用生成式人工智能技术从事违背社会道德的活动[81],从而对人与社会和自然之间的关系准则产生影响[82],包括违背学术道德[83]、导致情感异化、人类主体性价值消解等。",
    "context2": "首先,利用生成式人工智能实施学术不端的行为[84-85]有多种形式，如生成虚假学术文章[86]、编造不存在的引用[87]、通过重新措辞剽窃他人作品[88]以及生成虚假的数据和图表[89]。AI编撰的内容表面上符合学术标准,但实际上缺乏实验和证据支持，误导读者和评审,扰乱学术界的规范和诚信。",
    "context3": "其次,长期使用生成式人工智能可能会使人产生技术依赖[90-91],并对个体的神经系统产生实质性影响,导致批判性思维受损和记忆保持能力改变[92]。与此同时，拟人化程度越来越高的生成式人工智能为人类创造能够提供情感支持的数字空间，但人机交往的单向性可能会使人类陷入“群体性孤独”,长期的单向人机交往可能导致人类情感异化[93]。",
    "context4": "最后，由于生成式人工智能愈发具有自主道德行动能力[94],人的主体价值被削弱或消解[95-97],有学者认为生成式人工智能有可能会颠覆传统人本主义道德体系[98-99]。生成式人工智能也可能引导人类做出含有道德倾向的决策[100-101],所以 AI是否拥有权利[102],人工智能权利的基础和来源[103],人工智能的权利是否应当与人类权利保持一致[104-105]等问题将受到关注。"
  },
  "2.2.2 法律风险": {
    "context1": "生成式人工智能在高效完成用户指令的同时，其算法不透明,数据来源具有隐蔽性,输出内容具有匿名性[106],因此容易产生法律风险。",
    "context2": "首先,生成式AI的无约束使用可能侵犯知识产权[107-110]。2023 年 Stable Diffusion 用户李胸锴状告百家号不当传播自己利用AI生成的图片，成为中国首例 AI生成图片侵犯知识产权案[1I]。此外,训练数据集可能未经所有者授权，导致数据产权纠纷[12],如汤森路透曾指控罗斯智能公司“非法复制其法律数据库内容以训练人工智能系统”,导致罗斯智能公司在版权纠纷下被迫停业[113]。",
    "context3": "其次，远程生物识别,如远程人脸识别、指纹收集可能会给个体的基本权利带来风险。生成式 AI有可能侵犯隐私[114]和个人信息[115-116],被用于伪造他人信息实施诈骗或勒索[117]。如2024年韩国警方某加密软件中存在大量社交群组利用Deepfake(深度伪造)技术换脸合成色情照片和视频,参与用户多达22万人[118]。",
    "context4": "最后，生成式AI还面临算法失控、算法滥用、算法欺诈等风险。McAfee调查发现,生成式人工智能被犯罪分子用来克隆声音，以实施诈骗,超过 $70 \\%$ 的受访者难以辨别真实语音与克隆语音的差异，约 $10 \\%$ 的受访者表示曾收到AI生成的语音诈骗消息,其中 $7 7 \\%$ 的受害者在收到此类消息后蒙受了财务损失[119]。",
    "context5": "为应对生成式AI的潜在法律风险，美国加州议会拟定了《SB-1047前沿AI大模型安全创新法案》,旨在对高风险AI模型建立安全标准，防止滥用和灾难性后果，但遭到包括李飞飞在内的科学家们的强烈反对，他们认为这将损害AI生态系统，不必要地惩罚开发者，扼杀开源社区，并阻碍学术研究，且对模型风险的评估方法不科学[120]。在应对生成式AI的潜在法律风险与促进技术创新之间，如何取得平衡仍然是一个巨大的挑战。"
  },
  "2.2.3政治风险": {
    "context1": "生成式AI技术能够在极短时间内生成大量内容，并通过社交媒体、搜索引擎优化等渠道迅速扩散[121],传播范围远超传统人类生成内容(HGC)。同时，通过分析用户的行为数据和兴趣偏好，AI能够生成高度定制化的内容，并通过智能推荐系统精准推送给特定用户群体[122]。然而，尽管个性化的定向传播方具有广泛的应用场景,但如果监管不当也可能会带来诸多政治风险。",
    "context2": "首先，A生成内容泛滥有可能污染网络信息环境。生成式AI可能被用于大规模传播虚假信息、谣言、煽动性言论等有害内容[63],污染网络信息环境[123],",
    "context3": "破坏社会公序良俗[124]。",
    "context4": "其次，生成式AI可以自动产生虚假新闻，并通过伪造可信来源的方式迅速传播。这种大规模传播的虚假信息不仅会混淆公众视听，还可能导致社会信任体系的崩溃,引发公众的普遍焦虑和不安[125]。",
    "context5": "第三,生成式AI可能被用于舆情操纵,影响公众认知[126-127]。如在美国总统选举中,生成式AI被用于生成大量宣传内容,通过社交媒体平台影响选民的政治倾向和投票意图[128],进而削弱公众对政府和社会机构的信任[129]。",
    "context6": "最后，AI生成内容可能带有某种特定的意识形态偏向,故意曲解事实或隐瞒真相。借助于特定事件情境，这些意识形态材料可能被用于国家间的舆论战或信息战,严重威胁国家意识形态安全[130]。通过有偏内容的输出来影响公众观点，甚至成为境外势力干预内政的手段[131]。"
  },
  "2.2.4社会风险": {
    "context1": "生成式人工智能技术应用的社会风险主要涉及劳动力替代与就业难题、社会不公加剧、新型智能鸿沟的出现等。",
    "context2": "首先，自动化程度的提高使低技能和重复性的工作需求减少[132]。生成式人工智能强大的知识生产与应用能力使其能够承担的工作类型越来越多，人类脑力劳动领域的咨询、分析、教育以及媒体制作等过去“专属于人的岗位\"将被机器取代[133]。虽然生成式人工智能催生的新兴职业在一定程度上增加了就业机会，但高技能职位要求技术教育与技能提升[134],而就业压力的持续增大可能带来社会稳定方面的风险[135]。",
    "context3": "其次，人工智能生成的歧视与偏见性信息[136]的广泛传播可能会加剧社会不公[137]。计算资源分配不均以及数据和算法歧视等技术鸿沟、访问限制等政策变量、人机协同能力等用户差距均会导致不同个体在与生成式人工智能交互的各个环节上拉开差距,尤其会放大数字弱势群体的脆弱性,使之在人工智能技术发展的浪潮中被进一步边缘化[138],从而出现新型智能鸿沟。",
    "context4": "最后，过度依赖与AI的交互可能会改变传统的人际交互模式，导致个体认知与情感能力削减、自我身份认同困难和自我存在意义消解等风险[69,139]。"
  },
  "2.2.5经济风险": {
    "context1": "当前，由于数据、算力以及专业壁垒，生成式人工智能的基础模型和微调模型的市场高度集中并主要被极少数大型科技企业所垄断[140]。自从2020年GPT-3的突破以来，只有11个大型基础模型得以建立，其中有8个基础模型由大型平台直接开发，如谷歌/DeepMind、Meta、微软和百度等，其他模型也离不开大型平台的间接投资和控制[141]。为了应对生成式人工智能市场竞争可能带来的风险，美欧英的竞争管理机构联合发表声明，提出保护人工智能生态系统中的竞争原则,内容包括公平交易、互操作性、自由选择和消费者保护等[142]。"
  },
  "3生成式人工智能风险治理与创新发展对策研究": "",
  "3.1生成式人工智能风险治理对策": {
    "context1": "负责任的人工智能强调以人为本、社会责任和可持续性,有效解决、记录和管理人工智能风险和潜在的负面影响，从而带来更值得信赖的人工智能系统[143]。针对生成式人工智能应用可能带来的种种风险，现有研究从生态体系建设、数据质量管理、治理技术、组织结构优化、问责机制建设等方面提出了治理对策。",
    "context2": "(1)构建可信赖人工智能生态体系,实施政府主导下的AI风险协同治理",
    "context3": "生成式人工智能生产、应用的产业链十分复杂，使得算法检视性工具、算法风险评估性工具和算法主体责任制存在局限[144],单一的政府治理难以实现生成式人工智能风险的有效治理,因此需要实施政府主导下的企业、社会多元主体协同治理[145]。如建立虚假信息等级评估制度，以政府主导强化多元主体间的协同,强化效能管理和快捷响应[146]。商汤科技指出,人工智能治理应当是一个价值牵引、技术先行、多方参与、分层推进的动态进程[147]。欧盟《人工智能白皮书》(2020)提出构建可信赖人工智能生态系统，包括与会员国合作、支持建立世界领先的测试中心、通过吸引顶尖教授和科学家来提升欧洲人工智能研究中心的能力、关注中小企业、与私营部门合作、推动公共领域应用人工智能、加强安全访问数据和计算基础设施的建设、引入非欧盟组织和政府观察员[148]。阿里巴巴《生成式人工智能治理与实践白皮书》提出，政府在生成式AI治理中应完善顶层设计、健全治理体系、推动国际合作;产业界应通过标准化形成行业自律、建设分类分级治理制度、持续发展治理技术、合理分配主体责任、吸收多方意见；社会应普及新技术，弥合公众认知鸿沟，校企联合助力人才培养[149]。",
    "context4": "(2)实施人工智能风险评估，进行风险分级分类管理",
    "context5": "对人工智能的风险进行评估,有助于明确治理范围,实施精准治理。欧盟《人工智能法案》[150](2024)将人工智能风险划分为不可接受的风险(需要禁止，如社会评分系统和操纵性人工智能);大部分涉及文本的高风险人工智能系统(需要管制);风险有限的人工智能系统(透明度义务较轻),但开发人员和部署人员必须确保终端用户知道他们正在与人工智能(聊天机器人和深度冒充者(Deepfakes)进行交互;极小风险(不受监管),包括目前欧盟单一市场上可用的大多数人工智能应用程序,如支持人工智能的视频游戏和垃圾邮件过滤器等。美国商务部国家标准与技术研究所(NIST)发布的《人工智能风险管理框$1 . 0 \\rangle$ 提出，与不直接与人类交互的AI系统相比，设计或部署用于直接与人类互动的AI系统,如由个人身份信息等敏感或受保护数据组成的大型数据集上进行模型训练，或者输出内容对人类有直接或间接影响,应设定更高的风险初始优先级[143]。中国信息通信研究院建议聚焦人工智能应用场景，以监管沙箱试点摸清场景应用风险特点，针对典型风险细化治理规则和方案[151]。商汤科技将人工智能治理的实现分为可用、可靠、可控、可信四个层次，并依据AI对最终产品安全、个人权益、市场公平、公共安全和生态安全的影响程度,将伦理风险由低至高划分为E0至E4四个等级[147]。",
    "context6": "(3)强化AI项目管理,实施项目全周期风险管理",
    "context7": "生成式人工智能被用于改造传统产业，具有显著的项目属性。因此,需要强化AI项目管理,将风险管理贯穿于项目生命周期的各个阶段，包括规划阶段、商业和技术规范阶段、测试阶段和部署阶段[152]。在规划阶段设定AI的安全、伦理和法律标准，明确监管范围;在商业和技术规范阶段制定生成式AI技术和商业规范,利用自动化工具监控数据质量、隐私保护和算法合规性;在测试阶段通过自动化模拟测试和算法优化,确保AI在不同场景下安全运行，并根据测试结果进行调整;在部署阶段则需要AI系统运行中进行实时监控,检测问题后自动调整，确保系统稳定、安全。阿里巴巴提出，在AI模型训练阶段，对训练数据进行筛选和过滤以剔除含有风险的数据,对模型实施全面的安全评测,通过模型对齐与内生安全增强确保模型遵循人类的价值观;在服务上线阶段,服务提供者需要选择安全有效的模型,并进行模型核验,确保服务使用的工具集(如插件)的合理性和必要性,并执行合规动作，如算法安全自评估和算法备案;在内容生成阶段,需要对账号进行管理,保护个人信息，建立内容审核与处置机制,包括建立审核制度、专职团队，对生成内容进行分类分级，并采用技术手段进行内容审核;在内容传播阶段,需要添加标识以提示内容是由人工智能生成，并建立风险监测和应急处置机制，以快速响应和控制虚假信息的传播[149]。",
    "context8": "(4)严格控制数据质量,实施AI全生命周期数据治理",
    "context9": "生成式人工智能的风险与数据质量密切相关，数据质量管理应当贯穿生成式人工智能的全生命周期[153]。在预训练阶段应对数据收集、标注、准备、降维、增强和版本控制环节进行治理,尤其是对数据进行严格过滤,避免使用AI生成数据训练模型,减少模型崩溃风险[];在评估阶段应开展同分布评测、异分布评测及相关评测数据集的治理,引入严格的标准数据审查机制，减少数据中的隐性或显性偏见，减少算法输出的歧视性结果[154];在部署推理阶段需要对部署数据、指令数据集、偏好数据集、强化学习数据集、提示工程数据、运维监控数据等进行治理；运维监控阶段的数据治理包括运维数据的治理、数据的安全治理、数据的理解呈现、数据的质量保证、数据存储检索的治理等,更新存储知识库资源以扩展模型的知识边界,避免错误信息引入，确保生成结果的时效性与正确性;在退役(迭代)阶段则需要合规处置数据、对模型知识数据进行迁移复用、对大模型全生命周期的技术与经验数据进行继承[155]。中国信息通信研究院指出，大模型发展需要重视个人隐私保护问题,包括确保训练数据来源的合法性,坚持数据使用透明和可问责原则，保障用对户交互信息的删除权[155]。信息系统审计与控制协会(Informa-tion Systems Audit and Control Association,ISACA)强调确保人工智能系统决策过程和数据来源的透明度和可追踪性,企业需要增强审计功能来监控AI系统的决策，保证决策过程的透明度，追踪数据来源以确保数据的可靠性，实施记录和监控操作以快速响应问题,确保符合数据保护和隐私法规,从而提高用户对AI系统的信任度[156]。",
    "context10": "(5)以 AI治理AI,开发智能化的技术工具应对算法风险",
    "context11": "当前，人工智能的性能提升远超安全技术的发展,这种不平衡导致\"Crippled AI”,即高性能但安全性不足的AI系统[157]。为了跟随高速迭代的AI系统,“以技术治理技术,以AI治理AI\"是一种新型算法治理方向。“AI治理AI\"通过智能算法自我监管，提高效率并解决传统人工监管迭代慢的局限性[158]。2022年6月，Meta与美国司法部达成法律和解，同意删除广告商用来间接分析和定位某些受众群体的特殊广告工具,并同意部署一个新的\"方差减少系统\"(VRS)以消除其广告定位和投放系统中机器学习算法的偏见[159],通过开发治理技术减少了住房广告中的性别和种族偏见风险。人民日报社传播内容认知全国重点实验室基于“AI治理AI\"的理念,打造AIGC-X平台,实现对人工智能生成内容的精准识别,在文本识别方面,基于人工智能生成模型倾向于采用高频词的特点，构建语义与风格特征融合的语言模型评价方法;在图像识别方面,研发基于视觉上下文、子块信号特征的人工智能生成图像检测模型,实现对人工智能生成图像的精确识别;在视频识别方面,通过提取真实视频中人物的视觉、音频特征,建立人物关联特异性模型,检测出合成人物在全局一致性、几何特性等方面与真实人物存在的偏差[160]。商汤科技建立了模型体检平台，对模型进行推理攻击和逆向攻击测试,检测算法模型对数字世界白盒对抗、数字世界黑盒查询对抗鲁棒准确率、数字世界迁移攻击对抗鲁棒准确率、物理世界对抗样本攻击成功率、模型后门攻击成功率，以判定算法模型是否符合设计要求[147]。阿里巴巴则建议提升模型的鲁棒性、可解释性、公平性，并构建有效的防滥用机制，以提高模型的可靠性和用户的信任,具体措施包括：开发评估工具来测试和强化模型抵御攻击的能力，提高模型韧性；使用模型可视化和解释性工具来揭示模型的决策逻辑;通过在训练过程中引入公平性约束并平衡数据来减少偏见;建立内容审核和版权监测系统来防止模型滥用[149]。",
    "context12": "(6)强化生成内容检测,提高AI生成内容的准确性与可靠性",
    "context13": "生成式人工智能容易生成虚假信息，带来幻觉问题，对社会和个人造成误导，缓解AIGC 的幻觉风险需要强化内容治理,措施包括数据质量控制、优化训练过程、改进解码推理算法和强化后处理机制。在数据层面,减少幻觉的关键在于严格筛选和验证数据来源,避免错误信息和偏见的引入，同时加强对数据利用的优化，定期更新数据集以扩展模型的知识边界[161]。在模型训练层面,可以利用技术手段优化预训练模型的架构。如借助 Self-Alignment机制,根据检测出来的事实性幻觉对模型进行微调[162];通过人类反馈强化学习减少毒害内容的产生，让模型与指令需求保持一致[13]。在推理层面,可以通过降低解码过程中的随机性实现算法优化，如调整采样温度(Temperature)或采用束搜索等确定性策略，减少生成错误的概率[163]。在生成内容后，可以引入外部知识库进行事实验证或使用语言模型检测逻辑错误，进一步提高生成内容的准确性和可靠性[164]。",
    "context14": "(7)进行组织结构的适应性优化,实施敏捷治理与自我规制",
    "context15": "为了应对潜在、复杂、瞬时的AI风险,还需要对组织结构进行优化,提高AI环境下的组织韧性。现有研究提出两种优化方向：敏捷与适应性治理,自我规制调整。",
    "context16": "首先,敏捷治理被视为应对生成式人工智能技术快速变革的关键策略,强调通过动态调整和灵活应变[165],提高组织应对复杂性和不确定性的能力[166]。阿里巴巴提出，生成式人工智能的发展还存在着较大的不确定性,采用过重、不科学的治理方式会抑制产业的发展,而敏捷治理作为更加顺应科技研发应用的治理模式,通过小步快走、迭代试错、动态更新，可以解决技术高速演进的不确定性问题[149]。中国信息通信研究院则建议坚持敏捷治理理念，强化跨部门协同和多元敏捷互动机制，平衡创新发展和风险治理[151]。",
    "context17": "其次,为了进一步提升AI系统的安全性与合规性，组织内部自我管理型规制的引入成为必要。通过强化平台自律和合规建设，平台企业能够主动识别和排除潜在的安全隐患，从源头减少AI系统带来的风险[167]。德勤报告发现仅 $23 \\%$ 的企业认为自己为生成式AI的风险管理和法规合规做好准备， $41 \\%$ 的企业难以精确衡量生成式AI的影响，并建议企业管理层应持续关注生成式AI战略，加强数据管理、风险控制和价值衡量,推动生成式AI深入嵌人业务流程,并通过跨部门合作和强有力的监管框架确保技术的长期成功和合规性[168]。ISACA指出组织需要构建AI伦理准则，以确保AI技术的发展与应用遵循伦理原则、透明度、公正性,涵盖利益相关者的权益,确立责任与问责机制,并进行持续的伦理审视与教育培训,促进AI技术负责任的演进[158]。",
    "context18": "(8)采用\"软硬法\"结合治理方式，强化问责机制与标准约束",
    "context19": "应对AI对技术发展带来的法律争议，还需要加强政策法规和技术标准建设。在大模型治理过程中，可以使用强制性法律(硬法)和非强制性的指导性文件、标准、伦理原则(软法)的\"软硬法\"结合的治理方式[151]。现有研究从生成式人工智能的法律责任、法律准则、安全体系、技术标准等视角提出具体建议。",
    "context20": "首先，人工智能系统的不可预测性和自主性可能导致责任漏洞,包括证明过错、因果关系以及界定生产者与使用者的责任，因此需要调整现有的责任规则[169],包括明确显性隐性披露体系,确定内容责任主体,完善投诉机制[170]。",
    "context21": "其次，监管机构应制定和完善相关法律法规，明确数据使用、隐私保护、算法偏见等方面的规范[171]。",
    "context22": "如中国在算法备案与评估方面提出了一系列措施[172];欧盟出台了AI法案，建立了欧盟通用的AI法律规制框架,并于2024年8月1日正式生效[173]。",
    "context23": "第三，研究AI安全可信指南，统筹规划AI安全标准体系，并加快建设AI安全检测能力[174],可以从确定绩效、设计和内部管理三类标准方向以保障人工智能生成内容的安全[175]。",
    "context24": "最后，完善生成式人工智能技术专利标准。如引入人工智能作为技术“发明人\"的制度设计，创立人工智能生成技术方案的专利性标准[176]。"
  },
  "3.2生成式人工智能创新发展对策": {
    "context1": "生成式人工智能是一项新兴的颠覆性技术，对于未来经济社会发展的影响十分深远。为进一步推动生成式人工智能产业高质量发展和创新应用，政策制定部门和学术界提出了一系列发展对策。",
    "context2": "(1)坚持发展创新与风险治理并重的原则",
    "context3": "1982年,大卫·科林格里奇在其《技术的社会控制》一书中提出了科林格里奇困境（Collingridge'sDilemma)：一项技术如果因为担心不良后果而过早实施控制,那么技术很可能就难以发展。反之，如果控制过晚,已经成为整个经济和社会结构的一部分，就可能走向失控,再来解决不良问题就会变得昂贵、困难和耗时间,甚至难以或不能改变[177]。生成式人工智能的发展也面临这种技术控制的两难困境[178]。在生成式人工智能技术的创新发展过程中，如何平衡技术进步与潜在风险是各国政策制定的重要议题。现有研究对主要经济体在生成式人工智能的发展[179]、监管[180]、教育[181]等方面的政策进行了比较研究,明确了各国对生成式人工智能技术的社会功能、潜在影响与战略定位。这些政策法规基本上秉持发展与治理并重的政策原则，旨在通过设定法律框架和治理机制,促进技术的负责任发展[182],不断寻求创新和规制的平衡点[183]。",
    "context4": "在AI发展方面，美国将人工智能界定为战略竞争的前沿领域,坚持绝对安全理念，依赖技术优势推行单边主导型治理,尝试构建以美国为中心的非对称单向度的数据流动秩序[184]。此外,美国政府重视AI基础设施长期投资和人工智能研究，以增强美国在制造领域的竞争力[185]。美国白宫发布的报告认为,",
    "context5": "AI有潜力彻底改变科学研究的方式，通过快速运行数百万次基于计算机的模拟实验,为现实世界中重要的科学实验提供指导;同时AI的发展需要解决数据偏见、计算能源、错误生成以及滥用等问题,建议采用负责任和可信赖的AI使用原则以及鼓励创新的AI集成方法[186]。",
    "context6": "中国的生成式人工智能政策体现出强烈的国家主导特征,重点在于通过政府规划和投资推动技术在国家安全和经济竞争力方面的应用[187]。在技术和应用监管方面,具有风险分析中性、深入技术管理、技术应用范围广、新兴市场应用潜力强等特点[188]。在借鉴其他国家的生成式人工智能监管经验的基础上，我国生成式人工智能管理制度的设计应该坚持发展与安全并重[189]、促进创新与依法治理相结合的监管原则[190]。为了进一步促进生成式AI的健康发展,需要坚持发展创新与风险治理并重的原则,确保技术进步与安全、合规和伦理责任相协调,使生成式AI在促进经济社会进步的同时，符合公共利益和人权保障的要求[191]。",
    "context7": "(2)加速AI产业布局,充分激发企业活力",
    "context8": "AI产业规模发展迅猛，成为世界各国关注的焦点。2023年，全球AI市场价值超过1300亿欧元，预计到2030年将增长至近1.9万亿欧元。美国在 AI私人投资方面以625亿欧元领先，中国以73亿欧元位居次席，而欧盟和英国合计吸引了90亿欧元的私人投资[192]。2024年我国政府工作报告提出深化人工智能研发应用,打造具有国际竞争力的数字产业集群[193]。美韩等国也在加速AI制造产业布局,以激发AI在促进生产力提升和产业升级方面的巨大潜能[194]。",
    "context9": "企业是技术创新的主体,生成式人工智能发展离不开企业创新赋能。得益于麦肯锡、埃森哲和AIport等机构报告，企业高管已经认识到生成式AI的变革潜力，并计划加大对人工智能领域的投资力度[195-196]。欧盟报告指出，在全球AI竞争中，中美处于领先地位,欧盟相对落后,建议通过 Horizon Europe、DigitalEurope、EICAccelerator和InvestEU计划为欧洲的初创企业和中小企业提供资金,帮助企业开发可信赖的AI技术,并修改欧洲高性能计算联合企业(EuroHPCRegulation)的相关限制，以便欧盟公司可以使用AI超级计算机来训练大型语言模型[192]。欧盟审计部门则发现由于缺乏协调和明确的投资目标,欧盟在 AI投资方面没有跟上全球领导者的步伐；中小企业采用 AI技术的基础设施和资本支持的实施进展缓慢，到审计时尚未产生显著结果；尽管委员会设法从欧盟预算中增加了对AI研究项目的资金投入，但并未监控这些投资对发展欧盟AI生态系统的贡献[197]。",
    "context10": "研究表明，中国企业领导层对生成式AI的认知度正在提升，试点和推广活动增多，企业内部的知识管理和答疑成为AI应用的关键场景,但预算投入有限,转化率有待提高[198-199]。美国则更多地依赖市场驱动的创新模式,注重通过法规和市场力量推动AI技术的发展,同时重视保护个人隐私和数据安全[200]。美国智库ITIF的报告《中国在人工智能领域的创新能力如何?》指出，中国在AI研究和应用方面取得显著进展，同时私人部门参与不足抑制了技术商业化转化率[177]。为了进一步促进我国生成式人工智能的发展和应用,政府应加速AI产业布局，充分激发企业活力,将企业的变革计划转化为创新实践,使其成为生成式人工智能创新发展的主体。",
    "context11": "(3)构建多领域、分层次法律架构，在合规的基础上兼顾敏捷治理",
    "context12": "生成式人工智能广泛应用于许多行业，但是各行业相关法律词义模糊,制度供给体系杂乱[201],目前实践只能参照通用性法律法规而少有专门规定可循。立法供给不足和司法裁判机制的缺失[202],监管工具不完善[203],导致司法机关在处理生成式人工智能相关案件时面临困难。为了应对生成式人工智能带来的复杂政策和法律挑战，需要进一步建立和完善法律框架体系[204],针对生成式人工智能的不同应用领域构建多领域、分层次的法律架构，以实现生成式人工智能的分层精准治理。",
    "context13": "欧洲在生成式人工智能政策制定方面展现出引领者的姿态，制定了严格的合规框架并推动跨大西洋合作,以确保AI技术的发展既符合社会公共利益，又能够保障基本人权[205]。欧盟在 2020 年发布的《人工智能白皮书》中提出构建可信赖人工智能生态系统，并确定了七项关键要求：人事代理机构和监管，技术鲁棒性和安全性，隐私和数据治理,透明性，多"
  }
}