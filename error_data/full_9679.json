{
  "original_filename": "full_9679.md",
  "跨语言情感分析研究综述\\*": {
    "context1": "徐月梅 曹晗王文清杜宛泽徐承炀（北京外国语大学信息科学技术学院北京100089)",
    "context2": "摘要：【目的】对跨语言情感分析的研究脉络进行梳理总结。【文献范围】以Webof Science数据库为检索平台，以TS $=$ cross lingual sentiment OR crosslingual word embedding为检索式,筛选90篇文献进行述评。【方法】根据跨语言情感分析所采用的技术进行分类概述,包括基于机器翻译及其改进、基于平行语料库、基于双语情感词典三种早期的主要方法,再到引人Word2Vec和GolVe等词向量模型后,基于跨语言词向量模型的方法,最后到2019年以来基于Multi-BERT等预训练模型的方法。【结果】总结跨语言情感分析相关研究的主要思路、方法模型、不足之处等,分析现有研究覆盖的语言、数据集及其性能。发现虽然Multi-BERT等预训练模型在零样本的跨语言情感分析上取得较好性能,但是仍然存在语言敏感性问题。早期的跨语言情感分析方法对现有研究仍有一定指导和参考价值。【局限】部分跨语言情感分析模型属于混合模型,分类时仅按照主要方法进行归纳。【结论】展望跨语言情感分析的未来发展和亟待解决的问题。随着预训练模型对多语言语义的深层次挖掘,适用于更多更广泛语种的跨语言情感分析模型将是未来发展方向。",
    "context3": "关键词：跨语言多语言情感分析　双语词嵌入分类号：TP391DOI: 10.11925/infotech.2096-3467.2022.0472",
    "context4": "引用本文：徐月梅，曹晗，王文清等.跨语言情感分析研究综述[J].数据分析与知识发现,2023，7(1)：1-21.(Xu Yuemei, Cao Han, Wang Wenqing,et al. Cross-Lingual Sentiment Analysis: A Survey[J]. Data Analysis andKnowledge Discovery, 2023,7(1): 1-21.)"
  },
  "1引言": {
    "context1": "跨语言情感分析（Cross-Lingual SentimentAnalysis,CLSA)旨在借助某一种或多种源语言（一般为情感资源丰富的语种，如英语),对另一种语言(目标语言，一般为情感资源匮乏的语种)开展情感分析工作。CLSA隶属于情感分析（SentimentAnalysis)领域,通过挖掘文本中的主观信息来判断其情感倾向。与单语言情感分析研究相比,跨语言情感分析重点需要解决不同语言间的语法、语用等差异，搭建不同语言之间的知识关联以实现语言间的资源共享，使得将英语语言下的情感分析成果用于开展其他语种的情感分析研究，从而解决大部分非英语语言所面临的情感资源匮乏问题。",
    "context2": "根据Ethnologue数据库统计分析，全球现有7139种语言。而情感分析研究集中在少数几种语言、尤其是英语语言下开展,因此英语积累了丰富的情感资源，如标注文本、情感词典等;而其他语言的情感分析研究则相对较少、情感语料资源较为匮乏。如果针对每一种语言进行情感数据的标注，将耗费大量的人力物力，因此,跨语言情感分析提供了一种有效的解决方法。在经济全球化背景下,跨语言情感分析的意义日益凸显，逐渐成为情感分析领域的重要方向之一。",
    "context3": "CLSA最早可追溯到2004年，研究学者首次探索性地通过机器翻译(Machine Translation)来解决跨语言情感分析问题[。诸多研究表明,CLSA能够将英语语言下积累的研究成果在其他语言情境中推广应用。例如, $\\mathrm { W a n } ^ { [ 2 ] }$ 利用英语标注的情感分类数据,通过机器翻译实现对中文文本的情感分类预测。Vulic等[3]通过跨语言词向量实现英语和荷兰语的相互检索。",
    "context4": "CLSA对一些目标语言的性能接近于单语言情感分析性能,但仍有一些问题尚未解决。例如,基于机器翻译的跨语言情感分析方法仍难以避免词汇覆盖(Vocabulary Coverage)问题,即由于不同语言之间语义表达和书写风格的差异,从单一源语言翻译的文本不能覆盖目标语言的所有词汇。此外，基于双语词嵌入的跨语言情感分析方法无法很好地解决语言对之间的语义差异(Semantic Difference)问题,即两种语言的语义差异较大，使得互为翻译的两个单词的词向量表示差距较大，难以得到高质量的双语词向量表示。",
    "context5": "本文整理和回顾了2004年至今、尤其是近10年间跨语言情感分析的文献。以WebofScience数据库为检索平台，构造检索式 $\\mathrm { T S } { = }$ cross lingualsentiment OR cross lingual word embedding,选择相关性较高的656篇文献作为研究基础,延伸阅读相关文献后最终选取90篇作为参考文献，按照CLSA的研究脉络以及CLSA采用的模型方法进行分类，并对各模型的原理、代表性研究、采用的实验数据集等进行总结和比较。",
    "context6": "CLSA的研究发展与机器学习、神经网络模型密不可分，从总的研究脉络上可分为两个阶段：早期是跨语言情感分析研究阶段,主要包括基于机器翻译及其改进的方法、基于平行语料库(ParallelCorpora)的方法和基于双语情感词典的方法；自2013年Mikolov等[4]提出分布式词向量表示模型Word2Vec,以及随着机器学习算法和神经网络模型的快速发展,跨语言情感分析进入了新的研究阶段，不再停留在对基于机器翻译或基于平行语料库等有监督(Supervised)方法的改进,而是逐渐发展到弱监督（Weakly-Supervised）、完全无监督（Fully-Unsupervised)的跨语言情感分析。本文贡献如下：",
    "context7": "(1)系统地总结跨语言情感分析的研究方法，按照研究方法和技术进行细分归类，总结CLSA研究的主要思路、方法模型以及不足之处等。",
    "context8": "(2)概述现有跨语言情感分析覆盖的语言、数据集及其性能。不同语言对之间的语义距离不同，使得不同语言对的CLSA性能差别很大，即存在语言敏感性(Language-Sensitive)问题。现有跨语言情感分析的性能验证大多在少数几个语种开展，一定程度上限制了跨语言情感模型的应用推广。本文对现有跨语言情感分析覆盖的语言、数据集及其性能进行总结分析，为语言无关的跨语言情感模型的研究提供思路借鉴。",
    "context9": "(3)概述跨语言情感分析研究中面临的重要挑战、重要问题,并提出有待探索的研究方向。"
  },
  "2早期的跨语言情感分析研究": "",
  "2.1基于机器翻译及其改进的方法": {
    "context1": "2004年,Shanahan等[1首次探索性地通过机器翻译解决跨语言情感分析问题,如图1所示。在之后近10年间，机器翻译一直是跨语言文本情感分析的主要方法,其基本思想是使用机器翻译系统将文本从一种语言翻译到另一种语言[5-9],从而实现多语言文本到单一语言文本的转换。",
    "context2": "将源语言的带标注数据翻译为目标语言[10-11]利用翻译后的数据训练情感分类器,实现对目标语言未标记数据的预测。也有一些研究将目标语言的未标注数据翻译为源语言，在源语言中进行情感分类预测[7-8,2]。此外,一部分研究兼顾上述两种翻译方向，创建从源语言到目标语言和从目标语言到源语言两种不同的视图，以弥补一些翻译局限（Translation Limitations )[9,13-15]。",
    "context3": "由于目标语言和源语言之间存在固定的内在结构（FixedIntrinsic Structure）和不同的术语分布（TermDistribution），即便采用最好的翻译系统，机器翻译的失误仍然会带来约 $10 \\%$ 的文本情感扭曲或反转现象[15]。为克服机器翻译质量对跨语言情感分析的影响，相关研究尝试对基于机器翻译的跨语言情感分析进行改进,具体的改进思路有：借助对源语言情感词典的翻译[16、对源语言的训练集进行优化[17]、设置标准数据集对机器翻译进行优化[18]、使用多种源语言的标记数据[19]以及将目标语言未标记数据[20]添加到训练集。部分早期跨语言情感分析的代表研究如表1所示，其中\\*标注的是近年关于基于机器翻译改进的代表性论文，&标注的是近年基于平行语料库的代表性论文。",
    "context4": "![](images/019758101172586d6be665d51dce1989343d9df6d2d99afb9c4d30add96a2acb.jpg)  \n图1基于机器翻译的跨语言情感分析方法示意[1]  \nFig.1Cross-Lingual Sentiment Analysis Based on Machine Translation",
    "context5": "为解决基于机器翻译的CLSA存在的泛化问题,尤其是当源语言和目标语言的文本属于不同领域时效果不佳的问题, $\\mathrm { H e } ^ { [ 1 6 ] }$ 提出一种弱监督的潜在情感模型(Latent SentimentModel,LSM),在隐含狄利克雷分布(LatentDirichletAllocation，LDA)模型中融入从源语言的情感词典中通过机器翻译得到的目标语言可用的情感先验知识。LSM将该情感先验知识纳入LDA模型中对目标语言文本进行情感分类,LDA主题分类的类别数等于情感分类的类别数。",
    "context6": "为使源语言的训练集合样本更接近目标语言的文本,Zhang等[17]提出对源语言的训练集合样本进行优化选择(Refinement),通过相似度计算将与目标语言高度相似的样本作为改进后的训练样本，构建一个以目标语言为中心的跨语言情感分类器，通过选择有效的训练样本来消除源语言和目标语言之间的语义分布差异。Al-Shabi等[18]研究机器翻译引入的噪声对CLSA的影响，提出通过设置标准数据集优化机器翻译，并以英语为源语言、阿拉伯语为目标语言进行实验。首先通过英语的标记数据集训练多个机器学习算法，例如朴素贝叶斯、支持向量机,再用训练好的模型预测目标语言的情感类别,选出表现最好的模型;最后通过该模型确定噪声与情感分类精度之间的关系。研究表明，该方法训练出的最优模型能够为阿拉伯语这类资源稀缺的语种生成可靠的训练数据。",
    "context7": "为了改进基于机器翻译方法的跨语言情感分析,Hajmohammadi等[19-21]首先从增加源语言种类入手,提出一种基于多源语言多视图的CLSA模型[19]。该模型将多个源语言的标记数据作为训练集,尝试克服单一源语言的机器翻译所导致的词汇覆盖问题,使不能被覆盖的词汇有可能从另一源语言的翻译中得到覆盖。随后，提出基于机器翻译将目标语言的未标记数据整合到学习过程中,进一步提高性能[20]。利用主动学习从翻译成源语言的目标语言无标记文本中选择信息量最大、最可信的样本进行人工标记,丰富只有源语言带标记文本的训练数据。最后，为克服源语言和目标语言的术语分布不同的问题,提出一种基于多视图的半监督学习模型[2I],将多种源语言的标记数据作为训练集，通过自动机器翻译从源语言和目标语言的文档中创建多个视图，并将目标语言中未标记的数据合并到多视图半监督学习模型中，从而提高跨语言情感分析的性能。",
    "context8": "综上，为解决基于机器翻译的CLSA存在的泛化问题、词汇覆盖问题、源语言和目标语言之间的语言鸿沟问题,相关研究针对特定问题展开探索，取得了较好的效果，然而仍然没有获得一个一致的解决机器翻译根源性问题的方案。这些改进工作大多采用亚马逊产品评论数据集,数据集的多样性不够，难以全面支持和反映所改进方法的性能效果。",
    "context9": "表1早期跨语言情感分析的代表研究  \nTable 1Representative Researches on Early Cross-Lingual Sentiment Analysis",
    "context10": "<table><tr><td colspan=\"3\"></td><td colspan=\"3\"></td></tr><tr><td>作者</td><td>模型</td><td>特点</td><td>数据来源</td><td>语种</td><td>准确率/%</td></tr><tr><td>He*[16]</td><td>LSM</td><td>借助对源语言情感词典的翻译,得到目标语 言的情感词先验知识,纳入到LDA模型进 中国商品评论数据 行学习</td><td></td><td>英-中</td><td>81.41</td></tr><tr><td rowspan=\"4\">Zhang等*[17]</td><td rowspan=\"4\">ATTM</td><td rowspan=\"4\">基于训练集选择,将与目标语言高度相似的 标记样本放入训练集中,构建一个以目标语 言为中心的跨语言情感分类器</td><td rowspan=\"4\">测试集：COAE2014; 训练集：中国科学院计算 技术研究所带标记中文数中-法 据集</td><td>中-德</td><td>84.3</td></tr><tr><td>中-英</td><td>87.7</td></tr><tr><td></td><td>80.1</td></tr><tr><td>中-西</td><td>83.3</td></tr><tr><td>Al-Shabi等*[18]</td><td>SVM、NB、KNN</td><td>设置标准数据集对机器翻译优化，以此找到 最优的基线模型,并确定了机器翻译数据中 亚马逊产品评论 的噪声与情感分类精度之间的关系</td><td></td><td>英-阿</td><td>一</td></tr><tr><td rowspan=\"3\">Hajmohammadi等*[19]</td><td rowspan=\"3\">MLMV</td><td rowspan=\"3\">将多种源语言的标记数据作为训练集,克服 从单一源语言到目标语言的机器翻译过程 导致的泛化问题</td><td rowspan=\"3\">亚马逊产品评论; Pan Reviews数据集</td><td>英+德-法</td><td>79.85</td></tr><tr><td>英+法-德</td><td>81.55</td></tr><tr><td>英+法-日</td><td>73.73</td></tr><tr><td rowspan=\"3\">Hajmohammadi 等*[20]</td><td rowspan=\"3\">DBAST</td><td rowspan=\"3\">将目标语言无标记文档通过机器翻译转化 为源语言文档后,从中选择信息量最大、最</td><td rowspan=\"3\">亚马逊产品评论； Pan Reviews数据集</td><td>英+日-中</td><td>76.65</td></tr><tr><td>英-法</td><td>78.63</td></tr><tr><td>英-中</td><td>71.36</td></tr><tr><td>Hajmohammadi等*[21]</td><td>Graph-Based Semi-Supervised</td><td>可信的样本进行标记以丰富训练数据 提出一种基于多视图的半监督学习模型,将 目标语言中未标记的数据合并到多视图半 亚马逊产品评论; 监督学习模型中,即在文档级分析中加入目 Pan Reviews数据集</td><td></td><td>英-日 英-中</td><td>70.04 73.81</td></tr><tr><td>Lu等&amp;[22]</td><td>Learning Model</td><td>标语言内在结构的学习 联合双语有情感标注的平行语料库和未标 MPQA；NTCIR-EN; 记平行数据,为每种语言同时学习更好的单NTCIR-CH;ISI中-</td><td></td><td>英-日</td><td>72.72</td></tr><tr><td></td><td>Joint</td><td>语情感分类器 不依赖机器翻译标记目标语言文本,从未标 MPQA;NTCIR-EN;</td><td>英平行语料库</td><td>英-中 中-英</td><td>83.54 79.29</td></tr><tr><td>Meng等&amp;[23]</td><td>CLMM</td><td>记的平行语料库中通过拟合参数学习情感 词,扩大词汇覆盖率 基于平行语料库和词对齐构建双语词图,从 General Inquirer Lexicon;</td><td>NTCIR-CH;ISI中- 英平行语料库</td><td>英-中</td><td>83.02</td></tr><tr><td>Gao等&amp;[24]</td><td>BLP</td><td>现有源语言(英语)情感词典中学习到目标 ISI中-英平行语料库；英-中 语言的情感词典</td><td>NTCIR情感语料库</td><td></td><td>78.90</td></tr><tr><td rowspan=\"4\">Zhou等&amp;[25]</td><td rowspan=\"4\">NMF</td><td rowspan=\"4\">提出一个子空间学习框架,利用少量文档对 齐的并行数据和双语下非并行数据,缩小源 亚马逊产品评论</td><td rowspan=\"4\"></td><td>英-法</td><td>81.83</td></tr><tr><td>英-德</td><td>80.45</td></tr><tr><td>英-日</td><td>75.78</td></tr><tr><td>法-英</td><td>79.47</td></tr><tr><td rowspan=\"4\"></td><td rowspan=\"4\"></td><td rowspan=\"4\">语言和目标语言的差距</td><td rowspan=\"4\"></td><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>德-英</td><td>79.56</td></tr><tr><td>日-英</td><td>78.79</td></tr></table>"
  },
  "2.2基于平行语料库的方法": {
    "context1": "平行语料库是由相互翻译的文本组成的语料库。基于平行语料库的CLSA无需借助翻译系统，以平行或可比语料(ComparableCorpora)为基础完成源语言和目标语言的空间转换[26],是早期CLSA的主要方法之一。",
    "context2": "基于平行语料库的CLSA代表性论文(见表1)，主要思路为：借助目标语言的未标记数据[22]、通过平行数据的学习来扩大词汇覆盖率[23]、通过平行语料库生成目标语言的情感词典[24]以及借助少量并行数据和大规模的不并行数据[25]。",
    "context3": "基于平行语料库的CLSA方法示意如图2所示。",
    "context4": "平行语料库包括大量平行句对的集合，通过将平行句对中两个对齐的单词连接起来,构建语言间的映射关系。例如，图2(b)是两个表达相同语义的中英文句子,即一组平行句对。句对中的中文单词\"快乐\"与英语单词\"happy\"对应,可以说这两个单词对齐（Word-Aligned)。图2(a)将平行语料库中两种语言的单词作为节点,通过语料库的单词对齐及同义词、反义词等信息建立节点间联系，从而构建语言间的关系。",
    "context5": "![](images/d994b9f9fca512afbbe8f5452c942f0752686657889775025557147b1db46bb2.jpg)  \n图2基于平行语料库的CLSA方法[24]  \nFig.2Structure of CLSA Based on Parallel Corpora",
    "context6": "Lu等[22]首次提出借助无标注的平行语料库提高基于有标注的平行语料库获得的情感分类器性能。认为未标注的语料库中的平行语句也应具有相同的情感极性,因此提出在句子级别同时联合每种语言的标记数据和未标记平行数据,使用标记数据基于最大熵分类器进行期望最大化(ExpectationMaximization,EM)迭代更新，逐步提高两个单语分类器对未标记平行语句的预测一致性,以最大化平行语料库的预测一致性。实验表明,该方法对两种语言的情感分类准确率均有提升。",
    "context7": "然而，Lu等[22要求两种语言都有带标记的数据,这些数据通常不易获得。因此,Meng等[23]提出一种生成性跨语言混合模型(Generative Cross-LingualMixtureModel,CLMM）,去除对目标语言标记数据的要求，不依赖不可靠的机器翻译标记数据，而是利用双语并行数据弥合源语言和目标语言之间的语言差别。CLMM通过拟合参数最大化双语平行数据的可能性，从未标记的平行语料库中学习情感词,显著提高词汇覆盖率,从而提高跨语言情感分类的准确率。",
    "context8": "在将情感信息从源语言传递到目标语言的过程中，现有方法[27-28]使用少量词汇翻译源语言，从而导致目标语言的情感词汇覆盖率较低。为解决该问题,Gao等[24]提出一种基于平行语料库和词对齐构建的双语词图方法，从现有源语言(英语)情感词典中学习到目标语言的情感词典,从而将情感信息从英语情感词转移到目标语言的情感词上。",
    "context9": "大规模文档对齐(Document-Aligned)或者句子对齐(Sentence-Aligned)的平行数据很难获得,通常只存在少量的平行数据以及大量不平行的各语言下的文本。Zhou等[25]提出一种子空间学习框架同时学习源语言和目标语言间少量的文档对齐数据和大量的非对齐数据。研究者认为，文档对齐的并行数据在两种不同的语言中描述着相同的语义,它们应该在相同的分类任务中共享相同的潜在表示，通过此共享表示来减少源语言和目标语言之间的语言差距[25]。",
    "context10": "上述基于平行语料库的研究，共性之处在于借助平行语料库建立两种语言的单词对应关系，从语义和概念上弥合源语言和目标语言之间的术语分布和结构差异,避免机器翻译的噪声问题。传统基于平行语料库的方法需要大量并行或标记数据,往往不易获得;故上述相关研究通过采用可比语料库、非并行数据和未标记数据等，减少对并行标记数据的依赖。"
  },
  "2.3基于双语情感词典的方法": {
    "context1": "文档级或句子级的机器翻译容易引入较大的翻译误差,而词语级别的机器翻译准确率较高,因此,双语情感词典(Bilingual Sentiment Lexicon)被提出用于跨语言情感分析。基于双语情感词典的CLSA，首先构建双语情感词典，再计算目标语言文本中各个单词的情感分值得到总文本的情感分值，作为文本情感判别的重要依据。",
    "context2": "基于双语情感词典的CLSA属于一种无监督方法，无需借助源语言和目标语言的标注数据,天然地具有一定的优势。近年来,一些学者致力于研究双语情感词典的构建这一子任务，进而完成CLSA任务。",
    "context3": "构造双语情感词典的主要方法有基于机器翻译、基于同义词词集(Synset)和基于平行语料库的方法。双语情感词典的生成质量评估主要采用覆盖率和准确率两种标准：覆盖率是统计生成的情感词典所包含的单词在实验所用单词表中所占比例；而准确率是统计分类正确(积极/消极)的单词在实验所用单词表中的所占比例。",
    "context4": "基于机器翻译的双语情感词典构建,将已有单语情感词典经机器翻译后得到跨语言情感词典，实现较为简单。Darwich等[29]将印尼语WordNet和英语WordNet通过机器翻译后映射得到马来西亚语的情感词典，该方法对于资源较为丰富的语言有较好的表现，但是对于资源相对稀缺的语言表现并不理想：经过5轮迭代后生成的情感词典准确性只有",
    "context5": "0.563。",
    "context6": "基于同义词词集的双语情感词典构建利用现有单语同义词词集,通过一些映射方法得到跨语言情感词典。Nasharuddin等[30]设置跨语言情感词典生成器（Cross-Lingual Sentiment Lexicon Acquisition）,根据同义词集和词性将马来西亚语情感词典映射到英语情感词典中形成双语词典。Sazzed[31通过英语WordNet和孟加拉语评论语料库获取孟加拉语近义词集,并以此生成孟加拉语情感词典。",
    "context7": "基于平行语料库的双语情感词典构建是近年来较为常用的双语情感词典构建方法，该方法通过对两种语言的平行语料库进行分析和抽取后构建双语情感词典。Vania等[32]基于英语和印度尼西亚语的平行语料库,从中抽取情感模式(Sentiment-Pattern）信息并构建双语情感词典。Chang等[33]使用多语言语料库基于Skip-Gram生成保留上下文语境的单语词向量表示,而后计算英语词向量与其对应的翻译为中文的词向量之间的最优转化矩阵,通过这个转化矩阵将英语的情感单词词向量转化为中文空间中的词向量,利用余弦相似度构造中英跨语言情感词典。",
    "context8": "完成双语情感词典构建后，研究者基于双语情感词典开展跨语言情感分析研究。例如,Gao等[24]提出LibSVM模型,结合双语情感词典对NTCIR数据集中的数据进行情感分类。He等[34]基于中文-越南语双语词典,利用卷积神经网络对中越新闻进行情感分析研究。Zabha等[35使用中文-马来语双语情感词典,利用情感得分统计(TermCounting)方法对马来语的推特文本进行情感分析。",
    "context9": "综上，基于双语情感词典能够进行跨语言情感分析,但也存在一定局限性：跨语言情感分析的性能一方面依赖于所构建的双语情感词典的质量，另一方面还受到跨语言情感预测所使用方法/模型的影响,例如采用卷积神经网络或基于情感得分统计的方法等。基于双语情感词典的CLSA属于一种无监督的方法，相比其他有监督方法具有天然优势。"
  },
  "3跨语言词嵌入生成研究": {
    "context1": "随着分布式词向量表示模型Word2Vec[4]GloVe[36]和ELMO[37被相继提出,文本的语义开始通过词嵌人（Word Embedding)向量进行表示。跨语言词嵌入（Cross-Lingual Word Embedding,CLWE）能够获得源语言和目标语言在同一语义空间下的词向量表示。基于CLWE,含义相同、来自不同语言的单词具有相同或相似的向量表征。英语和西班牙语的一组单词在特征空间中的CLWE分布情况如图3所示。可见,语义相同的双语单词在空间中的位置彼此靠近,如西班牙单词\"gato”与英文单词\"cat\"的位置相比于\"dog\"或\"pig\"更为接近。基于CLWE的跨语言情感分析依赖于CLWE的生成质量，近年来，许多研究者致力于开展CLWE生成研究。",
    "context2": "![](images/7d8c2dcbefa96910c65dcfdb55d15b125e729e053d876e15ae6f69cefd8c7ee7.jpg)  \n图3英语和西班牙语的CLWE示意[38]  \nFig.3Schematic of CLWE in English and Spanish",
    "context3": "早期的CLWE主要采用有监督方法，依赖于源语言和目标语言之间的平行语料库或双语种子词典[3941]作为跨语言监督信号。然而对于大多数语言,这样的平行语料和双语种子词典并不容易获得。因此,半监督方法被提出,尝试用更小规模的语料或者种子词典减少对监督信息的依赖,并在一些语言对上取得了较好的结果,例如在英-法双语词典生成任务中获得 $3 7 . 2 7 \\%$ 的翻译准确率,在英-德双语词典生成任务中获得接近 $40 \\%$ 的翻译准确率[38]。近年来，无监督的方法成为跨语言词嵌入生成的研究热点[42-45],主要原因在于无监督方法无需借助任何平行语料库或者种子词典,适用的语种范围更广泛，可移植性更强。总结有监督、半监督和无监督跨语言词嵌入生成的研究思路、优点和缺点，如表2所示。",
    "context4": "表2跨语言词嵌入生成方法分类及总结  \nTable2Classification and Summarization of Cross-Lingual",
    "context5": "<table><tr><td colspan=\"3\">Word Embedding Generation</td></tr><tr><td colspan=\"3\">方法</td></tr><tr><td>方法</td><td>有监督的借助大量的双语平 行文本</td><td>优点：将平行文本蕴含的嵌入空 间(Embedding Space)信息作为 参考,有效保证映射的效果； 缺点：双语平行语料难以获得， 尤其是大规模的双语平行语料。</td></tr><tr><td>半监督的 方法</td><td>基于小样本的启发 式双语种子词典作 为映射锚点，学习 转移矩阵</td><td>优点：只需要用到小样本的种子 词典,较易获得; 缺点：本质上是利用种子词典对 齐词空间的映射矩阵来代替整 个空间的映射矩阵，不一定能代 表源-目标语言整个空间的映射 矩阵。</td></tr><tr><td>完全无监 督的方法</td><td>借助大规模的非平 行语料资源,通过 生成对抗网络、自 动编码器-解码器 等模型学习双语之 间的转换矩阵</td><td>优点：无需借助平行语料库/双语 词典； 缺点：存在初始化不鲁棒问题， 对于初始解要求比较高,不同的 初始解对结果影响较大;在缺少 监督信息的情况下,容易陷入局 部最优解。</td></tr></table>"
  },
  "3.1基于有监督的跨语言词嵌入生成": {
    "context1": "有监督的CLWE模型需要依靠大量的双语平行文本,根据生成CLWE方法和模型的不同,现有工作在对情感表达的语言差异建模[46]、借助机器翻译的单词词对[47]、在跨语言词向量中加入情感信息[48]、基于方面级(Aspect-Level)细粒度的跨语言词嵌人[49]以及研究单词的词序调整对跨语言词嵌入生成的影响[50]等方面开展有监督的跨语言词嵌入生成研究。",
    "context2": "Chen等[46]认为现有的跨语言情感分析中语言的差异性(Language Discrepancy)被大大忽略,因此提出对情感表达中固有的语言差异进行建模，以更好地进行跨语言情感分析。给定源语言及其翻译文档构成的混合情感空间，语言差异被建模为源语言和目标语言在每个特定极性下的固定转移向量，基于目标语言文档与其翻译副本之间的转移向量来确定目标语言文档的情感。",
    "context3": "Abdalla等[47]采用向量空间矩阵转换的方法，借助由机器翻译获得的2000个单词对，计算从源语言到目标语言向量空间的转换矩阵。研究结果发现,当单词对的翻译质量较低时，情感信息仍然是高度保存的,不影响词向量转换矩阵的生成质量。为更好地适应跨语言情感分析任务，Dong等[48]在Abdalla等[47]工作的基础上,在生成跨语言词向量的同时加入情感信息,基于标注的双语平行语料库,将潜在的情感信息编码到CLWE模型中。",
    "context4": "现有大部分跨语言情感分析模型仅覆盖较粗糙的情感分析，如句子级情感分析、文档级情感分析。Akhtar等[49]关注于更加细粒度的方面级情感分析，结合负采样的双语连续跳跃元语法的模型(Bilingual-SGNS)对两种语言进行词嵌入向量表示，使两种语言被映射到同一共享向量空间中。在方面级的多语言情感分析任务中,该模型的准确率达到$7 6 \\%$ ;在实体级跨语言情感分析任务中，该模型的准确率也达到 $60 \\%$ 以上。",
    "context5": "Atrio等[50]注意到语言之间的词序存在差异，进而研究词序对跨语言情感分析研究的影响。以英语为源语言、西班牙语和加泰罗尼亚语为目标语言的双语平行语料库作为数据集,对目标语言进行词序调整,包括名词-形容词调整(Noun-Adjective)和全部调整(Reordered)。研究发现,词序调整有助于短文本的情感分析任务,例如方面级或者句子级别，而不适用于文档级别的CLSA任务。",
    "context6": "综上，有监督的CLWE生成模型的优点是能够将平行文本蕴含的嵌入空间信息作为参考,有效保证了双语空间的映射效果。"
  },
  "3.2基于半监督的跨语言词嵌入生成": {
    "context1": "基于半监督的CLWE方法认为不同语言同一个含义的词嵌入向量之间具有相似性(Isometry)。基于该假设,半监督方法舍弃了大量的平行语料,利用小样本的启发式种子词典,将种子词典表示为 $\\pmb { D } =$ $\\left\\{ x _ { i } , y _ { i } \\right\\} _ { i \\in \\left\\{ 1 , n \\right\\} }$ 作为映射锚点（Anchor）, $x _ { i }$ 为源语言单词， $y _ { i }$ 为 $\\boldsymbol { x } _ { i }$ 在目标语言对应的翻译词,每个单词表示为 $d$ 维的向量， $n$ 为 $D$ 中单词的个数。令 $X$ 表示源语言的单语词向量空间,Y表示目标语言的单语词向量空间，则 $X$ 和Y都是大小为 $n \\times d$ 的矩阵。通过学习源语言和目标语言单语词向量空间的相似性,获得两种语言间的映射矩阵 $W$ ，使得 $W X$ 与Y这两个空间尽可能相近,即优化以下目标函数：",
    "context2": "$$\n\\boldsymbol W ^ { * } = \\operatorname * { a r g m i n } _ { \\boldsymbol W } \\| \\boldsymbol W \\boldsymbol X - \\boldsymbol Y \\|\n$$",
    "context3": "其中, $W$ 为 $d \\times d$ 维矩阵。对于公式(1)目标函数的求解是一个迭代阶段。得到 $\\boldsymbol { W } ^ { * }$ 以后,对于任意一个源语言单词 $w _ { x }$ ,通过与转移矩阵 $\\boldsymbol { W } ^ { * }$ 点乘将其映射到空间 $Y$ 中，然后利用聚类算法，如K-NN算法，找到该点的最近邻点 $w _ { y }$ ，则 $w _ { y }$ 就是 $w _ { x }$ 的互译词。",
    "context4": "根据采用的种子词典以及生成转移矩阵 $W$ 的方法不同,现有研究对于半监督CLWE生成的思路有：使用双语同根词[51]、基于多语言概率模型得到种子词典[52]、利用单语词向量的相似度构造种子词[38]在CLWE中考虑emoji表情信息[53]以及考虑句子的情感信息[54]。",
    "context5": "Peirsman等[51]在构建双语词向量空间时舍弃了双语平行语料库或大样本双语词典,而使用双语同根词(Cognates)构成小样本种子词典,并以此作为初始解构造双语词向量空间，生成双语词向量。Vulic等[52]认为两种语言的单词映射存在一对一或一对多的映射关系。基于一对一映射关系，直接构造一对一映射的种子词典作为初始解;基于一对多映射关系,使用多语言概率主题模型(MultilingualProbabilistic TopicModeling)生成一对一映射的种子词典,并只保留对称翻译词对作为初始解进行CLWE的生成。",
    "context6": "Artetxe等[38]基于两种语言的单语词向量之间的相似度构造种子词典,将相似度最接近的两个单词看作对应的翻译，并加入种子词典中。研究结果表明,基于构造好的初始解,通过迭代自学习方法能够从25个单词对的种子词典中得到高质量的CLWE映射;该方法在初始解不够好时容易陷入局部最优解,因此不适用于规模较小的CLWE生成。",
    "context7": "Chen等[53]认为微博和推特用户评论中的表情符号可以作为跨语言情感分析的纽带，提出一个基于表情的CLSA表征学习框架Ermes。在Word2Vec词向量模型的基础上,Ermes使用emoji表情符号补充情感监督信息，基于注意力的堆叠双向LSTM模型，获得源语言和目标语言融合情感信息的句子表征。在这个过程中,需要借助机器翻译系统获得与源语言标注数据对应的目标语言伪平行语料。",
    "context8": "Barnes等[54]提出一种双语情感词嵌入（Bilingual Sentiment Embeddings，BLSE)表示,借助一个小的双语词典和源语言标注的情感数据,得到源语言和目标语言映射到同一个共享向量空间、同时携带情感信息的变换矩阵。以英语为源语言，西班牙语和加泰罗尼亚语为目标语言进行验证,BLSE能够借助源语言的情感信息提升CLSA性能，但是也容易在功能词的向量表示上分配太多的情感信息。",
    "context9": "综上，基于半监督的CLWE方法舍弃了大量的平行语料,利用对齐的种子词典学习语言映射矩阵$W$ ,本质上是利用种子词典对齐词空间的映射矩阵来代替整个空间的映射矩阵，该方法存在一定的局限性。尤其是对于语义距离比较远的两个语种，利用种子词典学到的映射矩阵来代替整个空间的映射矩阵会引入较大的误差，例如英语和日语之间。因此,基于半监督方法的CLWE应同时兼顾种子词典和词嵌入向量中丰富的信息,引导映射矩阵W的学习。"
  },
  "3.3基于无监督的跨语言词嵌入生成": {
    "context1": "相较于有监督以及半监督的方法，基于无监督的CLWE生成无需借助双语平行语料,其主要思路是：利用单语词向量模型获得源语言和目标语言的词向量空间 $X$ 和Y后，借助大规模的非平行语料资源,通过生成对抗网络、自动编码器-解码器等模型挖掘两种语言表示之间存在的关系，并通过上述模型学习得到双语之间的转换矩阵 $W _ { s }$ 和 $W _ { T }$ ，将两种语言的词嵌入表示映射至同一空间中，如图4所示。",
    "context2": "![](images/47e8d8b10b0d1cec131f7286bc500b457fde77b809e1c3815bbb4154365f81c6.jpg)  \n图4基于无监督方法的CLWE结构示意  \nFig.4Structure of Cross-Lingual Word Embedding Based on Unsupervised Approach",
    "context3": "现有研究对无监督的CLWE生成采用以下思路提高性能：优化词语相似度矩阵[55]、使用对抗性编码器[4.56]优化迭代自学习的初始解[57]、引人同一语言家族的多个源语言等方法[58]。",
    "context4": "Gouws等[55]发现有监督及半监督的CLWE普遍存在两个问题：一是训练耗时过长,不适用于大规模数据集；二是过分依赖双语平行语料库。因此，",
    "context5": "Gouws等[55]首次尝试将无监督方法应用到跨语言词嵌入中,即无需单词级别的双语平行语料库,提出一种BilBOWA模型生成CLWE。该方法在英语-德语、德语-英语CLSA任务中的准确率分别达到$8 6 . 5 \\%$ 和 $7 5 . 0 \\%$ ，远高于Hermann 等[59]提出的BiCVM模型以及Chandar等[提出的BAEs模型。同时，BiIBOWA优化了词向量映射矩阵的计算，大大缩短训练时间,仅需BAEs[60]训练时间的1/800。",
    "context6": "Barone[44]首次尝试使用对抗性自动编码器(AdversarialAuto-Encoder,AAE)将源语言的词嵌入向量映射到目标语言的词嵌入向量空间中。该方法能够在一定程度上提高两种语言的语义信息转换，但是如果训练数据不是平行语料,实验结果并不理想。Shen等[5利用AAE学习双语的平行文本,通过线性变换矩阵将两种语言映射到同一共享向量空间,将其作为BiGRU模型的输入,获得最终的预测结果。将AAE引人BiGRU后,提升效果明显,在亚马逊产品评论数据集上的F1值达到 $7 8 . 6 \\%$ 。",
    "context7": "Artetxe等[38]等在半监督CLWE方法的基础上,提出一种无监督模型Vecmap来构造初始解,摆脱对小规模种子词典的依赖。Vecmap模型基于假设：不同语言中具有相同语义的单词应该具有相同的分布,以此构造初始解的单词对。该方法在英语-意大利语、英语-德语双语词典生成任务中均达到 $4 8 \\%$ 的准确率,在英语-西班牙语双语词典生成任务中也获得了 $3 7 \\%$ 的准确率[57]。",
    "context8": "Rasooli等[58]在无监督的基础上考虑了语系家族对于CLWE的影响,选取来自同一语言家族、资源丰富的语言作为多个源语言，通过多种源语言的方法缩小目标语言和源语言之间的差异,并采用标注投影和直接迁移这两种不同场景下情感分析的迁移方法，为那些没有标记情感训练数据且机器翻译能力较小的语言设置鲁棒性的情感分析系统。结果表明,使用同一语系家族的语言能够提升跨语言情感分析任务的准确度,例如,斯洛文尼亚语和克罗地亚语、英语和瑞典语。",
    "context9": "综上，基于无监督的CLWE能够获得较好的双语词嵌入向量，并且在下游CLSA任务上有比较突出的表现。例如,BilBOWA模型[55]在英语-德语的CLSA中准确率达到 $8 5 \\%$ 以上。TL-AAE-BiGRU模型[56在英语-中文、英语-德语的亚马逊评论数据集上F1值达到 $78 \\%$ 以上。",
    "context10": "尽管基于无监督的CLWE无需借助双语平行文本或语料库,减少了对数据的依赖,在性能上也有较好的表现,但仍存在一定的缺点。Gouws等[6研究发现，基于无监督的CLWE模型对于语言对的选择非常敏感。对于部分语言对，依靠完全无监督的",
    "context11": "CLWE难以得到高质量的双语词向量表示。此外，无监督CLWE基于假设：不同语言间具有相同含义的单词,应具有相似的词嵌入向量，从而依靠单语下的词嵌入向量生成CLWE。这一假设在语义和语法结构相差较大的两个语种之间不一定成立，例如英语-日语、西班牙语-中文语言对。因此,无监督的CLWE对初始解的要求较高,容易在迭代过程中陷入局部最优解甚至较差解中。",
    "context12": "上述有监督、半监督和无监督的基于跨语言词嵌入的CLSA相关研究工作总结如表3所示。"
  },
  "4基于生成对抗网络的方法": {
    "context1": "生成对抗网络(Generation AdversarialNetwork,GAN)由Goodfellow等[62]提出,在图像生成任务方面取得了巨大成功,近年来被应用于CLSA。",
    "context2": "基于GAN的跨语言情感分析，其核心思想是生成-对抗，如图5所示，一般具有三个模块：特征提取器、语言鉴别器和情感分类器。",
    "context3": "具体地,特征提取器作为生成器提取文本特征;语言鉴别器判别特征来源是源语言还是目标语言；二者组成生成对抗网络并进行训练。每次迭代中，鉴别器首先提升鉴别语言能力,特征提取器随后尽力混淆语言鉴别器,训练结果是特征提取器使得语言鉴别器完全无法鉴别语言，即认为它能提取语言无关特征,将该特征运用于跨语言的情感分类。最后，特征提取器和情感分类器组合并输入源语言的带标注数据进行训练，实现对目标语言的情感分析。",
    "context4": "基于GAN的跨语言情感分析代表工作有Chen等[63]提出的ADAN模型和Feng等64]提出的CLIDSA/CLCDSA模型。同时,生成对抗网络的变种也被广泛应用于跨语言情感分类，如条件生成对抗网络（Conditional GAN）、基于Wasserstein 距离的Wasserstein GAN模型等。",
    "context5": "Chen 等[63]提出一种对抗深度平均网络模型（Adversarial Deep Averaging Network，ADAN）,通过特征提取器和语言鉴别器的多次迭代提取源语言和目标语言中的语言无关特征。在对抗学习中尝试最小化源语言和目标语言分布的Wasserstein距离，保证特征提取器能够提取出源语言和目标语言的语言无关特征。"
  }
}