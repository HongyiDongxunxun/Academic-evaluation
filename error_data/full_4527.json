{
  "original_filename": "full_4527.md",
  "基于文本语料的上下位关系识别研究综述": {
    "context1": "邱科达1,2.3，马建玲1.,2.3",
    "context2": "（1.中国科学院兰州文献情报中心,甘肃兰州 730000；2.中国科学院西北生态环境资源研究院,甘肃兰州 7300003.中国科学院大学 经济与管理学院，北京100049)",
    "context3": "摘要：【目的/意义】上下位关系描述概念之间的\"is-a\"关系，是分类法、本体和知识图等的重要基石,且在自然语言处理中也有广泛的应用。本文将对从文本语料中识别上下位关系的研究进展、相关资源及应用情况进行分析，为相关领域人员提供参考【方法/过程】本文采用内容分析法,以Webof science、维普和中国知网为信息源对其中刊载的上下位关系识别相关研究成果进行了梳理与分析【结果/结论】上下位关系识别取得了一定的成果,但远未解决,对此还需要进一步的探索和研究。最后从研究方法、基准与评估、领域知识、语言以及应用5个方面对上下位关系识别研究给出了建议。",
    "context4": "关键词：上下位关系;文本语料;知识组织 中图分类号：G254 DOI:10.13833/j.issn.1007-7634.2020.07.023"
  },
  "A Review of Hypernym Relation Recognition from Text Corpora": {
    "context1": "QIU Ke-da $^ { 1 , 2 , 3 }$ ,MA Jian-ling1,2.3",
    "context2": "(1.Lanzhou Library Chinese Academy of Sciences , Lanzhou 73oo00,China;",
    "context3": "2.Northwest Institute of Eco-Environment and Resources,Chinese Academy of Sciences,Lanzhou 73ooo,China;   \n3.SchoolofEconomics and Management, University of Chinese Academy of Sciences, Beijing 10o049,China)",
    "context4": "Abstract:【Purpose/significance】 Hypernym relation represents the \"is $^ { - 2 }$ \" relationship between concepts.It is the basis of taxonomy,ontologyandknowledgemaps,andisalsowidelyusedinnaturallanguage procesing.This paperanalyzestheresearch progress,relatedresourcesandapplicationofidentifying Hypernymrelationfromtextcorpora,soprovideseference for people in related fields.【Method/process】Weuses content analysis method,using Web ofscience,Weipu and CNKI as sources of information to sortoutand analyzetheresearch resultsrelatedtohypernym relationrecognition.【Result/conclusion】Hypernymrelationrecognition hasachievedcertainresults,butitisfarfrom being solved,andfurther explorationand research is needed.Finaly，we give suggestions ontheresearch ofhypernym relation recognitionfrom five aspects:research method, benchmark and evaluation, domain knowledge,language and application.",
    "context5": "Keywords: hypernymy relation;text corpora;knowledge organization",
    "context6": "随着互联网的发展和大数据时代的到来，社会上积累了大量可用的数据资源。如何组织异构分布的海量信息,挖掘更深入的知识内容，并为用户提供高效精准的信息服务成为迫切的研究课题。上下位关系是分类法、知识本体和知识图谱等知识密集型系统的重要基石，在知识表示和推理中无处不在。上下位关系又称等级关系或层级关系，是计算语言学中的基本语义关系，是人类认知的核心,表示概念或术语之间的\"is-a\"关系，比如\"国家\"和\"中国”、“狗\"和\"动物”。在过去的二十多年里，自然语言处理(NLP)团体一直致力于研究半自动或自动的方法来识别上下位关系。这种努力是由于上下位关系具有改善许多下游NLP和信息检索(Informa-tion Retrieval)任务的能力,包括查询理解、问答系统、个性化推荐等;而且它还支持像信息管理、电子商务和生物医学系统等的实际应用。",
    "context7": "表1文献检索结果",
    "context8": "<table><tr><td>数据库</td><td>主题词</td><td>筛读后的文献数量</td></tr><tr><td>WOS</td><td>Hypernymy Detection、Hypernymy Prediction、Hypernym Relation、Hypernymy Identification、hypernym hy- ponym、taxonomy relation Hypernymy recognition Hypernymy extraction、Concept Hierarchies</td><td>75(136)</td></tr><tr><td>中国知网</td><td>上下位关系、上位词识别、下位词识别、层次关系、等级关系、isa关系</td><td>42(350)</td></tr><tr><td>维普</td><td></td><td>35(398)</td></tr></table>",
    "context9": "目前,已从诸如Wikipedia、WikiData、百度百科等人工编译的资源中构建了许多分类法和知识图,如WordNet、Cyc、YAGO、Probase、CN-Probase等。这些知识图能够在智能化知识服务中发挥作用，支撑更高层次的NLP任务，且在信息处理领域也有着广泛的应用价值，如语料属性词的层次构建]。",
    "context10": "近些年，从文本语料中识别上下位关系已经取得了显著的进步，但是此任务远未解决，主要有以下三个原因： $\\textcircled{1}$ 文本语料库的大小、主题和质量可能会有所不同,不可能针对所有情况开发一种\"一刀切\"的解决方案。例如，给定一个非常大的语料库,基于Hearst模式, $\\mathbb { V } \\mathbf { u }$ 等[2]构建了英文的Pro-base;对于特定领域的语料库，基于词嵌入的方法则更合适。 $\\textcircled{2}$ 从自由文本中识别上下位关系的准确性通常比从诸如维基百科等分类体系中识别的准确性低，原因是很难从自由文本中对上下位关系的语言规律进行建模。 $\\textcircled{3}$ 上下位关系识别任务仍未得到充分研究：a)在新兴领域和特定领域;b)对于非英语和资源不足的语言[3]。"
  },
  "1概述": {
    "context1": "关系抽取最早是在1996年的 MUC-6(Message Under-standingConference,MUC)会议上提出的,在1998 年召开的MUC-7会议则首次引入了模板关系抽取任务，包括位置关系、雇佣关系和生产关系。MUC会议一共举办了7届，停办后，在1999年美国国家标准技术研究院(NIST)组织召开了自动内容抽取会议（Automatic Content Extraction,ACE）,对关系抽取的评测任务和评测语料进行了融合拓展和细化完善，ACE评测会议将关系抽取研究推到了一个新的高度。SemEval(Semantic Evaluation)是继MUC、ACE后信息抽取领域又一重要评测会议,SemEval-2007的评测任务4设置了7种常用名词和名词短语间的实体关系。随后,SemEval-2010的评测任务8对其进行了丰富和完善,但是以上的测评任务中并不包括上下位关系。有关上下位关系的测评任务出现在SemEval-2015的任务17和SemEval-2016的任务13,它们在更广泛的分类法评估任务上进行了从文本语料中对上下位关系识别任务的评估。受到近期相关研究的启发，SemEval-2018提出了\"Hypernym Discovery\"的测评任务[4]。",
    "context2": "为了保证获取数据的准确性和全面性，本文选取Webof Science(WOS)作为外文文献的获取来源。对于中文文献的获取，考虑到单一数据库数据不完备的情况，笔者同时使用了中国知网和维普。利用表1中的中外文主题词来构建检索式限定数据范围，并将检索的起始时间设置为1992年，外文文献获得了136篇，在中国知网和维普中分别检索到了309篇和373篇。通过对文献题目、摘要和引言部分的阅读，筛选出相关的外文文献75篇，经过去重后得到52篇相关中文文献。",
    "context3": "通过对中外文文献进行梳理,本文深入到文献的内容本身，首先系统研究了文本语料中典型上下位关系识别技术的研究进展,并调研分析了中文上下位关系识别技术的研究状况，随后又简单介绍和分析了上下位关系识别中所涉及的相关资源以及在不同领域不同语言的应用，最后讨论了存在问题和未来研究方向。"
  },
  "2典型的上下位关系识别技术": "",
  "2.1 基于模式的方法": {
    "context1": "基于模式的方法使用词汇句法模式从文本中识别上下位关系。该研究领域中最早且最具影响力的是Marti Hearst教授[],她手工定义了7种用来从英文语料中识别\"is-a\"关系的语言模式。这些模式引起了广泛关注，并且今天也经常使用。“ $[ C ] s u \\operatorname { c h } a s [ E ]$ ”“ $[ E ] i s a [ C ]$ ”是典型Hearst模式,其中 $[ C ]$ 和 $[ E ]$ 是名词短语占位符,分别代表了在\"is-a\"关系$( x , y )$ 中的上位词(类) $y$ 和下位词(实体) $x$ 。Probase[]就是利用Hearst模式从数十亿个网页中抽取\"is-a\"关系构建的,它包含了256万个概念和2076万对\"is-a\"关系。Kozareva等也采用了相似的方法,他们利用Hearst模式从网页中提取了用来学习分类法的\"is-a\"关系。",
    "context2": "基于模式的方法所定义的模式很精确,并且对英语语料中的上下位关系有很高的覆盖率,但由于自然语言的歧义性和复杂性，这些太具体的模式无法覆盖所有的语言情况，因此往往召回率很低。而且简单模式匹配常常由于惯用表达式、解析错误、不完整或无用信息的提取以及模棱两可的概念而出现错误。在下一部分,本文总结了提高基于模式的方法的精度和召回率的一些研究。值得注意的是，健壮的\"is-a\"关系抽取系统可以结合多种技术来实现高精度和高召回率。"
  },
  "2.1.1 改善召回率的方法": "",
  "(1)模式泛化": {
    "context1": "这种方法主要有两种途径，一是通过语言规则来扩展原始的Hearst模式，二是通过语料来学习更广义、更泛化的词汇句法模式。Ritter等用 $k$ 个名词短语列表来替换了Hearst模式中的名词短语\" $[ E ]$ ”（即候选下位词)。Anh等[8]利用从句法结构和 $\\mathrm { { W e b } }$ 数据中获取的上下文信息来设计更加灵活的模式，以增加模式匹配的范围。但是我们很难手动枚举所有的语法模式,Snow等提出了一种基于依赖关系路径的模式自动提取方法。这种方法比直接匹配更能抵抗噪声,被许多关系抽取系统采用。",
    "context2": "从文本语料库中生成的模式数量异常庞大，会导致了特征稀疏问题。这是因为词组必须以准确的配置同时出现,否则无法识别到任何关系。因此，一些学者研究从“原始\"模式中学习更多抽象模式来提高模式的通用性[-I。表2给出了从语料库中学习到的一部分具有代表性的模式。",
    "context3": "表2更加通用的模式",
    "context4": "<table><tr><td>Pattern</td></tr><tr><td>E which isa (examplelclasslkindl...) of C</td></tr><tr><td>E (andlor) (anylsome)other C</td></tr><tr><td>E which is called C</td></tr><tr><td>E is JJS (most)? C</td></tr><tr><td>E a special case of C</td></tr><tr><td>E is an C that</td></tr><tr><td>E isa!(memberlpartlgiven) C</td></tr><tr><td>!(featureslproperties) C such asE1,E2...</td></tr><tr><td>(Unlikellike) (mostlalllanylother) C,E</td></tr><tr><td>C including E1,E.,.</td></tr></table>",
    "context5": "(2)迭代抽取",
    "context6": "由于语义歧义和语义漂移，从文本语料库中会学习得到过度概括的模式，从而导致抽取出不正确的关系。为了解决问题，一些研究使用高质量的模式来进行迭代。Koza-reva等[]使用\"双锚\"模式(如“cars su ch as Ford and \\*”)来获取特定的上位词,并通过bootstrapping循环扩展上位词和下位词。“双锚\"模式的另一个优点是可以消除术语的歧义。为了从非结构化的语料库中准确学习\"is-a\"关系，Huang等[3]通过基于语义迭代模型的匹配方法以及句法分析方法来获取高质量的种子关系。"
  },
  "(3)关系推断": {
    "context1": "关系推断克服了术语对 $( x , y )$ 必须在同一个句子中出现的限制。Ritter等的研究思路是如果 $y$ 是 $x$ 的上位词,而$x _ { 0 }$ 与 $x$ 及其相似,则 $y$ 很有可能是 $x _ { 0 }$ 的上位词。为了更好度量 $x$ 与 $x _ { 0 }$ 的相似性,文章中训练了比基于矢量方法更好的隐马尔可夫模型(HMM)。Anh等提出了句法上下文包含方法(Syntactic Contextual Subsumption,SCS）。",
    "context2": "此外，对下位修饰语的句法推断也可以产生额外的“is-a\"关系。例如,机器可以基于\"grizzlybear\"的首词是“bear\"的证据来推断\"a grizzly bear\"是\"bear”。Taxify系统[14]会将术语的语言首词识别为其直接上位词,还有Gupta等[1s]的研究工作。诸如BabelNet、YAGO、DBpedia和Wikipedia等知识库，\"is-a\"关系是由人类手动编译的，这些半结构化资源是用于细粒度信息提取的令人满意来源。对于著名的本体工程YAGO,Suchanek等[结合WordNet同义词集和Wikipedia类别来建立的分类层次结构。相似的研究还包括文献【17】。"
  },
  "2.1.2 提高精度的方法": {
    "context1": "(1)置信度评估",
    "context2": "在提取出候选上下位关系对 $( x , y )$ 之后,可以使用统计方法来计算置信度分数，得分低的关系对将会被过滤。KnowItAll系统[8]根据搜索引擎查询结果的命中次数来估算$x$ 和 $y$ 的逐点互信息(PMI)。除了识别结果的统计特征,Luu等[9]考虑了外部因素,如WordNet等词典中所包含概念以及数据源的可信赖度。对于CN-Probase,Chen等[2提出了三种有效的启发式策略来过滤提取的错误\"is-a\"关系以提高精度。Google的\"KnowledgeVault\"[21]构建经验表明,评估置信度分数对于从不同抽取系统中获取和融合知识至关重要。值得注意的是,负面的依据也可以用来计算置信度分数。Wang等[22结合投影模型的预测以及模式的统计数据来给每对关系提供正面分数和负面分数,其中负面分数可通过丢弃错误预测的关系来提高准确性。",
    "context3": "(2)基于分类的验证",
    "context4": "这些方法通过训练分类器 $f$ 来预测所抽取关系对 $( x , y )$ 的正确性,选用的典型模型主要包括支持向量机(SVM）、逻辑回归以及神经网络。而分类器 $f$ 所用特征大概可以划分为以下几类：表层名称、语法、统计信息、外部资源等。Snow等[23]将 $x$ 和 $y$ 之间的依赖路径用作相应词汇句法模式中的特征。Ritter等介绍了一系列的特征，这些特征主要基于关系对和Hearst模式的匹配频率,如模式\" $\\boldsymbol { x } \\ i s \\ a \\ y$ \"在语料库中的匹配数量。在Probase[2的构建中,作者根据朴素贝叶斯分类器的结果评估了提取出的\"is-a\"关系的合理性。基于语料库和维基百科摘要,Bansal等[24进一步采用了从Hearst模式匹配中获得的统计数据,这是因为维基百科的摘要中包含\"is-a\"关系对中的概念的定义和简介。"
  },
  "2.2 基于分布的方法": "",
  "2.2.1无监督方法": {
    "context1": "最早的分布方法是无监督的,为每个 $( x , y )$ 单词对分配一个分数,其中上下位关系对的期望分数比负样例的分数更高。根据对不同文章中方法的总结和分析，无监督方法可以细分为基于分布假设、基于分布包含假设(DIH)、基于分布信息假设以及其他方法。",
    "context2": "(1)基于分布假设",
    "context3": "按照分布假设,相似的词共享许多上下文信息,因此它们之间有很高的相似度。尽管上下文关系是不对称的,但相似性却是其特性之一[25]。关于相似性度量的方法有很多,比如余弦、Jaccard 相似系数、Jensen-Shannon 散度、Lin相似度以及APSyn等[2]。另外,有关分布相似性度量的详细概述可以在向量空间语义模型的早期调查中找到[27]。",
    "context4": "(2)基于分布包含假说",
    "context5": "受到分布包含假说的启发，研究者们开始对\"is-a\"关系的不对称性进行不对称测度建模。它假定一个下位词仅在其上位词的某些上下文中出现,但是一个上位词在其下位词的所有上下文中出现。例如,“fruit\"的概念比\"apple”,\"ba-nana\"和\"pear\"等下义词具有更广泛的语境。相关研究包括WeedsPrec、BalAPInc、ClarkeDE、cosWeeds、invCL、Weighted-",
    "context6": "Cosine等[28]。Roller等[29]验证了DIH的正确性,但是仅在将其应用到相关维度时才成立。",
    "context7": "(3)基于分布信息假说",
    "context8": "一些研究表明分布包含假说并不是在所有情况下都正确,例如\"American\"是\"Barack Obama\"的上位词,但是在政治相关的语境下，“Barack Obama\"不能被\"American\"涵盖。根据分布信息假设，上位词比下位词的信息量更少,因此上位词比下位词更可能出现在更一般的情况下，内容更加笼统。为了解决上述问题,Santus等[2]介绍了SLQS,这是一种新的基于熵的度量方法，用于在分布语义模型(DMSs)中无监督地识别上下位关系及其方向。在这之后,SLQS的一些变体就出现了,如SLQS Sub和SLQSRow[28],它们在上下位关系识别任务中都有不错表现。"
  },
  "(4)其他方法": {
    "context1": "Santus等[28对上面的一些关系度量方法进行了评测工作,实验表明这些度量方法并没有比其他方法全都取得更好结果。一些研究也尝试使用聚类、形式概念分析等统计方法。这些方法的使用通常与知识组织系统相联系，如,基于形式概念分析，Tovar等[3提出了一种自动识别受限领域本体中关系的方法;Takahiro等[3为了实现对科学技术叙词表的维护，采用了基于熵的词向量聚类方法;为了解决传统的分类法组织方法忽略主题邻近性和语义相关性,Zhang等[32]提出了利用自适应球形聚类的递归方法来构建主题分类法。聚类和形式概念分析的方法通常准确率较低,而且均无法应对大规模数据的解析。"
  },
  "2.2.2有监督方法": {
    "context1": "近几年,研究界的重点转移到了有监督的分布方法，诸如像英语这样含有丰富资源的语言，有了可用的训练集,就可以根据术语对 $( x , y )$ 的表示采用分类方法来训练预测模型。另外,词嵌入投影建模如何将下位词的词向量投影至上位词的词向量,以训练上下位关系分类模型。"
  },
  "(1)分类": {
    "context1": "在分类方法中，最受欢迎的术语表示方法是由预训练神经网络生成的词嵌入,如Word2Vec、Glove、fasttext和ivL-BL。SensEmbed则会针对同一术语的不同含义生成不同嵌入。Baroni等33采用xy的组合方式来表示术语对的含义,其中 $\\vec { x }$ 表示术语 $x$ 的词向量，然后训练像SVM的分类器,该方法在一些研究中被认为是很强的基准。但是一些工作发现这种模型存在严重的\"词汇记忆\"问题[34]。这意味着分类器学习的其实是术语语义,而不是术语之间的关系。因此,当训练集和测试集明显不同时,模型的性能会很差。",
    "context2": "为了解决这个问题,研究者提出使用向量偏移作为特征,即 ${ \\vec { y } } - { \\vec { x } } ^ { [ 3 5 ] }$ 。Roller等[29提出了一个同时使用向量差和平方向量差的不对称模型。之后,Turney等[36又提出了simDiff模型,该文献中还提到了其他的向量组合,如向量和 $\\vec { x } + \\vec { y }$ 以及向量点积 $\\vec { x } \\cdot \\vec { y }$ 。Roller和 $\\mathrm { E r k } ^ { [ 3 7 ] }$ 探索了分布向量中的Hearst模式，并介绍了类似于PCA的迭代程序来学习分类器。除了单一考虑上述特征外， $\\mathbb { W } \\mathrm { a n g }$ 等[38]将 $\\vec { y } - \\vec { x }$ 和x",
    "context3": "进行了一定的加权组合。",
    "context4": "上述表示方法的基本思想是：在相似上下文中出现的单词具有相似的词嵌入。但是模型仍然是基于DIH,并且选定的特征依赖于术语的上下文特征，这意味着它们与训练语料密切相关。因此一些研究直接利用预提取的上下位关系对来训练术语嵌人,即设计独立的嵌入学习算法。 $\\mathrm { Y } _ { \\mathrm { U } }$ 等[39]比较早开始了这方面的研究，对于每个单词 $x$ ,他们分别学习了术语的下位词嵌入 $\\overrightarrow { x } _ { o }$ 和上位词嵌入 $\\overrightarrow { x } _ { e }$ 。之后 $\\mathrm { L u u }$ 等[40]通过在动态加权神经网络中对上位词和下位词之间的上下文进行建模,进一步扩展了该方法。直接训练的术语嵌入通常是分布共现编码,无法将上下位关系与其他关系区分开，Nguyen等[4提出了一种新颖的基于负采样的神经模型Hy-perVec。考虑到上位词的上下文一般在语义上包含下位词的上下文,Chang等[4提出了分布式包含嵌入模型。一些研究关注双曲嵌入空间,基于Lorentz模型,Nickel等[43提出将概念结构嵌入到双曲嵌入空间中。Le等[44]利用双曲嵌入从大型文本语料中推断概念层次结构。"
  },
  "(2)词嵌入投影": {
    "context1": "线性投影是通过模型将 $x$ 映射到接近 $y$ 的向量来预测一对 $( x , y )$ 。这一思路不仅能保留分布方法的优点，还能在词嵌入空间中显示表达关系，克服了“词汇记忆\"问题。$\\mathrm { F u } ^ { [ 4 5 ] }$ 是这个领域的先驱者，提出了均匀线性投影和分段线性投影。随后，在 $\\mathrm { F u } ^ { [ 4 5 ] }$ 研究的基础上提出了改进的方法。Wang等[22提出了迭代式投影模型,通过更新映射过程中的转换矩阵并迭代提取新的\"is-a\"系。负采样技术被证明可以有效地增强投影学习,Ustalov等[4充分利用了非\"is-a\"关系的语义,在 $\\mathrm { F u }$ 等[45]的研究模型中增加了一个正则项。Ya-mane 等[47提出了协同训练式投影模型,负例采用机器自动生成。相似的研究还有Wang等[3提出的多个Wahba模糊正交投影模型以及文献【48]。如Yamane等[4]的研究所述，就F值而言，这些方法可与最新的分类方法相媲美。最近，汪诚愚等3基于已有研究成果，详细阐述了基于词嵌入投影模型的研究进展，并对在公开中英文公开数据集上进行了充分的评测,探讨了不同投影模型在不同场景下的优缺点。"
  },
  "2.3方法分析": {
    "context1": "在文献中,研究者们对哪种方法能够更有效地识别上下位关系存在一些分歧。Shwartz等[49声称分布方法优于基于模式的方法,而Levy等[34认为分布方法不太管用。最近，Roller等[5研究了两种方法在多个相关任务上的性能,发现基于简单模式的方法在通用基准数据集上始终优于分布方法，原因可能是模式提供了重要的上下文约束，而这些约束尚未在分布方法中捕获。这一小节概述了本章节的主要内容,并分析了两种方法的利。",
    "context2": "模式的方法基于语料库中连接 $x$ 和 $y$ 之间的词汇句法路径来识别上下位关系，该路径显式地表达了\"is-a\"关系。最初的Hearst模式和更泛化的模式已经用于大量分类法、本体以及知识图的构建。Nakashole等[指出模式作为特征的缺点是会导致特征空间的稀疏性，而且大多数的方法需要 $x$ 和 $y$ 同时出现在相同的句子中。这导致召回率普遍很低,因此基于模式的方法未能确定许多有价值的\"is-a\"关系。另外,模式过度依赖语言类型，而且如果在其他语言中没有类似的Hearst模式的话会难以应用。",
    "context3": "分布方法使用从上下文中派生出来的单词表示,而与它的上位词和下位词无关。词嵌入使机器可以基于整个语料库进行预测。但是分布的方法在检测特定、严格的上下位关系时不太精确，并且倾向于发现术语之间广泛的语义相似性。Weeds等35还发现通过分布方法识别的关系可能是共下位关系和整体部分关系,而不是上下位关系。另外,词表示依赖于领域,且模型与训练集密切相关,然而建立标记好的训练集是昂贵的，并且这样的数据集在实践中不总是可用。Levy等[34进一步提出了分布方法所存在的问题,他们发现有监督的方法实际上学习成对单词中一个单词的独立属性,而不是学习两个单词之间的关系。这是因为x和特征是独立生成的,它们是通过内核函数将对内相似性与差集模型集成在了一起,但实现的仅仅是增量改进。不过，线性投影的方法可以减轻了“词汇记忆\"问题。",
    "context4": "尽管两种方法都存在一些缺点,但基于模式的方法和基于分布的方法被认为是互补的。融合它们的想法很早就被提出来了,但是多年来都没有引起太大关注。Shwartz等[49]提出了HypeNET系统,它通过分布特征和基于模式的特征来表示一对 $( x , y )$ ,其中的每个模式都使用长短期记忆网络(LSTM)对相应的依赖路径进行编码。随后,又提出了Hy-peNET的改进系统 $\\mathrm { L e x N E T } ^ { [ 5 1 ] }$ ,并且验证了两种方法确实是互补的，还分析了每种方法的贡献。在SemEval-2018的任务9中，许多提交的系统都采用了混合的方法，如获得第一名的Bernier等4结合了基于模式的方法与线性投影。语言模式也能和关系度量结合，Roller等[5o提出基于Hearst模式的上下位度量。Le等[44]进一步改进了Roller等[50的研究,提出结合Hearst模式和双曲嵌入。",
    "context5": "随着深度学习发展,神经网络能学习上下位关系和非上下位关系的线性和复杂非线性特征。Wang等[52设计了一种基于注意力机制的Bi-GRU-CapsNet端到端上下位关系分类模型;在SemEval-2018的任务9中,Zhang等[53]通过经验研究了各种神经网络模型。基于神经网络的投影模型也被广泛应用于上下位关系识别[38.48]。"
  },
  "3中文语料上下位关系识别": {
    "context1": "相较于英文，从中文文本语料库中识别上下位关系更是一项艰难的挑战。从语言学的角度看,中文是表意文字的一种形式,其词的结构、语义和语法是灵活和不规则的,如中文在单复数、时态以及词边界上没有明显区别,而且中文的词序和英语相比有很大不同。目前，基于分布的方法和基于模式的方法在英语语料上已经具有较高的精度和召回,但在中文语料下有以下难点： $\\textcircled{1}$ 中文的表达方式非常灵活且具有复杂的句法结构和语法规则； $\\textcircled{2}$ 中文是一种资源相对匮乏的语言，缺少语料库资源。为了克服上述难点，随后的研究主要结合中文语言特点从模式规则和分布统计两个角度来改进中文上下位识别方法。"
  },
  "3.1基于模式的方法": {
    "context1": "(1)模式扩充",
    "context2": "上下位关系识别已经有很多效果不错的方法，尽管基于模式的方法覆盖率低，但这样的方法也可以实现相对较高的精度[50]。早期的中文上下位识别研究主要基于原有的英文模式。与英文“ $[ E ] i s a [ C ]$ \"模式相对应的中文模式为“是一个\"模式,刘磊等[54提出了一种从符合“是一个\"模式的句子中获取下位概念的方法，主要是利用半自动获取的词典和句型对\"是一个\"模式进行分析。除了“是一个\"模式,随后Fu等[55还手动设计了几种中文风格的Hearst模式，如表3所示。",
    "context3": "已有的英文模式过度依赖语言类型,在不同语言中的可移植性差。于是一些研究开始尝试设计新的中文模式,Wang等[22]收集了与\"is-a\"关系有关的泛化模式,并将它们分为三种类型:\"Is-A”、“Such-As\"和\"Co-Hyponym”;其中在\"Is-A\"类型中扩展了\"E是C之一\"模式,在\"Such-As\"类型中扩展了\"C(，)例如E\"和\"C(，)包括E\"两种模式。",
    "context4": "表3中文风格的Hearst模式",
    "context5": "<table><tr><td>Pattern</td><td>Translation</td></tr><tr><td>E是(一个/一种)C</td><td>E is (a kind of) C</td></tr><tr><td>C(、)等H</td><td>E(.)and other C</td></tr><tr><td>C(,)叫(做)E</td><td>C(.) called E</td></tr><tr><td>C(,)(像)如E</td><td>C(.) such as E</td></tr><tr><td>C(,)特别是E</td><td>C(,) especially E</td></tr></table>"
  },
  "(2)模式改进": {
    "context1": "模式扩充也是依赖模式从中文文本语料中进行字符串匹配,存在一些明显的缺点： $\\textcircled{1}$ 模式关键字中的字符串可能包含一个以上的名词或非名词; $\\textcircled{2}$ 无法对未登录词进行分类； $\\textcircled{3}$ 未能充分使用词语的语义信息。在这种情况下，多策略混合在一定程度上提高了识别的准确性和召回率。Huang等[提出了更加全面、泛化的中文模式,并使用最长修饰符匹配和依赖项解析来提高匹配成功率。陈金栋等[56]根据句法模板的质量将其分为更细粒度的强句法模板和弱句法模板,并将语义信息融人弱句法模板来构建语义模板。孙佳伟等为了降低模式匹配的难度,提出了出一种融合短语模式和词模式的上下位关系分类方法。Wang等[5考虑了中文语言的特性,综合运用了中文上下位关系模式和同下位词关系模式来进行迭代学习，避免了“语义漂移\"问题。",
    "context2": "词向量既能包含上下文信息，也在很大程度上表示词的语义,能够解决未登录词的表示,这一特点使得考虑了词嵌入的方法具有较高召回率。马晓军等[58基于词向量的相似度计算方法对种子集中包含的上下位关系模式进行聚类，然后筛选出置信度高的模式并对未标注语料进行上下位关系识别。孙佳伟等将词嵌入应用于语言模式中,解决了语言模式的稀疏问题。",
    "context3": "表4中文识别技术分类表",
    "context4": "<table><tr><td></td><td>方法类型</td><td>关键词</td></tr><tr><td rowspan=\"3\">中文上下位关系识别 技术</td><td>分类</td><td>支持向量机(SVM）、条件随机场(CRF)、决策树、朴素贝叶斯、随机森林、神经网络、关系分类、注意 力机制、Wikipedia、症状和疾病、症状知识库、分类法、复合实体、社交标签、知识挖掘、知识图谱、本 体、知识推理</td></tr><tr><td>词嵌入投影</td><td>词嵌入、分段线性投影、迭代投影模型、转导学习、神经网络、分类体系、语言规则、知识增强</td></tr><tr><td>其他</td><td>K-means、BIRCH算法、布朗聚类、形式概念分析、奇异值分解、PCA降维、LDA、层次关系、专利、数 字图书馆、医学学科资源、气象灾害、主题词表、本体、知识建模、知识检索、矩阵分解</td></tr></table>"
  },
  "3.2 基于统计的方法": {
    "context1": "对于像中文这样表现灵活和不规则的语言，英文的上下位关系识别方法不一定适合中文。表4是中文上下位关系识别技术的主题分类，并对相关关键词进行了展示。基于统计的方法通过对大规模语料库的统计处理发现规律，从而识别上下位关系，主要分为：分类、词嵌入投影以及其他。",
    "context2": "分类方法集中在机器学习算法的研究上，机器学习也是研究成果中出现最多、应用最广泛的信息抽取技术，主要涵盖支持向量机、条件随机场、决策树、朴素贝叶斯、神经网络等。考虑到中文语言的特性,分类方法通常会结合额外的语言规则、句子结构特征、句法特征、词典以及知识库等。黄毅等[59利用领域术语所在百度百科名片中结构化的上下位信息,提炼特征词词典。《知网》(HowNet)是一个以汉语和英语的词语所代表的概念为描述对象，以揭示概念与概念之间以及概念所具有的属性之间的关系为基本内容的常识知识库,并用义原树来描述词汇之间的关系。王婷等[抽取了《知网》中已有的上下位信息构成上下位词典，并在此基础上获取了词典特征。类似的知识库还有同义词词林、CN-Probase[20]等。多种特征以及知识库的融合在一定程度上能够有效提高识别的准确率,但分类方法人面临着一些挑战，如特征构建的过程随机、难以复制且不可控，知识库的维护更新代价太高等。",
    "context3": "最新的中文上下位识别方法是基于词嵌入的投影模型，马晓军等[58训练了单投影模型进行领域实体层级关系组织。Wang等提出了一种为中文量身定制的转导投影学习模型，该模型引入转导学习正则项来建模非线性因子，同时利用正例(上下位关系)和非正例(非上下位关系)来训练模型。投影模型在多个英文数据集上取得了State-of-the-art的结果,在中文上的表现还有很大提升的空间。深度学习能够自动学习特征，避免了繁重耗时的特征工程,在分类和词嵌入投影方法中都有涉及。Wang等2设计的深度学习模型在中英文症状和疾病语料库上都取得了不错的结果。孙佳伟等利用简单的前馈神经网络和softmax结构来实现了关系的分类。深度神经网络可以有效地学习上下位关系和非上下位关系的复杂非线性投影,Wang等[48在中英文数据集上实验了基础和扩展的神经网络投影模型。",
    "context4": "除了分类和词嵌入投影方法，中文上下位关系识别还采用了聚类、形式概念分析、奇异值分解等,它们不需要利用各种相似性度量方法,而是从语料中获取词语特征来识别上下位关系。另外从文献关键词中可以发现,针对具体问题,分类法、本体以及知识库等的构建和维护是广泛关注的问题;从研究对象的呈现类型看,包括用户生成内容、百度百科、中文维基百科、开放信息、web信息等。"
  },
  "4资源与应用": "",
  "4.1资源": {
    "context1": "上下位识别研究需要有多种资源的支持,包括高质量的分类法、知识库和语义网络，如表5所示。典型的英语资源有Freebase、WordNet、YAGO、Probase、WikiTaxonomy、WiBi、DefIE等,多语言资源包括YAGO3、BableNet、MultiWiBi。早期由于缺乏中文版的WordNet,一些中文语义词典就被编写了出来,如\"Sinica BOW”、“SEW”“COW\"等;随着多年的发展与积累，出现了大词林（Bigcilin）、CN-Probase、ChineseWikiTaxonomy、HowNet、同义词林(扩展版)等中文知识库。除了分类法、知识图之外，还存在一些为了进行上下位关系研究而构建的真实数据集[3],英文包括两个通用领域的数据集BLESS 和ENTAILMENT、三个特定领域的数据集：ANI-MAL、PLANT和VEHICLE;中文包括两个公开数据集FD和BK。此外，一些研究者考虑到数据集的平衡性，在标准数据集的基础上进行了整合，如Shwartz数据集[49],该数据集包含14135个正例和16956个负例。",
    "context2": "表5知识库统计信息",
    "context3": "<table><tr><td>Name</td><td># of concepts</td><td># of isa pairs</td></tr><tr><td>Freebase</td><td>1,450</td><td>24,483,434</td></tr><tr><td>WordNet</td><td>25,229</td><td>283,070</td></tr><tr><td>YAGO</td><td>352,297</td><td>8,277,227</td></tr><tr><td>WikiTaxonomy</td><td>111,654</td><td>105,418</td></tr><tr><td>Probase</td><td>2,653,872</td><td>20,757,545</td></tr><tr><td>CN-Probase</td><td>270,000</td><td>33,000,000</td></tr><tr><td>Chinese WikiTaxonomy</td><td>79,470</td><td>1,317,956</td></tr><tr><td>Bigcilin</td><td>10,041,344</td><td>12,081,984</td></tr><tr><td>HowNet</td><td>53,335</td><td>12,563</td></tr><tr><td>同义词词林(扩展版)</td><td>77,456</td><td>&gt;10.000</td></tr></table>",
    "context4": "SemEval-2018的任务9是专门为上下位关系识别而设计的共享任务。这项任务有两个子任务，任务一是发现通用语料库中的上位词,并提供了三种不同语言的数据：英语、意大利语和西班牙语;任务二是识别医疗和音乐领域的上位词[4。在中文方面,2017年的第六届国际自然语言处理与中文计算会议(简称\"NLPCC 2017\")提出了汉语词语语义关系分类任务，主办方公开了用于评价的2000对词语对，其中上下位词对有500。在以前的研究中，除了Wikipedia、百度百科等通用语料库之外，为了进行领域性的上下位关系研究,研究者以研究目标为导向构建了不同的上下位关系库，其中包括科技论文、生物医学、与动植物相关的网页、旅游、法律、MH370、恐怖主义报告、疾病报告和电子邮件等。这些数据集对于正确评价上下位关系识别模型来说是无价的。"
  },
  "4.2应用": {
    "context1": "知识组织的目标是使知识从无序走向有序，降低知识的分散化,促进知识传播与利用。信息技术的快速发展促进了知识组织方法和技术的不断发展，如何借助新的知识组织方法和技术来帮助人们快速有效地获取所需信息，一直是图书情报领域的重要研究课题。另外，上下位关系识别技术还能够改善许多自然语言处理的下游任务，国内外学者在相关方面的研究从未间断。"
  },
  "4.2.1 知识库的构建与服务": {
    "context1": "上下位关系是分类法、知识本体和知识图谱等知识库的重要基石。面对多源异构、庞大复杂的数据,知识库的构建离不开上下位关系识别技术的支持。上下位关系识别技术的应用,能够及时快速地处理海量数据，为知识库构建节省了大量的人力资源。",
    "context2": "目前,针对分类法、本体、知识图等知识库的构建已经做了大量的研究。对于科技文献的处理,基于人工构建规则或统计的方法,完成了对科技文献的属性抽取,从而自动建立了主题分类法或本体。针对科技文献，上下位关系识别技术实现了文本内容中重要术语的抽取，句子级关系抽取等,在信息检索、知识建模、自动标注也有所应用,帮助信息资源的有效利用[13]。除了科技文献外,知识库中的\"is-a\"知识还可以来源于自由文本。Probase中[2的\"is-a\"知识是利用模式从数十亿个网页和数年的搜索日志中获得的。王婷等[利用医疗健康网站上的数据构建了具有上下位关系的症状知识库。从涉及的领域来看,上下位关系识别技术应用范围十分广泛,涵盖了生物医学、计算机科学、法律、数字图书馆、电子商务、专利、气象灾害等。",
    "context3": "在语义网环境下,基于上下位关系识别技术构建的知识库包含着大量的结构化语义知识,在面对用户个性化的信息需求时,能够辅助信息服务工作者提供主动、友好、准确的信息[58]。通过对用户的信息行为进行信息抽取,例如用户的访问数据、搜索日志、浏览记录等资料,实现对用户信息需求的分析，并以此为依据提供个性化的信息推送服务。"
  },
  "4.2.2 自然语言处理": {
    "context1": "上下位关系对于各种下游NLP任务也至关重要，如文本理解、问答系统、推荐系统,信息检索。 $\\mathrm { H u a }$ 等[2]建立了一个短文本理解的原型系统，该系统不仅利用了Probase中的语义知识,还从网络语料库中自动获取更多的语义知识。对于问答系统,Yang等[提出了一种可以模拟人类思维方式的问题理解方法，其关键部分是一个包含大量技术信息的层次知识图。在此基础上构建的问答系统，能够为信息用户提供接近人工问答水平的体验以及准确无误的服务质量。最近,Zhang等[64基于树状数据集提出了一种基于分类法感知的降噪自动编码器模型，该模型利用分类学的知识来辅助推荐系统，以提高推荐精度并缓解数据稀疏性和冷启动问题。在信息检索方面，为了解决电商平台上对产品静态信息和动态评论关联上的不足,丁晟春等[提出基于知识图谱的产品画像构建方法,研究结果表明该方法能够有效帮助电商平台改善产品对比和产品搜索等机制,为用户提供更好的产品服务。"
  },
  "5总结与展望": {
    "context1": "根据上述分析，我们对上下位关系识别研究提出了如下建议。"
  },
  "(1)集合表示和深度架构": {
    "context1": "Shwartz等[49较早的将基于模式的方法和分布方法相结合起来进行研究;在SemEval-2018的任务9中，表现最好的CRIM系统结合了线性投影与Hearst模式[4;最近孙佳伟等[]和Le等[4将基于模式的方法与基于词嵌入的方法进行了有效融合。这些研究从不同方面都显著提高了\"is-a\"关系的识别性能,因此如何相互加强两种类型的方法会是一个不错的研究方向。不论是中文还是英文,神经网络模型可以有效地学习更深层次的特征表示,而且它也有可能解决\"词汇记忆\"问题,通过添加从模式中挖掘的术语对之间的语义相关性信息到分布式表示中。",
    "context2": "尽管上面提到了几种嵌入的学习方法，但是基于深度学习的上下位关系识别的成功范例有限。如文献【52]和文献【4】研究了各种神经网络模型，以建立空间中词语的表示形式;文献[48]中使用了常用于图像生成、序列建模的对抗学习。可能原因是难以为神经网络设计单个目标以针对该任务进行优化。另外,由于很多领域缺乏大量规范文本、标注语料、基础本体或词库等领域基础资源条件,深度学习的研究应用目前仍主要集中在生物医学领域和通用领域,其他领域还非常少。如何利用深度学习的热潮进行上下位关系的获取值得将来研究。",
    "context3": "(2)基准与评估",
    "context4": "上下位关系的基准对于量化最新方法的表现非常重要，它应该包括文本语料库、黄金标准和评估指标。Cama-cho-Collados等[4已经在医疗和音乐领域以及英语、意大利语和西班牙语中提供了一些黄金标准,Bordea等[提出的几种领域和语言的分类法标准也可以辅助评估上下位关系。",
    "context5": "以前的工作已经指出了当前基准和方法中的几个问题。Levy等[34证明了受监督的系统由于\"词汇记忆\"问题而实际上表现不佳。Shwartz等[28提示无监督方法比有监督方法更健壮,但有监督方法优于无监督方法。Roller等[o发现基于简单模式的方法在通用基准数据集上始终优于分布方法。我们可以看到，应该针对更完整、被广泛接受的评估基准进行更深入的研究。",
    "context6": "(3)整合领域知识",
    "context7": "领域知识对于领域特定语料库中的术语和关系提取至关重要,但很难从这种有限的语料库中获取知识。通过利用从领域知识库中得出的事实，可以利用远程监督来学习领域上下位关系[48],并且比现有方法具有更高的覆盖范围。因此,基于文本语料库和特定领域的知识库来获取上下位关系也是一项重要的任务。"
  },
  "(4)非英语和资源匮乏的语言": {
    "context1": "对于资源匮乏的语言，本文概述了中文上下位关系识别的研究情况,其他相似语言尚未对本文解决的任务进行广泛的研究。具体而言，虽然基于模式的方法对英语有效,但在很大程度上取决于语言。中文方面的研究取得了一定的成功,但是在通用性以及效果上还不够理想。句法分析和命名实体识别在中文语料上的表现仍需改进，以支持稳健的关系提取。此外,由于基于英汉翻译的词序差异很大,因此仍然难以使用基于机器翻译的方法来提取这种关系。如何将现有方法应用于与英语有显着差异的语言(例如,中文、阿拉伯文和日文)值得研究。"
  },
  "(5)应用领域": {
    "context1": "上下位关系识别技术具有很高实用价值，而且较为容易应用于实践,取得的实际效果也不错。不过，当前上下位关系识别技术应用范围主要集中在知识库的构建和自然语言处理的下游任务，更多的应用领域还有待研究人员进一步探索与发掘，如在图书情报领域的研究。图书情报领域的研究本质上是关于信息、知识的科学,在信息时代下，上下位关系识别也是图书情报领域中知识组织与建设、信息服务、情报分析等研究的重要组成部分。因此,研究如何开展信息服务、支持决策支持以及进行综合集成等也可以成为有价值的研究课题。",
    "context2": "本文开展了一项关于从文本语料库中识别上下位关系的调研,首先阐述了基于模式和分布的方法，以从文本中学习上下位关系，并讨论了相关的资源以及上下位关系识别技术的应用。尽管取得了巨大的成功,但这项任务还远远没有解决。通过解决本文中讨论的问题,我们建议可以在更多的领域和语言中进行上下位关系识别研究，从而对自然语言处理和图书情报领域研究产生更大的影响。"
  },
  "参考文献": {
    "context1": "1 孙佳伟,李正华,陈文亮,张民.基于词模式嵌入的词语上 下位关系分类[J].北京大学学报(自然科学版),2019,55 (1):1-7.   \n2 Wu W,Li H,Wang H,et al.Probase:A probabilistic taxonomy for text understanding[C]//Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012: 481-492.   \n3 汪诚愚,何晓丰,宫学庆,周傲英.面向上下位关系预测的词 嵌入投影模型[EB/OL].http://kns.cnki.net/kcms/detail/11.1826.TP.20191031.1234.002.html,2020-03- 21.   \n4 Camacho-Collados J,Delli Bovi C,Espinosa-Anke L,et al. SemEval-2018 task 9: Hypernym discovery [C]//Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)；2018 $\\mathrm { J u n } ~ 5 \\mathrm { - } 6$ ; New Orleans, LA. Stroudsburg (PA): ACL; 2018:712 - 24.   \n5Hearst M A.Automatic acquisition of hyponyms from large text corpora[C]//Proceedings of the 14th conference on Computational linguistics-Volume,Association for Computational Linguistics,1992: 539-545.   \n6Kozareva Z, Hovy E. A semi-supervised method to learn and construct taxonomies using the web[C]// Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics,2010: 1110-1118.   \n7 Ritter A, Soderland S,Etzioni O. What Is This,Anyway: Automatic Hypernym Discovery[C]//AAAI Spring Symposium: Learning by Reading and Learning to Read,2009: 88-93.   \n8 Anh T L,Kim J, $\\mathrm { N g ~ S ~ K }$ . Taxonomy construction using syntactic contextual evidence[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),2014: 810-819.   \n9 Snow R, Jurafsky D, $\\mathrm { N g }$ A Y. Learning syntactic patterns for automatic hypernym discovery[C]//Advances in neural information processing systems,2005: 1297-1304.   \n10 Navigli R,Velardi P.Learning word-class lattices for definition and hypernym extraction[C]//Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.Association for Computational Linguistics, 2010:1318-1327.   \n11 Nakashole N,Weikum G, Suchanek F.PATTY: a taxonomy of relational patterns with semantic types[C]//Proceedings of the 2O12 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics,2012:1135-1145.   \n12Kozareva Z,Riloff E,Hovy E. Semantic class learning from the web with hyponym pattrn linkage graphs[C]// Proceedings of ACL-08: HLT,2008: 1048-1056.   \n13 Huang S,Luo X,Huang J,et al. An unsupervised approach",
    "context2": "for learning a Chinese IS-A taxonomy from anunstructured corpus[J].Knowledge-Based Systems，2019,(182): 104861.   \n14Alfarone D,Davis J. Unsupervised learning of an IS-A taxonomy from a limited domain-specific corpus[C]//Proceedings of the 24th International Conference on Artificial Intelligence,2015: 1434-1441.   \n15Gupta A, Lebret R,Harkous H,et al. Taxonomy induction using hypernym subsequences[C]//Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM,2017: 1329-1338.   \n16Suchanek F M, Kasneci G,Weikum G. Yago: A large ontology from wikipedia and wordnet[J]. Web Semantics: Science,Services and Agents on the World Wide Web,2008, 6(3): 203-217.   \n17 Gupta A,Piccinno F, Kozhevnikov M, et al. Revisiting taxonomy induction over wikipedia[C]//Proceedings of COLING 2016,the 26th International Conference on Computational Linguistics: Technical Papers, Osaka, Japan,December 11-17 2016,2016 (CONF): 2300-2309.   \n18 Etzioni O, Cafarella M, Downey D,et al. Web-scale information extraction in knowitall:(preliminary results) [C]// Proceedings of the 13th international conference on World Wide Web.ACM,2004: 100-110.   \n19 Anh T L, Kim J, $\\mathrm { N g ~ S ~ K }$ Incorporating trustiness and collective synonym/contrastive evidence into taxonomy construction[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Procesing,2015: 1013-1022.   \n20 Chen J, Wang A, Chen J, et al. CN-Probase: A Data-driven Approach for Large-scale Chinese Taxonomy Construction[C]//2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEE,2019:1706-1709.   \n21 Dong X, Gabrilovich E,Heitz G,et al. Knowledge vault: A web-scale approach to probabilistic knowledge fusion[C]// Proceedings of the 2Oth ACM SIGKDD international conference on Knowledge discovery and data mining.ACM, 2014:601-610.   \n22Wang C,Fan Y,He X,et al. Predicting hypernym- hyponym relations for Chinese taxonomy learningD]. Knowledge and Information Systems,2019, 58(3): 585-610.   \n23 Snow R, Jurafsky D, $\\mathrm { N g }$ A Y. Semantic taxonomy induction from heterogenous evidence[C]//Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.Association for Computational Linguistics,2006: 801-808.   \n24Bansal M, Burkett D, De Melo G,et al. Structured learning for taxonomy induction with belief propagation[C]// Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014:1041-1051.   \n25Santus E, Lenci A,Lu Q,et al. Chasing hypernyms in vector spaces with entropy[C]//14th Conference of the European Chapter of the Association for Computational Linguistics.EACL (European chapter of the Association for Computational Linguistics),2014: 38-42.   \n26Santus E,Chiu T S,Lu Q,et al. Unsupervised measure of word similarity: how to outperform co-occurrence and vector cosine in VSMs[C]//Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,2016: 4260-4261.   \n27Turney P D,Pantel P. From frequency to meaning: Vector space models of semantics[]. Journal of artificial intelligence research,2010,(37): 141-188.   \n28Shwartz V, Santus E, Schlechtweg D. Hypernyms under Siege:Linguistically-motivated Artillery for Hypernymy Detection[C]//Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1,Long Papers,2017: 65-75.   \n29Roller S,Erk K, Boleda G. Inclusive yet selective: Supervised distributional hypernymy detection[C]//Proceedings of COLING 2014,the 25th International Conference on Computational Linguistics: Technical Papers,2O14:1025- 1036.   \n30Tovar M,Pinto D,Montes A,et al.Patterns used to identify relations in corpus using formal concept analysis[C]// Mexican Conference on Pattern Recognition,Springer, Cham,2015:236-245.   \n31 Kawamura T, Sekine M, Matsumura K. Detecting Hypernym/Hyponym in Science and Technology Thesaurus Using Entropy-Based Clustering of Word Vectors[J]. International Journal of Semantic Computing,2017,11(4): 433-449.   \n32 Zhang C, Tao F, Chen X,et al. Taxogen: Unsupervised topic taxonomy construction by adaptive term embedding and clustering[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM,2018: 2701-2709.   \n33 Baroni M, Bernardi R,Do N Q,et al. Entailment above theword level in distributional semantics[C]//Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.Association for Computational Linguistics, 2012: 23-32.   \n34Levy O,Remus S,Biemann C,et al.Do supervised distributional methods really learn lexical inference relations? [C]//Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2015: 970-976.   \n35Weeds J,Clarke D,Reffin J,et al.Learning to distinguish hypernyms and co-hyponyms[C]//Proceedings of COLING 2014,the 25th International Conference on Computational Linguistics: Technical Papers.Dublin City University and Association for Computational Linguistics，2014: 2249-2259.   \n36 Turney PD, Mohammad S M.Experiments withthreeapproaches to recognizing lexical entailment[J]. Natural Language Engineering,2015,21(3): 437-476.   \n37Roller S,Erk K. Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment[C]//Proceedings of the 2O16 Conference on Empirical Methods in Natural Language Processing, 2016: 2163-2172.   \n38 Wang C, Fan Y,He X, et al. A Family of Fuzzy Orthogonal Projection Models for Monolingual and Cross-lingual Hypernymy Prediction[C]//The World Wide Web Conference.ACM,2019: 1965-1976.   \n39 Yu Z,Wang H,Lin X, et al. Learning term embeddings for hypernymy identification[C]//Proceedings of the 24th International Conference on Artificial Inteligence,2015: 1390-1397.   \n40 Anh T L, Tay Y,Hui S C,et al. Learning term embeddings for taxonomic relation identification using dynamic weighting neural network[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Procesing, 2016: 403-413.   \n41 Nguyen K A,Köper M, im Walde S S,et al. Hierarchical Embeddings for Hypernymy Detection and Directionality [C]//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017: 233-243.   \n42 Chang H S,Wang Z, Vilnis L,et al. Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection[C]//Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),2018: 485-495.   \n43Nickel M,Kiela D. Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry[C]//International Conference on Machine Learning,2018: 3779-3788.   \n44 Le M, Roller S,Papaxanthos L, et al. Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings [C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,2019: 3231-3241.   \n45Fu R,Guo J,Qin B,et al.Learning semantic hierarchies via word embeddings[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2014: 1199-1209.   \n46Ustalov D,Arefyev N,Biemann C,et al. Negative Sampling Improves Hypernymy Extraction Based on Projection Learning[C]//Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,2017: 543-550.   \n47 Yamane J, Takatani T,Yamada H,et al. Distributional hypernym generation by jointly learning clusters and projections[C]//Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,2016: 1871-1879.   \n48Wang C,He X,Zhou A. Improving Hypernymy Prediction via Taxonomy Enhanced Adversarial Learning[C]// Proceedings of the AAAI Conference on Artificial Intelligence,2019.   \n49 Shwartz V, Goldberg Y, Dagan I. Improving Hypernymy Detection with an Integrated Path-based and Distributional Method[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2016: 2389-2398.   \n50 Roller S,Kiela D,Nickel M. Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora [C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2018: 358-363.   \n51SHWARTZV,DAGANI. The Roles of Path-based and Distributional Information in Recognizing Lexical Semantic Relations [J]. CoRR,2016,abs/1608.05014.   \n52Wang Q，Xu C, Zhou Y,et al. An attention-based Bi-GRU-CapsNet model for hypernymy detection between compound entities[C]//2O18 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE,2018:1031-1035.   \n53Zhang Z，Li J，Zhao H，et al. SJTU-NLP at SemEval-2O18 Task 9: Neural Hypernym Discovery with Term Embeddings[C]//Proceedings of The 12th International Workshop on Semantic Evaluation,2018: 903-908.   \n54 刘磊,曹存根,王海涛,陈威.一种基于“是一个\"模式 的下位概念获取方法[).计算机科学,2006,(9):146-151.   \n55Fu R,Qin B,Liu T.Exploiting multiple sources for open-domain hypernym discovery[C]//Proceedings of the 2013 conference on empirical methods in natural language processing,2013: 1224-1234.   \n56 陈金栋,肖仰华.一种基于语义的上下位关系抽取方法. 计算机应用与软件,2019,36(2):216-221.   \n57Wang C,He X. Chinese hypernym-hyponym extraction from user generated categories[C]//Proceedings of COLING 2016 the 26th Internatinnal Cnnferenre on Cnmniita"
  }
}