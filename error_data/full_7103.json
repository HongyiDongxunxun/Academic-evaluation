{
  "original_filename": "full_7103.md",
  "文本分类特征降维研究综述": "",
  "奉国和郑伟": {
    "context1": "1华南师范大学经济管理学院信息管理系广州510006²河北北方学院理学院张家口075000",
    "context2": "（摘要）特征降维是文本分类的关键技术之一,包括特征选择与特征抽取两类，其中特征选择按特征子集获取范围、特征子集搜索策略、特征子集评价策略等方式进行不同划分。归纳出当前特征选择与特征抽取所用的常用方法,分析各种方法的原理、指出每种方法的优势与不足,总结出相应改进算法。",
    "context3": "〔关键词）文本分类特征降维特征选择特征抽取（分类号）G354"
  },
  "Review of Feature Dimension Reduction in Text Classification": {
    "context1": "Feng Guohe'Zheng Wei²",
    "context2": "Department of InformationManagement,Schoolof Economics & Management,，South China Normal University,Guangzhou51006   \n'Collge of Science，Hebei North University，Zhangjiakou 075000",
    "context3": "[Abstract]Featuredimensionreductionisoneof thekytechnologyfortextclasification.Itincludesfeatureselectionandfeatureextractionandfeatureseletionislaifdintothreecasesaccording toobtaingfeaturesubsetareabaingfeaturesubsestrategy， featuresubsetvaluationcriterion.Featureselectionandfeatureextraction methods’advantagesanddisadvantagesareelaboratedrespectrvely，and the improved algorithm are concluded.",
    "context4": "[Keywords] text clasification feature dimension reduction feature selection feature extraction",
    "context5": "文本分类是当前研究热点，其困难之一是特征空间的高维性。高维特征空间中，特征之间可能是冗余的或者不相关的,造成高维空间处理的不便，容易出现过学习现象，时间与空间开销大，在不影响分类精度情况下,需要进行特征降维。特征降维包括特征选择与特征抽取,论文将特征选择按特征子集获取范围、特征子集搜索策略、特征子集评价策略等方式进行不同划分,并对特征选择与特征抽取主要方法进行分析,比较各种方法的优劣性与改进方向。",
    "context6": "看，可将特征选择分为局部特征选择和全局特征选择。",
    "context7": "·局部特征选择。指对每个类别选择若干个最能识别它的特征作为新特征，构成新的特征空间，达到对原始特征空间的降维。",
    "context8": "·全局特征选择。指选择对整个分类最有用的若干个特征构成新的特征空间，以达到对原特征空间的降维。",
    "context9": "不同方法，采用的选择策略不同,但是通过特殊处理(如带权均值、最大值)后,特征对特定类的重要性也可以转换成特征对整个分类的重要性。"
  },
  "1 特征降维分类": "",
  "1.1.2按特征子集搜索策略按照特征子集搜索策略,特征选择可分为穷举法、启发法和随机法三类。": "",
  "1.1特征选择": {
    "context1": "特征选择不改变原始特征空间的性质，只是从原始特征空间中选择一部分重要的特征,组成一个新的低维空间。而特征选择按特征子集获取范围、特征子集搜索策略、特征子集评价策略等方式进行不同划分。",
    "context2": "1.1.1按特征子集获取范围从特征子集获取范围",
    "context3": "·穷举法。指遍历特征空间中所有特征的组合，选取最优特征组合子集的方法。穷举法优点是一定能得到最优子集，但特征空间过于庞大，时间耗费和计算复杂度太大，实用性不强。穷举法可分为： $\\textcircled{1}$ 完备穷举法,指遍历各种可能特征子集,借助评估函数评估，具有最优值的特征组合为所求解。当特征数目为N,则可能的特征组合子集数为 $2 ^ { \\mathrm { { N } } }$ ,完备穷举法时间、与计算复杂性极高； $\\textcircled{2}$ 非完备穷举法，指借助后发式算法，减少遍历次数，典型的如分枝定界、最好优先、宽度优先等后发式算法。",
    "context4": "·后发式。一种近似算法,具有很强的主观倾向。实际应用中通过采用期望的人工机器调度规则，重复迭代产生递增的特征子集。该方法实现过程简单快速,应用非常广泛,常用方法有向前(向后）选择、决策树法、Relief方法及其变体等。缺点是不能保证结果最优，一般能够获得近似于最优解的解。",
    "context5": "·随机法。细分为完全随机方法和概率随机方法两种。完全随机方法是指纯随机产生子集，概率随机是指子集的产生依照给定的概率进行。常用的方法有LVF、遗传算法、模拟退火算法及其变体等。此类方法需要进行参数设置，并且参数值决定是否能得到最优解。如何有效地设置这些参数是一个值得研究的问题。",
    "context6": "1.1.3按特征评价标准特征选择可以看作一个优化问题,其关键是建立一种评价标准来区分哪些特征组合有助于分类，哪些特征组合存在冗余性、部分或者完全无关。根据评价函数与分类器的关系，特征选择方法分成过滤式、封装式和混合式三种。",
    "context7": "·过滤式。特征选择与分类器无关，通常是选择与目标函数相关度大的特征或者特征子集，该类算法运行效率高,适用于大规模数据集。过滤式选择又可细分为距离测度、信息测度、相关性测度、一致性测度。",
    "context8": "·封装式。特征选择采用分类器的错误概率作为评价函数，根据分类器的分类精度选择特征子集，该类特征选择算法具有使得分类器分类精度高的优点，但特征选择效率较低。M.Dash从普适性、时间消耗、精确度三个方面对各种评价函数进行了比较，发现分类错误测度泛化性差，时间消耗大，但精确度也高。",
    "context9": "·混合式。过滤器与封装器结合，取长补短。最理想的情况是性能与封装器相似，而时间复杂度与过滤器相近。典型的混合器的原理是：以反映数据内部特性的评价准则快速删除无关或冗余特征，得到规模较小的中间子集,然后利用封装器进一步细化。"
  },
  "1.2特征抽取": {
    "context1": "特征抽取通过将原始特征空间进行变换,重新生成一个维数更小、各维之间更独立的特征空间。",
    "context2": "特征抽取涉及到语义上的分析,而目前自然语言语义处理技术尚不发达，用特征抽取方法进行特征降维的效果并不显著。相比之下，特征选择选出的特征集合是原始特征集的子集,所以更易实现,方法也更加多样。论文以下部分对各种主要特征选择与特征抽取方法进行分析,比较各种方法的优劣性与改进方向。"
  },
  "2特征选择": "",
  "2.1主要特征选择方法": {
    "context1": "目前特征选择常用算法是基于信息论与统计学思想而设计。",
    "context2": "2.1.1文档频率(Document Frequency,DF）训练文本出现某词的文本数量称为文档频率。DF值低于某个阈值的词条是低频词,它们不含或含有较少的类别信息，可以去掉，以降低特征空间的维数，提高分类的精度。传统的特征选择方法中最简单的是特征词频和DF方法,DF方法最简单,但是忽略了特征项和类别间的依赖性。一般不单独采用DF进行特征选择,而是将它与其他方法结合进行选择特征。",
    "context3": "2.1.2信息增益(Information Gain,IG） 给定词条T与文本类别 $\\mathrm { C _ { i } }$ ,T与 $\\mathrm { C _ { i } }$ 的相关程度可表示为：",
    "context4": "$\\begin{array} { r l } { I G \\{ \\ T \\} } & { { } = - \\ \\displaystyle \\sum _ { i = 1 } ^ { m } P \\{ \\ C _ { i } \\} l g P \\{ \\ C _ { i } \\} + P \\{ \\ T \\} \\sum _ { i = 1 } ^ { m } P \\{ \\ C _ { i } \\} } \\end{array}$ $T ) \\ l g P \\{ \\ C _ { i } \\mid \\ T \\} \\ + P \\{ \\ \\overline { { T } } \\} \\ \\sum _ { i = 1 } ^ { m } P \\{ \\ C _ { i } \\mid \\ \\overline { { T } } \\} \\ l g P \\{ \\ C _ { i } \\mid \\ \\overline { { T } } \\}$",
    "context5": "其中 $P ( C _ { i } )$ 表示类别 $C _ { i }$ 中文本数出现概率， ${ \\mathbf { } } J ( ~ T )$ 表示包含词条 $T$ 的文本数概率， ${ \\mathcal { P } } ( C _ { i } \\mid T )$ 表示包含词条 $T$ ，又属于类别 $C _ { i }$ 的文本数概率 $, P ( \\ C _ { i } \\mid \\overline { { T } } )$ 表示不出现词条 $T$ ，但属于类别 $C _ { i }$ 的概率。实际应用中,设定阈值,将信息增益大于阈值的特征词选择为文本分类需要的特征，达到降维目的。信息增益考虑词条在类中出现和不出现的两种情况,虽然某个单词不出现也可能对判断文本类别有贡献,但实验证明,这种贡献往往远小于考虑单词不出现情况所带来的干扰,而且计算复杂。",
    "context6": "2.1.3期望交叉熵(Expected Cross Entropy，ECE)期望交叉熵与信息增益含义相似，不同之处是它不考虑词条未出现情形，所以在信息增益计算公式中没有后面部分，即无 $P \\{ \\stackrel { \\_ } { T } \\} \\sum _ { i = 1 } ^ { m } P \\{ \\ C _ { i } \\mid \\stackrel { \\_ } { T } \\} l g P \\{ \\ C _ { i } \\mid \\stackrel { \\_ } { T } \\}$ 。期望交叉熵反映文本类别的概率分布以及在出现某种特定特征词条情况下文本类别概率分布之间的距离。进行特征选择时，选择ECE值大的特征。期望交叉熵与信息增益唯一不同之处在于没有考虑词条未发生的情况。",
    "context7": "2.1.4互信息(Mutual Information,MI）MI用于评估词条T与类别 $\\mathrm { C _ { i } }$ 相关程度的一种度量，其中， $P (  { T }$ ，$C _ { i } )$ 表示既包含特征 $T$ 又属于类别 $C _ { i }$ 的文本在训练集中出现的概率， $. P ( \\textit { T } )$ 表示训练集中文本包含特征 $T$ 的概率， ${ \\mathcal { P } } ( C _ { i } )$ 表示训练集中文本属于类别 $C _ { i }$ 的概率。则词条 $T$ 与类别 $C _ { i }$ 的 MI计算公式为，",
    "context8": "$$\n\\ M I { \\left( \\begin{array} { l } { T , C _ { i } } \\end{array} \\right) }  \\ = \\ { \\frac { P { \\left( \\begin{array} { l } { T , C _ { i } } \\end{array} \\right) } } { P { \\left( \\begin{array} { l } { T } \\end{array} \\right) } * \\ P { \\left( \\begin{array} { l } { C _ { i } } \\end{array} \\right) } } }\n$$",
    "context9": "在实际计算中， $\\mathrm { M I }$ 公式可以用训练集中相应的文本数近似。词条和类别的互MI体现了词条和类别的相关程度，互信息越大，词条和类别的相关程度也越大，该词条越可能选取为类别 $C _ { i }$ 特征。通过计算,得到各词条的互信息熵大小,设定阈值即可选择一定数量的特征。MI不足是忽略了类别中文本量的多少对词条在每个类别中出现的比率的影响。",
    "context10": "2.1.5 $\\chi ^ { 2 }$ 统计( $\\chi ^ { 2 }$ - statistics,CHI） 考察词项与类别属性的相关程度，该值越大说明词项与类别的相关性越大,携带的类别信息越多。相反，该值越小则此词项与类别越独立,即越不相关。设A为包含词条 $T$ 属于类别 $C _ { i }$ 的文本数， $B$ 为包含词条 $T$ 不属于类别 $C _ { i }$ 的文本数， $C$ 为不包含词条 $T$ 属于类别 $C _ { i }$ 的文本数， $\\mathbf { \\nabla } _ { J } D$ 为不包含词条 $\\mathrm { T }$ 不属于类别 $\\mathrm { C _ { i } }$ 的文本数,N为总文本数。则词条T与类别 $\\mathrm { C _ { i } }$ 的相关度为，",
    "context11": "$$\n\\chi ^ { 2 } ~ = ~ \\frac { N ^ { * } ~ \\left( ~ A D ~ - ~ B C \\right) ~ ^ { 2 } } { \\left( ~ A ~ + ~ C \\right) ^ { * } ~ \\left( ~ B ~ + ~ D \\right) ^ { * } ~ \\left( ~ A ~ + ~ B \\right) ^ { * } ~ \\left( ~ C ~ + ~ D \\right) }\n$$",
    "context12": "$\\chi ^ { 2 }$ 与互信息原理类似,差别在于它是一个归一化的统计量,比其他方法减少大约 $50 \\%$ 的词汇。但该方法统计开销大,对低频特征项区分效果不好。"
  },
  "2.1.6几率比(odds ratio,OR）几率比又称优势比,其计算公式如下，": {
    "context1": "$$\n\\begin{array} { r l } { O R \\left( T \\right) } & { = l o g \\frac { P \\left( \\mathrm { \\textit { T l } } _ { p o s } \\right) \\left( \\mathrm { \\textit { 1 } } - { p } \\left( \\mathrm { \\textit { T l } } _ { n e g } \\right) \\right) } { P \\left( \\mathrm { \\textit { T l } } _ { n e g } \\right) \\left( \\mathrm { \\textit { 1 } } - { P } \\left( \\mathrm { \\textit { T l } } _ { p o s } \\right) \\right) } , } \\end{array}\n$$",
    "context2": "其中 $p o s$ 表示正例文本集合， $\\mathbf { \\nabla } _ { n e g }$ 表示反例文本集合。正例出现条件下，特征词条 $T$ 出现概率越大；反例出现情况下,特征词条 $T$ 出现概率越小。几率比越大，特征词条 $T$ 对正确分类作用越大。几率比不像其他特征选择方法那样对所有类同等对待,而是只关心目标类值，因此这种特征选择方法特别适用于二元分类器。在二元分类中,希望能识别出尽可能多的正类，而不关心识别出负类,这使几率比比其他方法有额外优势。"
  },
  "2.1.7文本证据权(Weight of Evidence for Text,WET)": {
    "context1": "文本证据权法比较类别出现概率和给定特征词条情况下类出现的概率，其计算公式如下，",
    "context2": "$\\begin{array} { r l r } { W E T \\big ( ~ T _ { k } \\big ) \\ } & { { } = } & { P \\big ( ~ T _ { k } \\big ) \\ \\sum P \\big ( ~ T _ { k } \\big ) } \\end{array}$ _log $\\frac { P \\{ \\ C _ { t } \\mid T _ { k } \\} \\mid 1 - P \\{ \\ C _ { t } \\} \\mid } { P \\{ \\ C _ { t } \\} \\mid ( 1 - P \\{ \\ C _ { t } \\mid T _ { k } \\} \\mid ) }$",
    "context3": "如果类别与词条 $T _ { \\mathit { k } }$ 强相关，同时相应类别出现的概率比较小,文本证据权被放大的程度会比期望交叉熵更大，说明特征词条 $T _ { \\mathit { k } }$ 对分类的作用被放大。进行特征选择时,选择WET值大的特征。",
    "context4": "2.1.8其他新方法此外还有基尼指数d、Bayes 准则、类别特征域、SVM、正交质心、低损降维、区分类别能力的高性能算法、绝对比例区分等特征选择算法。"
  },
  "2.2各种特征选择效果比较": {
    "context1": "各种方法都有自己的优缺点，但没有哪种方法在特征选择上占绝对优势，需根据具体应用选择合适的方法。Y.Yang[分析和比较了DF、IG、MI、TS 和 CHI五种方法,发现IG和CHI特征选择方法比其他三种要好。M.Dunjia[通过实证分析得出二元几率是最好的特征选择方法。代六玲等通过实证发现,在英文文本分类中表现良好的IG、MI和CHI在中文文本分类实验效果不是最优的，需要对特征选择算法修正才能适合中文文本分类。周茜等提出类别区分词特征选择法，实验表明多类优势率和类别区分词是两种有效的特征选择方法。徐燕等[构造一个特征选择函数KG,通过试验发现IG和KG性能最好。"
  },
  "2.3算法改进": {
    "context1": "在互信息基础上,王卫玲等提出一种既考虑特征与类别之间的关联性，也考虑特征与特征之间的关联性的特征选择算法。伍建军等提出了一种考虑词频作用的互信息评估函数，提高了文本分类的精度。",
    "context2": "Shankar等研究了基尼指数进行加权特征选择问题;尚文倩[等采用了基尼指数的纯度原理改进了基尼指数算法，构造了7种适合特征提取的选择算法，并对各种算法性能进行了比较;林永民等在基尼指数纯度基础上构造了GinIndTxt算法,该方法比其他方法效果好。"
  },
  "3 特征抽取": "",
  "3.1主要特征抽取方法": {
    "context1": "特征抽取改变原特征的性质，形成新的特征集，目前特征抽取方法主要有主成分分析、潜在语义标引、非负矩阵分解、特征聚类等方法。",
    "context2": "3.1.1主成份分析（Principal Components Analysis,PCA）主成分法解决如何把多个特征化为少数几个综合特征，而此几个综合特征可以反映原来多个特征的大部分信息，新综合特征所包含的信息互不重叠。",
    "context3": "3.1.2潜在语义标引( Latent Semantic Indexing，LSI)",
    "context4": "LSI通过奇异值分解，将文本向量和词向量投影到一个低维空间，使得相互之间有关联的文本即使没有相同的词时也能获得相同的向量表示。LSI可视为特殊的PCA,它将原来变量转化为一组新的变量表示，新变量数目少于原先变量数目。奇异值分解方法对数据变化比较敏感,同时奇异矩阵又需要较大的存储空间，因此面向大规模的文本分类的应用受到一定的限制。",
    "context5": "3.1.3非负矩阵分解(Non-negative Matrix Faetoriza-tion,NMF）NMF将一个非负的矩阵分解为左右两个非负矩阵的乘积,原始矩阵中的列向量可以用左矩阵中所有向量的加权和表示，权重向量即为右矩阵中的列向量。与奇异值分解方法相比,NMF分解能更直观进行语义解释。",
    "context6": "3.1.4特征聚类特征聚类是指将语义高度相关的特征词重新组成或替换成新的特征词。Lewis等首次提出相互最近邻聚类,通过计算词与词之间相似度将词聚类。Li等的层次聚类特征选取法，Baker等的分配聚类方法,都是有效的特征抽取方法。",
    "context7": "3.1.5其他算法其他有 Simhash、Fisher线性判别分析、独立成分分析、流形学习法等特征抽取算法。"
  },
  "3.2算法改进": {
    "context1": "改进算法主要有基于核的KPCA、KFDA等;Lee等人提出其他解决非线性问题的方法。季铎等提出消除潜在语义结构中不存在共现特征信息算法;郝占刚等使用潜在语义索引初次降维,再利用遗传算法实现二次降维。Covoes等提出基于特征相似性的无监督式特征选择方法，改进算法提高分类效果。"
  },
  "4结语": {
    "context1": "$\\bullet$ 特征降维中的特征选择直接选出原始特征集的子集，不会改变原始空间的性质，并且特征代表的意义也未改变,更具有直观的解释作用。特征抽取基于原始特征集，通过变换生成新的组合特征集，而组合特征往往仅具有数学意义。",
    "context2": "·中文与西文的文本分类问题具有相当大的差别,体现在原始特征空间的维数更大，文章表示更加稀疏,词性变化更加灵活等多个方面。西文中表现良好的特征降维方法未必适合中文。",
    "context3": "·机器学习中涉及多种特征降维方法,但由于文本分类的特殊性，需要根据实际文本特点及分类算法，有选择地采取合适方法。每种方法都有各自优缺点，没有哪种方法占绝对优势。"
  },
  "参考文献：": {
    "context1": "[]龙凤姣．图书馆学科化服务与信息素质教育的互动关系研究.图书馆,2009（4):83-84.汤莉华,黄敏．论高校图书馆学科馆员制度的完善——由上海",
    "context2": "交通大学图书馆建立学科馆员制度说开去．大学图书馆学报，2006(1):45-48.  \nB吴文花．试论高校图书馆学科化服务的可持续发展．情报资料工作,2009(4):96-98.  \n李春旺.学科馆员制度范式演变及其挑战.中国图书馆学报，2005(3) :51 -54.  \n张晓林．科研环境对信息服务的挑战．中国信息导报,2003(9):18-22.  \n常唯.e-Science与文献情报服务的变革．图书情报工作,2005，49(3):27 -30.  \n[郭晶,陈进．IC2:一种全新的大学图书馆服务模式．图书情报工作 $, 2 0 0 8 , 5 2 ( 8 ) : 1 1 5 - 1 1 8 .$   \n宛文红，金英玉．第二代学科馆员网络化信息服务的拓展.情报资料工作，2009(3):89-90.",
    "context3": "（作者简介）邬宁芬,女,1968年生,副研究馆员,硕士,发表论文12篇。陈欣,男,1972年生,副研究馆员,副馆长,发表论著近10篇。"
  },
  "（作者简介）奉国和，男,1971年生，副教授,博士，发表论文40余篇。郑伟,男，1978年生,讲师,硕士,发表论文6篇。": "",
  "(上接第96页)": {
    "context1": "人员、“四大服务内容\"（院系联络、学科资源建设、知识信息服务、信息素养教育）。其宗旨是在现有国情的情况下，探索具有中国特色的高校图书馆学科化服务之路,从运行机制、服务模式、服务内容三方面探讨了打破传统的服务组织机制，将高校图书馆分散的资源（文献、人员、硬件)进行整合，并运用创新的服务模式,针对不同学科用户群体的需求,提供深层次、个性化、专业化和知识化的主动信息服务。"
  }
}