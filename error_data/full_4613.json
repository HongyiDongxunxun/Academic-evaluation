{
  "original_filename": "full_4613.md",
  "基于深度学习框架的实体关系抽取研究进展": {
    "context1": "李枫林1,柯佳1,2",
    "context2": "（1.武汉大学 信息管理学院，湖北 武汉430072；2.武汉华夏理工学院，湖北 武汉 430223)",
    "context3": "摘要：【目的/意义】从大量非结构化文本中抽取出结构化的实体及其关系，是优化搜索引擎、建立知识图谱、开发智能问答系统的基础工作【方法/过程】介绍了深度学习框架下不同神经网络模型实现实体关系抽取的方法,比较了各种模型的优劣势,结合远程监督和注意力机制进一步提高关系抽取性能,最后指出了深度学习模型的不足及未来发展方向。【结果/结论】实验发现,卷积神经网络擅长捕获句子局部关键信息，循环神经网络擅长捕获句子的上下文信息，能反映句子多个实体之间的高阶关系，递归神经网络适合短文本的关系抽取。如果模型能结合自然语言的先验知识，实体关系抽取将会取得更好的效果。",
    "context4": "关键词：深度学习;神经网络;实体关系抽取;词向量 中图分类号：G250.7 DOI: 10.13833/j.issn.1007-7634.2018.03.028"
  },
  "Research Progress of Entity Relation Extraction Base on Deep Learning Framework": {
    "context1": "LI Feng-lin1,KEJia1,2",
    "context2": "(1.School of Information Management,Wuhan University，Wuhan 430072,China",
    "context3": "2.Wuhan Huaxia University of Technology，Wuhan 430223,China)",
    "context4": "Abstract: 【Purpose/significance】Extracting structured entities and their relation fromalarge numberof unstructured exts is thebasic work ofoptimizing search engine,building knowledge graph and developing inteligentquestion answering system.【Method/process】This paper introduced the methodsof diferent neural network to implement entityrelation extraction,compared theadvantagesanddisadvantages of various models,further improvedtheperformanceof relation extraction with distantsupervision and atention mechanism,paper finall pointedout the shortcomings of the deep learning and the future development.【Result/conclusion】The experiments show that convolutional neural network is good atcapturing the localkeyinformationofsentences,recurentneuralnetworkisgoodatcapturingthecontextualinformationofsntences,re flectingthehigherorderrelations betweenmultipleentities,andtherecursive neuralnetwork issuitablefortherelation extraction of short texts.If modelcancombinethe prior knowledgeof natural language,theentityrelation extraction will achieve better results.",
    "context5": "Keywords: deep learning;neural network ;entity relation extraction ; word vector"
  },
  "1引言": {
    "context1": "信息抽取的研究内容主要包括三个方面：实体抽取、实体关系抽取、事件抽取。实体关系抽取是核心任务，其问题定义为\"输入一段文本，抽取出每句话中的实体及其之间的语义关系”。",
    "context2": "信息爆炸式增长，使人们淹没在纷繁无序的海量数据中，难以快速有效的搜索到准确的知识，使用计算机从非结构化的文本中抽取结构化信息，即信息抽取技术，一直是自然语言处理的研究热点，受到了学术界和工业界的广泛关注。",
    "context3": "目前的研究主要是指从一句话中抽取出两个实体及语义关系，通常用三元组(实体1,关系,实体2)表示。例如，从句子\"马化腾1998年11月在深圳创立了腾讯公司\"中抽取出实体1\"马化腾”与实体2\"腾讯公司\"并发现两者之间的语义关系,用三元组表示即为(马化腾,创始人,腾讯公司)。"
  },
  "2相关研究": {
    "context1": "在商业需求的推动下，实体关系抽取方法从上世纪九十年代基于知识工程的方法发展到近十年基于统计的机器学习方法[]。",
    "context2": "随着在线文本数量的增加和硬件计算能力的提高,基于统计的机器学习方法应用广泛，主要分为有监督、半监督和无监督三大类方法。由于有监督的方法准确率和召回率(F1值)较高，国内外学者大多采用有监督的方法。",
    "context3": "有监督机器学习方法将实体关系抽取视为一个分类任务,将句子中出现的实体关系划分到预先定义好的类别中。主要包括两大类方法：基于特征向量的学习方法(fea-ture-based)[2-3]和基于核函数的学习方法(kernel-based)[4]。",
    "context4": "基于统计的机器学习方法通常使用支持向量机[2、最大熵模型完成关系抽取任务，通过大量人工标注数据和引入外部信息提高抽取的性能[2-4]。深度学习方法提出之前最好公开评测成绩是Rink等人使用支持向量机结合大量外部语言特征取得 $8 2 . 2 \\%$ 的F1值。",
    "context5": "基于统计的机器学习方法首先需要大量人工标注的语料库,然后在语料库基础上进行特征抽取和选择,通过利用不同的机器学习算法训练分类模型，自动抽取、发现新的实体对及关系。这类方法很大程度上依赖于自然语言处理工具(如词性标注、句法分析等),这些步骤会带来如下问题： $\\textcircled{1}$ 自然语言处理工具存在不同程度的错误,使用多个工具会造成错误累积，影响最终的分类性能； $\\textcircled{2}$ 特征选择依靠专家知识和经验，需要花费大量时间设计和验证特征; $\\textcircled{3}$ 有些使用人数较少的语言没有完善的自然语言处理工具，此时这些方法不再适用。",
    "context6": "近年来，深度学习[5框架的神经网络模型成为实体关系抽取的新方法,深度学习是一种特殊的机器学习方法,是机器学习的一个分支,不同之处在于：神经网络模型自动学习句子特征，无需复杂的特征工程。",
    "context7": "基于深度学习框架的实体关系抽取方法和过去的方法相比主要有两个优势：",
    "context8": "$\\textcircled{1}$ 使用统一的低维、连续、实数词向量,表示不同粒度的语言单元,如词、短语、句子和篇章等；",
    "context9": "$\\textcircled{2}$ 使用递归、卷积、循环等神经网络模型对不同语言单元的特征向量进行组合，获得更大语言单元的向量表示。",
    "context10": "目前实体关系抽取任务最常使用的模型包括递归神经网络[6](RecNN,Recursive Neural Network)、卷积神经网络[7]（CNN，ConvolutionalNeural Network）、循环神经网络[8]（RNN,Recurrent Neural Network）和长短时记忆网络[9](LSTM,Long Short-Term Memory Network）。使用深度学习框架的神经网络模型,特征抽取和选择是自动完成的，在不使用自然语言处理工具的情况下,在多个自然语言处理任务上超过了过去基于统计的机器学习方法。"
  },
  "3神经网络模型": "",
  "3.1 模型的输入—词向量": {
    "context1": "词向量(Word vector)[10-\"]是一种词的分布式表示,这一表示方式是基于1954年Harris提出的分布式假设[2](distri-butionalhypothesis):出现在相同上下文的单词,其语义也相似。比如句子：“我今天坐火车去了北京\"和句子“我今天坐火车去了广州”,北京和上海前面的单词一样,可以判断\"北京\"与\"上海\"语义相似。词向量将每个单词表示成连续、低维、实数向量(通常50-100维),通过神经网络对句子上下文，以及上下文与目标词之间的关系建模,词向量空间距离表示词汇语义相似度，向量距离越近,代表语义越相似。与其他的单词分布式表示方法(one-hot)相比，词向量维度更低,包含更丰富的语义信息。目前采用深度学习框架进行自然语言处理，大多使用2013年Mikolov等人训练出来的词向量Word2vec[]作为神经网络模型的输入。",
    "context2": "来斯惟[3]从模型选择、语料选择和训练参数三个方面对比不同的词向量训练方法。实验发现语料规模越大,词向量的语义表示能力越强;根据不同的文本处理任务，选择不同的词向量和神经网络模型。Fu等人[14]在2014年ACL会议上提出利用词向量从文本中发现词汇的上下位关系。Yu等人[结合词向量与人工构建的特征，使用混合模型在SemEval-2010Task8的关系分类任务上取得了 $8 3 . 4 \\%$ 的F1值。Hashimoto等人[提出以实体对之间的关系词为预测目标,句子中其他词作为上下文信息构建模型,学习词向量表示，用于关系分类任务。"
  },
  "3.2递归神经网络模型(Recursive Neural Network, RecNN)": {
    "context1": "递归神经网络模型的特点在于首先需要对每句话进行句法分析，将句子顺序结构转化为树状结构，然后利用该结构构建网络模型。",
    "context2": "Socher等人[]首次提出使用矩阵-递归神经网络模型MV-RNN(matrix-vector Recursive Neural Network)完成关系抽取任务。模型首先对每个句子进行句法分析，句法树每个节点用向量表示,从句法树最底端的词向量开始,按照句子的句法结构迭代合并(组合矩阵),最终得到该句子的向量表示，实现关系分类。模型引入语义矩阵(Matrix)刻画句子副词对形容词的修饰关系,以及谓语动词真假逻辑(是与否)。模型的缺点在于每个节点都有一个词向量和一个矩阵,需要训练太多的参数,并且没有考虑两个实体在句子中的位置信息。",
    "context3": "递归神经网络模型优点在于可以分析句子的句法结构(通常句子的谓语能反映主语和宾语的语义关系),缺点在于模型受限于句法分析的准确性（一个句子可能有多个句法树),一旦句子相对复杂,句法分析出现错误,会产生错误累",
    "context4": "积,影响模型最终分类结果。"
  },
  "3.3卷积神经网络模型(Convolutional Neural Network,CNN)": {
    "context1": "卷积神经网络最早应用在图像识别领域[8], $\\operatorname { K i m }$ 首次提出将卷积神经网络应用到文本分类任务[9]，CNN模型特点在于能保留句子的局部关键信息，在多个文本分类(文本主题分类[19-20],情感分类[2]、垃圾邮件分类[22)任务上都有不错的表现。",
    "context2": "卷积神经网络模型首先从左到右用一个滑动窗口对句子进行扫描,矩阵的每一行对应句子的一个单词(或者一个字符)。在滑动窗口内,通过卷积层(convolution)抽取特征,再通过最大池化层(max pooling)选择特征。重复以上操作多次，得到多个向量表示，最后将这些向量连接起来得到整个句子的语义表示。句子的上下文依赖关系被简化为一种“局部\"特征,类似于传统的N-gram 语言模型。",
    "context3": "Liu等人[23最早提出在关系分类任务中使用卷积神经网络自动学习句子特征,运用随机产生的词向量,结合同义词词典编码,在模型中加入了词汇特征、词性特征,在ACE2005数据集上F1值超过了当时核函数方法9个百分点。",
    "context4": "Zeng 等人[24首次提出将目标实体(词)与句子其他词的相对位置信息输入到神经网络模型中，在没有使用NLP处理工具（POS、NER、syntactic analysis）的情况下，在SemEval-2010 Task8关系分类任务上取得了当年(2014年)最优的实验效果,后来的学者也多次借鉴了此方法。",
    "context5": "Nguyen等人[25]引入多尺寸滤波器,使用预训练好的词向量,将单词之间的相对位置信息输入卷积神经网络,通过模型优化词向量和位置参数，捕获浅层语义和句法信息。",
    "context6": "Santos 等人[2]提出一种新的分段排序(pairwise ranking)损失函数代替输出层softmax分类器的损失函数，并改进了other类分类器。",
    "context7": "$\\mathrm { X u }$ 等人[27]利用卷积神经网络模型表示句子最短依存路径,并考虑实体关系方向性,使用负采样策略(Negative Sam-pling)提升实体边界判别能力。",
    "context8": "Lee等人[2将词向量、词的相对位置(正向\\逆向）、词的属性(实体类型、词性标注)等特征输入到CNN模型中，将关系预先划分为三类：同义词、上下位关系，不存在关系。实验表明，引入单词逆向位置信息能提升分类性能。论文在科学论文集分类任务(SemEval-2017Task10)上取得了不错的效果。",
    "context9": "刘胜宇[29]对比两种不同结构的模型： $\\textcircled{1}$ 基于文本序列的卷积神经网络。 $\\textcircled{2}$ 基于依存结构的卷积神经网络,在药物相互作用的关系分类任务上的性能(预先定义两种药物相互作用关系分为四大类)。在国际公开的药物相互作用分类评测任务DDIExtraction2013(Drug-Drug Interaction Extrac-tion2013)上实验表明：第一个模型训练相对简单,需要训练的参数相对较少,训练时间较短,适用于长句子的药物关系分类,第二个模型训练相对复杂，需要训练的参数较多，训练时间较长,适用于短句子的药物关系分类。",
    "context10": "彤博辉等人[30]提出使用两种不同的词向量(Word2vec和Glove[\")输入到卷积神经网络不同通道,实验表明，多通道词向量比使用单一Word2vec词向量更适合关系抽取任务。",
    "context11": "王林玉等人[3提出卷积神经网络和关键词策略的实体关系抽取方法。在Word2vec词向量的基础上，通过句子级(sentence level)的关键词抽取(TP-ISP)算法捕获关系类别关键词特征。类别关键词的输入能提高关系分类区分度，弥补模型自动学习、抽取特征的不足。",
    "context12": "冯钦林[32研究生物医学领域实体(疾病与病症、疾病与治疗物质)之间的关系，利用句法树的最短依存路径信息表示两个实体间的关系，模型考虑不同特征的差异，对句子特征和单词语义特征赋予不同的权重，突出重要特征表示实体的关系。",
    "context13": "前面提到的实体关系抽取方法大多只涉及包含两个实体的句子,忽略了只包含一个实体的句子。Zeng[3针对这一问题,在两个目标实体之间构建了一个用于推理的中间实体,在神经网络的基础上引人关系路径编码器(path encod-er),实验表明考虑关系路径能取得更好的分类效果。",
    "context14": "尽管卷积神经网络在关系抽取任务上表现不错,但由于卷积核通常不会选择太大(小于5个),无法对长句子建模，解决两个实体的远距离依赖关系。为了解决这一问题,学者提出使用循环神经网络对句子建模。"
  },
  "3.4循环神经网络模型(Recurrent Neural Network, RNN)": "",
  "3.4.1循环神经网络模型": {
    "context1": "循环神经网络模型将一个句子看作一个单词序列,每个单词由一个向量表示,句子每一个位置上有一个中间向量，表示句首到这个位置的语义信息。模型可以将可变长度句子编码成固定长度，考虑实体对的上下文依赖关系，和CNN相比,模型能够保留句子全局特征。",
    "context2": "Zhang等人[34采用基于词位置信息的循环神经网络完成关系抽取任务，更好的利用了实体的上下文信息。",
    "context3": "3.4.2 长短时记忆网络(Long Short Term Memory Net-work,LSTM)",
    "context4": "循环神经网络模型在连续时间内不断叠加输入单词，导致句子中靠前的单词对当前词的影响变得微弱,但这两个词汇之间可能存在依赖关系，为了更有效的捕获长句子远距离单词之间的语义信息,Sundermeyer等人[3提出了RNN改进模型----长短时记忆网络模型(LSTM,Long short-termmemory network),通过构建专门的记忆单元存储历史信息，使得每个时间状态都保存了前面的输入信息，句子靠前的单词序列影响后面的输出，有效解决两个实体之间长距离依赖问题。",
    "context5": "$\\mathrm { X u }$ 等人[3使用长短时记忆网络模型对实体之间的最短依存路径信息进行学习,并利用了词向量、词性标注、句法依存、上位词信息,在关系分类任务上取得了 $8 3 . 7 \\%$ 的F1值。",
    "context6": "刘桑[37]提出 使用 SDP-LSTM（short dependence pathsLSTM)模型实现开放域(open domain)实体关系抽取,模型分别处理句子可能出现的多个依存路径。",
    "context7": "3.4.3 双向长短时记忆网络(BidirectionalLSTM)",
    "context8": "双向长短时记忆网络的思想是某时刻t的输出不仅依赖于文本序列中某个单词前面的单词,也依赖于后面的单词。双向LSTM能够捕获每个单词的上下文信息。",
    "context9": "Zhang[38提出使用双向长短时记忆网络建模整个句子（Bidirectional long short-term memory networks ）用于关系分类。",
    "context10": "胡新辰[39]在模型输入层额外添加了 $\\textcircled{1}$ 相对位置特征 $\\textcircled{2}$ 词性标注特征 $\\textcircled{3}$ 实体标注特征 $\\textcircled{4}$ 上位词特征 $\\textcircled{5}$ 依赖特征(包括依赖关系特征和相对依赖特征)五类组合特征作为BiL-STM模型的输人,使用相对最大池化(relative max pooling)获取句子级别特征，有效提高了关系分类性能。",
    "context11": "Li等人[40]对比序列模型（双向LSTM)和树模型(Tree-LSTM)在实体关系分类任务上的结果发现，序列模型分类效果更差的原因是：序列模型两个实体中间的干扰词影响了长句子实体关系抽取的结果。",
    "context12": "孙紫阳等人[4采用双向长短时记忆网络建模句子最短依存路径，同时模型加入词性特征，将LSTM模型的输出作为CNN输入，训练模型。此方法充分利用了双向LSTM模型擅长捕获长句子实体之间的依赖关系和CNN模型擅长捕获句子局部特征的优点。"
  },
  "3.5多个神经网络模型结合": {
    "context1": "Liu 等人[42]使用卷积神经网络建模句子最短依存路径,使用递归神经网络建模句法依存树子树的语义信息,这一混合模型充分利用RecNN擅长处理句子层级结构(hierarchi-cal structure),CNN擅长处理句子扁平结构(flat structure）。",
    "context2": "Cai等人[43]结合循环神经网络和卷积神经网络的优点，提出了最短依存路径(Shortest Dependency Path)的关系分类模型。为了提高模型对实体关系方向的判别能力,引入了双向循环卷积神经网络(BRCNN)同时从正反两个方向学习最短依存路径上的信息。"
  },
  "4神经网络模型结合远程监督方法": "",
  "4.1 远程监督方法的提出": {
    "context1": "2009 年斯坦福大学教授 $\\mathrm { M i n t z } ^ { [ 4 4 ] }$ 等人在ACL会议上提出远程监督(Distant Supervision)的实体关系抽取方法。这种方法无需人工标注，作者将纽约时报(NYT)英文新闻文本与大规模半结构化的知识库(Freebase)实体对齐，使用知识库中已有的三元组启发式地训练一个实体关系分类器。",
    "context2": "远程监督方法的假设是：如果两个实体在知识库中存在某种关系,则待抽取的非结构化文本中包含这两个实体的句子也反映这种关系。例如,\"Steve Jobs\",\"Apple\"在知识库",
    "context3": "Freebase中存在founder的关系,则包含这两个实体的句子“Steve Jobs ate an apple after busy work tonight.”也存在 found-er关系。",
    "context4": "远程监督方法虽然从一定程度上减少了人工标注数据的工作,但此方法很明显存在两个缺点：",
    "context5": "$\\textcircled{1}$ 由于Mintz假设过于宽松,难免引入大量的噪声（错误标注）。“Steve Jobs ate an apple after busy work tonight.\"这句话中并没有表示出 Steven Jobs与 Apple 之间存在 founder 的关系。",
    "context6": "$\\textcircled{2}$ 在数据处理的过程(命名实体识别、词性标注)中，很大程度上依赖自然语言处理工具，在完成整个任务的多个步骤中会产生错误累积,影响实体关系抽取性能。",
    "context7": "Riedel等人[45]认为Mintz 的假设过于严格,将Mintz 的假设放松为：如果两个实体之间存在某种关系,那么所有包含这两个实体的句子至少有一个句子(AtLeastOne)描述了该关系。为了解决噪声问题,作者提出使用多实例(Multi-instance)学习算法进行关系抽取。"
  },
  "4.2卷积神经网络结合远程监督的方法": {
    "context1": "Zeng[46]受Riedel45]启发,提出卷积神经网络结合远程监督的多实例(multi-instance)方法,选取置信度最高的训练样例训练模型，降低噪声，实验在大规模数据集NYT10(纽约时报，NewYorkTimes)上取得了远远高于传统基于特征的关系抽取方法。",
    "context2": "邵发[47]利用Word2vec词向量,在旅游领域提出卷积神经网络模型结合远程监督的实体关系抽取方法。模型自动学习词汇特征、上下文特征及句子特征,建立了一个云南旅游知识图谱。",
    "context3": "刘凯等人[48]在医疗领域提出卷积神经网络结合远程监督关系抽取方法。首先人工标注少量实体对关系,然后将该关系转换为特征向量矩阵输入到卷积神经网络进行分类训练。这一方法减少了训练语料的噪声，提升了分类性能。",
    "context4": "由于 $\\mathrm { Z e n g ^ { [ 4 6 ] } }$ 选取最大置信度的假设与Mintz[44]的假设本质一样,丢失了大量反映句子实体关系的有用负例信息。于是有学者提出使用注意力机制,充分利用语料中反映实体关系的\"错误\"样例,对不同置信度的句子赋予不同的权重。",
    "context5": "远程监督方法的局限性在于关系抽取任务依赖于一个已存在的半结构化知识库，分类类别候选项存在于知识库中，无法识别知识库中不存在的新的实体关系。"
  },
  "4.3模型引入注意力机制": {
    "context1": "注意力机制[49]（Attention）是Treisman 和Gelade 提出的一种模拟人脑的模型,通过计算概率分布，突出关键输入信息对模型输出的影响以优化模型。Bahdanau等人[5o首次将注意力机制应用在自然语言处理任务中，提升机器翻译的准确性。",
    "context2": "Lin等人[引入注意力机制，卷积神经网络结合远程监督的关系抽取模型,充分利用语料中所有包含两个实体对的句子，在句子中引入注意力机制,通过赋予实体关系标注正确的样例更高的权重,减少噪声，提高分类准确率。此方法不同与 $\\mathrm { Z e n g ^ { [ 4 6 ] } }$ 只在概率最大的关系语句上进行训练,本文充分利用语料中包含两个实体的所有句子。",
    "context3": "表1不同模型在经典关系抽取任务集SemEval-2010Task8上取得的结果(词向量单独输入神经网络模型和加入额外信息后的对比值)",
    "context4": "<table><tr><td rowspan=\"2\">Classifier Model</td><td rowspan=\"2\">Word Embedding</td><td rowspan=\"2\">Additional Information</td><td rowspan=\"2\">F1值(%)</td></tr><tr><td></td></tr><tr><td>SVM (Rink and Harabagiu,2010)[2]</td><td>No</td><td>POS,WordNet,Prefixes and other morphological features,dependency parse,PropBank,FanmeNet,</td><td>82.2</td></tr><tr><td>(Best in SemEval2010)</td><td></td><td>Google n-gram,paraphrases,TextRunner Word embeddings</td><td>82.8</td></tr><tr><td>RelEmb (Hashimoto et al.,2015)[16]</td><td>Yes</td><td>+ Dependency paths,WordNet,NER</td><td>83.5</td></tr><tr><td>MVRNN</td><td></td><td>Word embeddings</td><td>79.1</td></tr><tr><td>(Socher et al.,2012)[7]</td><td>Yes</td><td>+POS,NER,WordNet</td><td>82.4</td></tr><tr><td>CNN</td><td></td><td>Word embeddings</td><td>69.7</td></tr><tr><td>(Zeng et al.,2014)[24]</td><td>Yes</td><td>+Word position embeddings,WordNet</td><td>82.7</td></tr><tr><td>多通道CNN (彤博辉et al.,2017)[30]</td><td>Yes</td><td>Word2Vec,GloVe</td><td>76.8</td></tr><tr><td>CNN (Nguyen et al.,2015)[25]</td><td>Yes</td><td>Word embeddings,n-gram,Word position embeddings</td><td>82.8</td></tr><tr><td>FCM</td><td>Yes</td><td>Word embeddings+Dependencyparsing,WordNet</td><td>83.1</td></tr><tr><td>(Yu et al.,2015)[1s]</td><td></td><td>Word embeddings+Dependency parsing, NER</td><td>83.4</td></tr><tr><td>CR-CNN</td><td>Yes</td><td>Word embeddings</td><td>82.8</td></tr><tr><td>(Dos Santos et al.,2015)[26]</td><td></td><td>+Word position embeddings</td><td>84.1</td></tr><tr><td>SDP-LSTM</td><td>Yes</td><td>Word embeddings</td><td>82.4</td></tr><tr><td>(Xu et al.,2015)[36]</td><td></td><td>+POS+ Grammatical relations+ WordNet embeddings</td><td>83.7</td></tr><tr><td>DepNN</td><td>Yes</td><td>Word embeddings,WordNet</td><td>83.0</td></tr><tr><td>(Liu et al.,2015)[42]</td><td></td><td>Word embeddings,NER</td><td>83.6</td></tr><tr><td>DepLCNN</td><td>Yes</td><td>Word embeddings,WordNet, word around nominals</td><td>83.7</td></tr><tr><td>(Xu et al.,2015)[27]</td><td></td><td>+negative sampling from NYT dataset</td><td>85.6</td></tr><tr><td>CNN+关键词</td><td>Yes</td><td>Word embeddings,Word position embeddings</td><td>81.3</td></tr><tr><td>(王林玉et al.,2017)[31]</td><td></td><td>+关键词</td><td>86.2</td></tr><tr><td>BRCNN</td><td>Yes</td><td>Word embeddings</td><td>85.4</td></tr><tr><td>(Cai et al.,2016)[43]</td><td></td><td>+POS+NER+ WordNet embeddings</td><td>86.3</td></tr><tr><td>Att-LSTM</td><td>Yes</td><td>Word embeddings+Dependency parsing</td><td>87.16</td></tr><tr><td>(王红 et al.,2018)[52]</td><td></td><td>+POS+NER+ WordNet embeddings</td><td></td></tr><tr><td>Multi-Level Attention CNN</td><td>Yes</td><td>Att-Input-CNN</td><td>87.5</td></tr><tr><td>(Wang et al.,2016)[54]</td><td></td><td>Att-Pooling-CNN</td><td>88.0</td></tr></table>",
    "context5": "王红等人[2为了解决两个实体长距离依赖,提出LSTM模型与注意力机制相结合的关系抽取方法。首先文本向量化输入到双向LSTM模型,通过注意力机制对LSTM模型的输入与输出之间的相关性进行权重计算，根据重要性获取文本整体特征;最后融合局部特征和整体特征,输出分类结果。",
    "context6": "李博等人[53为了解决长文本两个实体距离较远容易产生错误的问题,在经典CNN模型中引入了注意力机制,提出最短依存路径选择性注意的CNN编码器，并依据实体关系具有方向性的特点，设计了一种正向实例和反向实例结合的分类方法。",
    "context7": "在关系抽取任务中,学者利用注意力机制(attention)优化模型,赋予句子重要特征更高的权重,减少噪声,取得了比以往方法更好的分类效果(参见表1)。"
  },
  "5 端到端的关系抽取(end-to-end)": {
    "context1": "实体关系抽取作为信息抽取的核心工作，目前根据任务完成的步骤主要分为两类：一类是串联抽取方法(又叫做流水线式抽取,pipeline),另一类是联合抽取(joint)方法（又叫做端到端抽取,end to end)。传统的串联抽取方法首先抽取实体，然后分类关系。这种分开的方法相对容易实现，各模块处理灵活度较高。但是,此方法实体抽取(识别)结果会影响到下一步关系抽取,容易产生错误累积。联合抽取方法将实体抽取和关系抽取整合到一个模型中，降低中间各个步骤产生的错误。",
    "context2": "Miwa[5提出一种端到端的神经网络关系抽取模型。模 型使用双向LSTM和树形LSTM(tree-LSTM)对实体和句子 建模,同时考虑了单词的词序和依存句法树。模型中有两个 双向的LSTM-RNN,一个是bidirectional sequential LSTM-R NN,用于实体抽取；一个 bidirectional tree- structured",
    "context3": "LSTM-RNN,用于关系抽取;前者的输出和隐含层作为后者输入的一部分,共享参数,实体抽取和关系抽取相互影响,使得关系抽取的过程可以利用实体信息。作者在模型训练过程中使用了预训练(entity-pretrain)实体和抽样(scheduledsampling)等方法进一步提升关系抽取性能。",
    "context4": "Zheng等人[5将联合抽取问题转化为序列标注问题，提出一种新型标注(特殊标签)方法完成联合抽取任务。端到端模型包含一个对输入语句编码的双向长短时记忆网络和一个具有偏置损失函数的长短时记忆网络,实验在公开数据集NYT(纽约时报)上取得了不错的分类效果。",
    "context5": "Wang等人[54在端到端关系抽取任务中引入多层级注意力机制（Multi-Level Attention）,对句子中反映实体关系的词汇赋予更大的权重。作者运用了两个注意力机制，一个应用在输入层，对句子两个实体关系添加注意力，另一个应用在混合层,改进目标函数,针对目标类别添加注意力。"
  },
  "6总结与展望": {
    "context1": "实体关系抽取作为信息抽取的核心工作，对完善知识库、优化搜索引擎、开发智能问答系统都具有重要的商业价值。机器学习希望计算机更智能、快速的处理数据,而深度学习框架的神经网络模型最大特点在于单词的特征表示和特征的自动学习,模型能保留文本的所有特征,数据量越大，模型泛化能力越强。使用神经网络模型实现关系抽取最关键的是根据任务选择合适的词向量，再利用RecNN/CNN/RNN/LSTM等不同的网络模型自动提取特征，槟弃复杂的人工特征工程。",
    "context2": "本文总结了近几年不同的神经网络模型实现实体关系抽取的方法,模型不需要NLP工具，各有优劣势。卷积神经网络模型通过卷积操作捕获句子局部信息，最大池化捕获句子最重要的关键特征。循环神经网络模型及长短时记忆网络模型捕获句子的上下文特征、能反映句子多个实体之间的高阶关系。递归神经网络模型依赖于句法分析的准确性，训练时间相对较长,复杂度较高,不太适合复杂长句子关系抽取,比较适合短文本抽取。",
    "context3": "为了弥补各种模型的劣势,学者提出了在模型中加入词汇特征(相对位置特征、词性、语言特征),引入注意力机制，多实例远程监督方法，提升关系抽取性能。",
    "context4": "深度学习方法已经在关系抽取任务上超越了基于统计的机器学习方法，学者逐渐开始从流水线式(pipeline,先实体识别再关系抽取)方法过渡到端到端(end to end)方法。“端到端\"思想，抛弃了过去自然语言处理过程中的\"分词 $\\longrightarrow$ 词法$\\longrightarrow$ 句法 $\\longrightarrow$ 语义”一系列特征构造、选择和标注的繁琐工作。",
    "context5": "虽然神经网络模型成为热点方法，但是学者对于关系抽取的研究仍然面临许多挑战，有些具体问题有待进一步思考。",
    "context6": "$\\textcircled{1}$ 神经网络模型类似于一个黑匣子,虽然避免了人工标注的特征工程(feature engineering),但增加了大量调整和优化参数、选择模型层数、梯度下降、激活函数等工作,如何解释参数大小的选择，为什么要改进模型的结构，理论依据不足,可解释性不强。",
    "context7": "$\\textcircled{2}$ 语言不同于图像和语音，具有多样性、歧义性，语境不同,语言传达的意思也不同，如何根据语境提高模型的泛化能力？",
    "context8": "$\\textcircled{3}$ 随着模型层数的加深,参数增加，训练的时间变长，如何设计、改进模型更好的进行特征抽取和特征选择,降低训练模型的时间复杂度,提升抽取效果。",
    "context9": "$\\textcircled{4}$ 目前学者的研究大多集中在改进模型结构，优化参数上,使用的数据集主要是 SemEval-2010 Task 8、ACE2005、ACE2008,这几个任务集缺点是数据量不足够大，模型容易出现过拟合。所以目前的研究方法仍处于探索阶段。",
    "context10": "$\\textcircled{5}$ 自然语言包括大量知识,包括语言学知识(如不同语言的语法结构）、词汇知识(如英语语义词典WordNet)和世界百科知识(如维基百科Wikipedia)。目前,深度学习方法尚未有效利用这些知识。如何将知识工程的符号表示和深度学习的向量表示结合、如何将单词特征表示与语言的先验知识、语言规则相结合,仍然是NLP领域一个有待深入研究的问题。",
    "context11": "$\\textcircled{6}$ 目前的实验和研究主要针对英文，无论是语言处理工具还是语料，中文几乎没有，未来国内学者应该尝试如何设计中文实验方案。",
    "context12": "$\\textcircled{7}$ 实体关系有可能随时间发生变化,如何更新已建立的知识库，在开放领域取得更好的抽取效果是下一步需要研究的问题。",
    "context13": "相信在学术界和工业界的不断探索和尝试下，关系抽取任务在未来几年还会取得更大的发展和突破。"
  },
  "参考文献": {
    "context1": "1 车万翔,刘挺,李生.实体关系自动抽取[J].中文信 息学报,2005,19(2):2-7.   \n2Rink B,Harabagiu S. Utd: Classifying semantic relations by combining lexical and semantic resources [C]//Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics,201O:256-259.   \n3Kambhatla N.Combining lexical,syntactic,and semantic features with maximum entropy models for extracting relations[C]//Proceedings of the ACL 2004 on Interactive poster and demonstration sessions.Association for Computational Linguistics,2oo4:22.   \n4庄成龙,钱龙华，周国栋.基于树核函数的实体语义关系 抽取方法研究[J].中文信息学报,2009,23(1):3.   \n5Hinton G E,Osindero S,Teh Y W.A fast learning algorithm for deep belief nets[J]. Neural computation, 2006,18(7):1527-1554.   \n6Goller C,Kuchler A.Learning task-dependent distributed representations by backpropagation through structure[C]//Neural Networks,IEEE International Conference on.IEEE,1996:347-352.   \n7 LeCun Y, Boser B, Denker J S, et al. Backpropagation applied to handwritten zip code recognition[J]. Neural computation,1989,1(4): 541-551.   \n8 Elman JL.Distributed representations,simple recurrent networks,and grammatical structure[J]. Machine learning,1991,7(2-3): 195-225.   \n9 Hochreiter S,Schmidhuber J. Long short-term memory[J]. Neural computation,1997,9(8): 1735-1780.   \n10Mikolov T,Chen K,Corrado G,et al. Efficient Estimation of Word Representations in Vector Space[EB/OL]. https:// arxiv.org/pdf/1301.3781.pdf,2013-09-07.   \n11 Pennington J, Socher R,Manning C. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),2014: 1532-1543.   \n12Harrs Z S. Distributional structure[J].Word,1954,10(2-3) 146-162.   \n13 来斯惟.基于神经网络的词和文档语义向量表示方法研 究[D].北京:中国科学院大学,2016.   \n14Fu R,Guo J, Qin B,et al. Learning semantic hierarchies via word embeddings[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2014: 1199-1209.   \n15 Gormley MR, Yu M,Dredze M, et al. Improved Relation Extraction with Feature-Rich Compositional Embedding Models[C]// Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,，2015: 1774-1784.   \n16 Hashimoto K, Stenetorp P,Miwa M,et al. Task-Oriented Learning of Word Embeddings for Semantic Relation Classification[C]//Proceedingsof the 19th Conferenceon Computational Language Learning, 2015: 268-278.   \n17 Socher R, Huval B, Manning C D,et al. Semantic compositionality through recursive matrix-vector spaces[C]//Proceedings of the 2O12 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics,2012: 1201-1211.   \n18 Fukushima K.Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position- Neocognitron[J].Electron.& Commun. Japan,1979,62(10): 11-18.   \n19 Kim Y. Convolutional Neural Networks for Sentence Classification[C]//Proceedings of the 2O14 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014: 1746-1751   \n20 Kalchbrenner N, Grefenstette E,Blunsom P,et al.A Convolutional Neural Network for Modeling Sentences[C]// Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014: 655-665.   \n21 Poria S,Cambria E,Gelbukh A.Aspect extraction for opinion mining with a deep convolutional neural network[]. Knowledge-Based Systems,2016,(108): 42-49.   \n22 沈承恩,何军,邓扬.基于改进堆叠自动编码机的垃 圾邮件分类[].计算机应用,2016,36(1):158-162.   \n23Liu C Y, Sun WB, Chao W H,et al. Convolution neural network for relation extraction[C]//International Conference on Advanced Data Mining and Applications. Springer, Berlin,Heidelberg,2013: 231-242.   \n24 Zeng D,Liu K,Lai S,et al.Relation Classification via Convolutional DeepNeural Network[C]//COLING,2014: 2335-2344.   \n25Nguyen T H, Grishman R. Relation Extraction: Perspectivefrom Convolutional Neural Networks[C]//VS $@$ HLT-NAACL,2015: 39-48.   \n26Santos C N, Xiang B, Zhou B,et al. Classifying Relations by Ranking with Convolutional Neural Networks[C]// Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,2015: 626-634.   \n27 Xu K, Feng Y,Huang S,et al. Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling[C]//Proceedings of the 2O15 Conference on Empirical Methods in Natural Language Processng, 2015: 536-540.   \n28 Ji Y L,Dernoncourt F, Szolovits P.MIT at SemEval-2017 Task 1O:Relation Extraction with Convolutional Neural Networks[EB/OL].https://arxiv.0rg/pdf/1704.01523.pdf, 2017-04-05.   \n29 刘胜宇.生物医学文本中药物信息抽取方法研究[D].哈 尔滨:哈尔滨工业大学,2016.   \n30 彤博辉,付琨,黄宇,等.基于多通道卷积神经网的实 体关系抽取[].计算机应用研究,2017,(3):689-692.   \n31 王林玉,王莉,郑婷一.基于卷积神经网络和关键词策 略的实体关系抽取方法.模式识别与人工智能,2017, 30(5):465-472.   \n32 冯钦林.基于半监督和深度学习的生物实体关系抽取[D]. 大连:大连理工大学,2016.   \n33 Zeng W,Lin Y,Liu Z,et al.Incorporating Relation Paths in Neural Relation Extraction[EB/OL]. htps://arxiv.org/ pdf/1609.07479.pdf,2017-12-13.   \n34 Zhang D,Wang D. Relation Clasification via Recurrent Neural Network[EB/OL].https://arxiv.org/pdf/1508.0100   \n35Sundermeyer M, Schliter R,Ney H. LSTM neural networks for language modeling[C]//Thirteenth Annual Conference of the International Speech Communication Association,2012.   \n36 Xu Y, Mou L,Li G, et al. Clasifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths[C]//EMNLP,2015:1785-1794.   \n37 刘桑.面向《大词林》的中文实体关系挖掘[D].哈尔滨: 哈尔滨工业大学,2016.   \n38Zhang S,Zheng D,Hu X,et al. Bidirectional Long Short-Term Memory Networks for Relation Classification [C]//PACLIC,2015.   \n39 胡新辰.基于LSTM的语义关系分类研究[D].哈尔滨:哈 尔滨工业大学,2015.   \n40 Li J,Luong T, Jurafsky D,et al. When Are Tree Structures Necessary for Deep Learning of Representations[C]//Proceedings of the 2O15 Conference on Empirical Methods in Natural Language Processing, 2015: 2304-2314.   \n41 孙紫阳,顾君忠,杨静.基于深度学习的中文实体关系抽 取方 法 [EB/OL]. htp://www.ecice06.com/CN/abstract/ abstract28113.shtml,2017-10-17.   \n42 Liu Y,Wei F,Li S,et al.A Dependency-Based Neural Network for Relation Classfication[C]// Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,2015: 285-290.   \n43 Cai R, Zhang X, Wang H. Bidirectional Recurrent Convolutional Neural Network for Relation Clasification[C]// ACL,2016.   \n44Mintz M,Bills S, Snow R,et al.Distant supervision for relation extraction without labeled data[C]//Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2. Association for Computational Linguistics,20o9:1003-1011.   \n45Riedel,Sebastian,Limin Yao,and Andrew McCallum. Modeling relations and their mentions without labeled text [C]. In ECML/PKDD,2010.   \n46 Zeng D,Liu K, Chen Y,et al. Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks[C]//Emnlp,2015:1753-1762.   \n47 邵发.基于词向量和深度卷积神经网络的领域实体关 系抽取[D].昆明:昆明理工大学,2016.   \n48 刘凯,符海东,邹玉薇,等.基于卷积神经网络的中文医 疗弱监督关系抽取.计算机科学,2017,44(10): 249-253.   \n49Treisman A,Sykes M,Gelade G. Selective attention and stimulus integration[J].Attention and performance VI,1977, (333):97-110.   \n50Bahdanau,D.,Cho,K.& Bengio,Y.Neural Machine Translation by Jointly Learning to Align and Translate[C]. Iclr,2015.   \n51Lin Y,Shen S,Liu Z,et al. Neural Relation Extraction with Selective Attention over Instances[C]//ACL,2016.   \n52 王红,史金钊,张志伟.基于注意力机制的LSTM的语 义关系抽取[EB/OL]. http://www.arocmag.com/article/ 02-2018-05-071.html,2018-05-07.   \n53 李博,赵翔,王帅,等.改进的卷积神经网络关系 分类方法研究[EB/OL].http://www.wanfangdata.com.cn/ details/detail.do?_type perio&id=pre_b926088c-dd8a-490 5-b2c6-74a352d0d2e0,2017-05-15.   \n54 Wang L,Cao Z,de Melo G,et al. Relation Clasification via Multi-Level Attention CNNs[C]//ACL,2016.   \n55 Miwa M,Bansal M. End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures[C]// Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,2016: 1105-1116.   \n56SC Z,Wang F,Bao H,et al. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme [EB/ OL]. https://arxiv.0rg/pdf/1706.05075.pdf,2017-06-07.",
    "context2": "(责任编辑：孙晓明)",
    "context3": "（上接第168页）   \n26Grassian E,Trueman RB,Clemson P. Stumbling,bumbling,teleporting and flying... librarian avatars in Second Life[J].Reference Services Review,2007,(1): 84-89.   \n27Lynette R,Beth S.When Off-Campus Means Virtual Campus: The Academic Library in Second Life[J]. Journal of Library Administration,2010,50(7):909-922.   \n28Versfeld R,Lee S,Fox E.Digital library in a 3D virtual world: The digital Bleek and Lloyd Collection in Second Life[C]//Lecture Notesin Computer Science,2010: 550-553.   \n29Bell L，Peters T,Pope K.GET A (SECOND)LIFE! PROSPECTING FOR GOLD IN A 3-D WORLDJ]. Computers in Libraries,2007,27(1):10-16.   \n30 Clarke CP.Second Life in the library: an empirical study of new users'experiences[J].Program Electronic Library & Information Systems,2012,46(2):242-257.   \n31Gerardin J,Yamamoto M,Gordon K. Fresh Perspectives on Reference Work in Second Life[J].Reference & User Services Quarterly,2008,47(4):324-330.",
    "context4": "(责任编辑：孙晓明)"
  }
}