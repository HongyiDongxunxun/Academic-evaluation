{
  "original_filename": "full_10105.md",
  "面向信息特征模式识别的核方法研究综述": {
    "context1": "黄炜1,2 刘坤",
    "context2": "(1．湖北工业大学管理学院，湖北 武汉 4300682；2．武汉理工大学管理学院，湖北 武汉 430070)",
    "context3": "[摘要]面对网络大数据的挑战，特征信息的模式识别已成为信息情报领域的研究热点。本文对模式识别的现状进行了剖析，研究了现广泛运用于信息模式识别中的核方法。梳理并对比分析了核方法的各种算法理念与思想，介绍了核算法的设计、核函数的构造与核参数的选择方法，特别探讨了在网络舆情信息模式识别中的应用前景。",
    "context4": "[关键词]模式识别；核方法；核参数；支持向量机；信息模式",
    "context5": "DOI:10.3969/i.issn.1008-0821.2014.03.036",
    "context6": "[中图分类号]G252.8〔文献标识码]A〔文章编号〕1008－0821（2014）03－0168－09"
  },
  "Review on Kernel Algorithm of Information Pattern Recognition": {
    "context1": "Huang Wei1,2²Liu Kun1 (1．School of Management，Hubei University of Technology，Wuhan 43OO68，China; 2.School of Management，Wuhan University of Technology，Wuhan 43OO7O，China）",
    "context2": "〔Abstract）Facingthechalengeof bigdatafrom thenetwork，thecharacteristicinfomationof paternrecognition has be comeahotresearchtopicinthefieldof infomationintellgence·Thispaperanalyzedthestatusquoofpattenrecognition，and exploredthe kernelmethods thatwidelyusedininfomationpatternrecognition·Byanalyzingandcombingthediferentkindsof kerneldeaandthoughtthatappliedinkerelalgorithm，thepaperintroducedthedesignoftekerelalgorithm，theconstruction of the kerelfunctionandthe methodof selectig kernelparameters，especialldiscussedtheapplication prospect innetworkpublic opinion information pattern recognition",
    "context3": "〔Key words）pattern recognition；kernel methods；kernel parameters；support vector machine；infomation pattern",
    "context4": "随着人工智能的发展，人们亟待解决机器学习的各项问题，模式识别是人工智能中一项基本智能，并应用于我们生活中的各个领域，如生物识别（语音识别、人脸识别、指纹识别等)、数据挖掘、文本分类等。20 世纪60 年代初，模式识别迅速发展并成为一门新的学科。但是传统的模式识别方法在大数据环境下，处理多类别的复杂的高维模式识别问题时效果不佳，在各模式存在复杂的非线性关系时甚至无解。20世纪90年代中期，出现了基于核的学习方法(简称为核方法)，该方法最终使得研究人员能够高效地分析复杂非线性问题。",
    "context5": "本文以当前核方法的重点研究方向为向导，收集并整理期刊文献179篇，学位论文9篇，以及基于核方法的模式识别书籍2本。其中模式识别相关文献16篇，核方法 87篇，核函数及核参数56篇，增量学习相关文献20篇，多核学习相关文献8篇，大数据相关文献3篇。文献统计如图1所示。",
    "context6": "本文重在总结和分析经典的核算法，并对核函数和核参数的选择进行探究。同时对在线动态模型 (增量学习)的原理和方法进行概述，最后对多核学习也有一定的介绍和探讨。通过对当前核方法的探究，给当前网络大数据环境下的数据处理提出一种可行的理论方法，特别是给网络信息情报模式识别的应用提供一种思路。"
  },
  "1特征信息的模式识别": {
    "context1": "一般认为，模式是通过对具体的事物进行观测所得到的具有时间与空间分布的信息。模式所属类别或同一类中模式的总体称为模式类，其中个别具体的模式往往称为样本。模式识别（Pattern Recognition）是对表征事物或现象的各种形式的（数值的、文字的和逻辑关系的）信息进行处理和分析，以及对事物或现象进行描述、辨认、分类和解释的过程[1]。",
    "context2": "![](images/ae95e30bdacb9dc85cff64f0e86596cd027757e1eb02dd3cf11637f7ebff40cc.jpg)  \n图1文献统计",
    "context3": "通常情况下，希望模式识别的算法能够具备3个性质：计算的高效性、健壮性以及统计稳定性[2]。传统的模式识别方法大致可以分为模板匹配、统计识别、结构识别、模糊识别和人工神经网络识别5种[3-4]。统计模式识别和结构模式识别是模式识别领域的两大主流研究方向，模糊模式识别和神经元网络模式识别是新近发展起来的模式识别方法。然而，大量实际的模式识别问题是具有多类别的高维的复杂模式的识别，且各模式之间存在复杂的非线性关系，传统的统计模式识别方法能高效率地解决具有线性关系的模式识别问题，但无法高效率地检测非线性关系。此外，传统统计学主要研究的是渐进理论，即当样本数趋近于无穷大的统计性质，而现实中的模式识别问题由于各种因素的约束样本数往往是有限的。虽然机器学习中神经网络模式识别的发展使得检测非线性模式成为可能，然而这些非线性算法是建立在梯度下降法和贪婪启发式法的基础上，因而受到局部极小化的限制。这些算法还经常遇到过拟合的问题，“过学习”的特点使得得出的算法结构表现出很差的推广能力。",
    "context4": "而面对当前网络信息环境，网络信息呈现的特点是：(1）数量庞大、增长迅速；(2）内容丰富、覆盖面广；（3)信息质量参差不齐，有序与无序并存，数据类型繁多；（4)信息共享程度高、使用成本低；（5）内容新颖实效性强。要实现网络信息的分类管理或是对信息的有效甄别、控制等，传统的模式识别方法显然力不从心。20世纪90 年代中期，出现了基于核的学习方法，该方法是从统计学习理论中发展出来的较新的学习方法，它有效克服了传统模式识别方法的局部极小化和不完全统计分析的问题，在处理非线性关系的高维复杂模式识别问题时，有着显著的优势。"
  },
  "2基于核方法的模式识别": "",
  "2.1核方法": {
    "context1": "基于核的学习方法 (简称为核方法)，它以统计学习理论和核技术为基础，该方法是统计分析方法的进一步发展。从计算、统计和概念的各个角度来看，核方法和线性算法一样，在高维特征空间内也能够达到很高的识别效率，具有很好的推广应用性。并且，神经网络和决策树中典型局部极小化和过拟合问题，核方法也已得到解决。",
    "context2": "核方法的基本思想主要是利用样本空间内定义的核函数直接计算出映射空间的点积，不必显式计算从样本输入空间到高维空间的映射，可以利用已有的线性求解算法访问那些非常复杂的非线性空间。核方法是一种模块的方法，可分为算法设计和核函数设计两个部分，其核心部分是核函数。核函数的基本作用就是接收两个低维空间的向量，并计算出经过某个变换后在高维空间的内积值。基于核函数的方法具体实施步骤为：",
    "context3": "（1）收集和整理样本，并进行标准化;（2）选择或构造核函数；（3）用核函数将样本变换成为核函数矩阵。这一步相当于将输入数据通过非线性函数映射到高维特征空间;（4）在特征空间对核函数矩阵实施各种线性算法;（5）得到输入空间中的非线性模型。",
    "context4": "显然，将样本数据核化成核函数矩阵是核函数方法中的关键。核算法为处理许多模式识别问题提供了一个统一的框架。核算法的流程如图2所示：",
    "context5": "![](images/efe1b13c05ec75876a186e75cb2a7f92b83eba413110b6b5024ab5cff59f244d.jpg)  \n图2核算法的流程",
    "context6": "核算法的特点可归纳为：",
    "context7": "（1）它是一类将非线性问题线性化的普适方法;(2）核算法的模块性表明它是一个具有可重用性的算法；（3）核算法的计算量只与样本数有关、与样本维数无关，有效地避免了“维数灾难”;（4）核函数的引用，使在无限维特征空间进行计算成为可能，提高了模式识别的能力；（5）无需关注低维向高维转化的非线性函数 $\\Phi ( \\cdot )$ 的形式及其参数;（6）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数的方法，而且这两部分的设计可以单独进行，为不同的实际问题选择不同的核函数和算法。"
  },
  "2.2模式识别的核方法": "",
  "2.2.1基于核的最小平方误差算法": {
    "context1": "基于核的最小平方误差算法即 KMSE，MSE（MinimumSquared Error最小均方误差）正如其名，其方法本质为最小二乘法。该方法主要用于两类分类。在使用其算法的过程中为解决非线性问题，把样本由低维向高维映射，并引入核。KMSE的主要算法可描述为：",
    "context2": "用1和一1作为两类样本的类别标签，设 $\\Phi$ 为一非线性映射，在映射后的特征空间考虑分类问题。令样本数为$l$ ，其中标记为1类的样本有 $l _ { 1 }$ 个，标记为一1类的样本有$l _ { 2 }$ 个（ $l _ { 1 } + l _ { 2 } { = } l ;$ 。通过训练样本，构建逼近关系：",
    "context3": "$$\n\\Phi _ { } \\mathrm { { { W } } } = \\mathrm { { } } _ { Y }\n$$",
    "context4": "其中， $\\begin{array} { r } { \\boldsymbol { W } = [ \\mathbf { \\nabla _ { w 0 } } w ^ { T } ] ^ { T } , \\boldsymbol { Y } = [ \\mathbb { 1 } \\cdots \\mathbb { - 1 } ^ { T } ] ^ { T } } \\end{array}$ ， $\\Phi =$ $\\left[ \\begin{array} { c c c c } { { 1 } } & { { 1 } } & { { \\cdots } } & { { 1 } } \\\\ { { \\Phi ( { x } _ { 1 } ) } } & { { \\Phi ( { x } _ { 2 } ) } } & { { \\cdots } } & { { \\Phi ( { x } _ { l } ) } } \\end{array} \\right]$ ，对式子 $\\Phi _ { \\sf } { W } = \\boldsymbol { Y }$ 进行求解，可求出其最小二乘解，即为解方程组：",
    "context5": "$$\n\\Phi ^ { T } \\Phi _ { W } = \\Phi ^ { T } Y\n$$",
    "context6": "根据再生核理论，鉴别方向 $W$ 可改写为：",
    "context7": "$$\n{ \\boldsymbol { \\mathbf { \\mathit { w } } } } = \\left[ \\sum _ { i = 1 } ^ { w _ { 0 } } \\alpha _ { i } ^ { l } \\Phi ( { \\boldsymbol { \\mathbf { \\mathit { x } } } } _ { i } ) \\right]\n$$",
    "context8": "引入Mercer 核 $k ( \\mathbf { \\Phi } _ { x _ { i } } , \\mathbf { \\Phi } _ { x _ { j } } ) = \\Phi ^ { T } ( \\mathbf { \\Phi } _ { x _ { i } } ) \\Phi ( \\mathbf { \\Phi } _ { x _ { j } } )$ ，公式（1）可转化为：",
    "context9": "$$\nK A = Y\n$$",
    "context10": "$$\n\\begin{array} { r } { \\mathrm {  ~ \\cdot ~ } _ { A } = \\left[ \\begin{array} { c } { w _ { 0 } } \\\\ { \\alpha _ { 1 } ^ { l } } \\\\ { \\vdots } \\\\ { \\vdots } \\\\ { \\alpha _ { l } ^ { l } } \\end{array} \\right] , K = \\left[ \\begin{array} { c c c c } { 1 } & { k ( x _ { 1 } , x _ { 1 } ) } & { \\cdots } & { k ( x _ { 1 } , x _ { l } ) } \\\\ { 1 } & { k ( x _ { 2 } , x _ { 1 } ) } & { \\cdots } & { k ( x _ { 2 } , x _ { l } ) } \\\\ { \\vdots } & { \\vdots } & { \\cdots } & { \\vdots } \\\\ { 1 } & { k ( x _ { l } , x _ { 1 } ) } & { \\cdots } & { k ( x _ { l } , x _ { l } ) } \\end{array} \\right] , } \\end{array}\n$$",
    "context11": "公式（4）的最小二乘解的一般形式为：",
    "context12": "$$\nA ^ { = } ( K ^ { T } K ) ^ { - 1 } K ^ { T } Y\n$$",
    "context13": "由于 $K ^ { T } K$ 是病态矩阵，可引入正系数 $\\mu$ ，及单位矩阵$I$ ，可得",
    "context14": "$$\nA { = } ( K ^ { T } K { + } \\mu _ { } ) ^ { - 1 } K ^ { T } Y\n$$",
    "context15": "求出 $A$ 后，不需要另外设计分类器即可实现分类。计算待测样本 $x$ 在鉴别方向上的投影值即可：",
    "context16": "$$\nl _ { p } ( \\boldsymbol { x } ) = \\boldsymbol { w } _ { 0 } + \\sum _ { i = 1 } ^ { l } \\alpha _ { i } ^ { l } k ( \\boldsymbol { x } , \\boldsymbol { x } _ { i } )\n$$",
    "context17": "$$\n\\mathbf { \\Psi } _ { y } = \\left[ \\frac { 1 } { \\sqrt { \\lambda _ { 1 } } } \\sum _ { j = 1 } ^ { N } \\alpha _ { j } ^ { 1 } k ( \\mathbf { \\Psi } _ { x _ { j } } , \\mathbf { \\Psi } _ { x } ) \\right.\n$$",
    "context18": "对KPCA方法分析可知：PCA主要为数据抽取技术，以抽取不相关的各个特征分量；从另一角度看，PCA也是一种使变化结果具有最大方差的技术，使得变换后的样本信息，不仅具有代表性，且差异显著；PCA在建模过程中不需要考虑训练样本的所属类别，为无监督学习；KPCA主要用于特征的抽取，要实现分类还需另外设置分类器；文献[5]实验证明了核主成分分析比主成分分析对分类器性若 $l _ { p } ( x ) > 0$ ，则样本 $x$ 分到1类，否则分到－1类。",
    "context19": "在这个算法中，有两点值得注意：KMES模型中方程个数与训练样本数相同；该模型的物理意义是依据W对样本进行变换时 (即进行特征抽取的过程)，特征抽取的结果与样本的类别标签具有最小均方误差，达到最大程度地逼近所属标签的目的。"
  },
  "2.2.2核主元分析": {
    "context1": "主元分析（PCA，Principal Components Analysis）的技术原理是在最小均方误差意义上压缩数据，即在维数不变的情况下，使用主分量分析的方法对原数据进行变换，变换后的数据中将包含最多的原数据中的信息，使得信息损失最小。获得变换轴的方法为求解特征方程较大的特征值相对应的特征向量，这些特征向量即为变换轴。核主元分析KPAC（Kernel Principal Components Analysis）即在PCA 方法中引入“核技巧”，借助“核技巧”将输入空间由低维向高维映射后，处理非线性问题，在映射后的特征空间实现PCA。",
    "context2": "KPCA具体可描述为：",
    "context3": "设映射为 $\\Phi$ ，且此时数据满足“中心化”条件：",
    "context4": "$$\n\\begin{array} { r } { \\sum _ { i = 1 } ^ { N } \\Phi ( x _ { i } ) = 0 } \\end{array}\n$$",
    "context5": "则特征空间中协方差矩阵为：",
    "context6": "$$\n\\begin{array} { r } { \\sum = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\Phi ( x _ { i } ) \\Phi ( x _ { i } ) ^ { T } } \\end{array}\n$$",
    "context7": "由于变换轴 $u _ { i }$ 必位于 $\\Phi ( x _ { 1 } ) , \\Phi ( x _ { 2 } ) , \\cdots , \\Phi ( x _ { N } )$ 的子空间中，即",
    "context8": "$$\n\\begin{array} { r } { u _ { i } = \\sum _ { j = 1 } ^ { N } \\alpha _ { j } ^ { i } \\Phi ( x _ { j } ) } \\end{array}\n$$",
    "context9": "式中： $\\alpha ^ { i } = \\left[ \\begin{array} { c c c c } { { \\alpha _ { 1 } ^ { i } } } & { { \\alpha _ { 2 } ^ { i } } } & { { \\cdots } } & { { \\alpha _ { N } ^ { i } } } \\end{array} ^ { T } \\right. _ { \\circ }$",
    "context10": "求解特征方程",
    "context11": "$$\nK \\alpha = \\lambda \\alpha\n$$",
    "context12": "设若干个较大的非零特征值按降序排列为 $\\lambda _ { 1 } , \\lambda _ { 2 } , \\dots , \\lambda _ { m }$ $( m { \\le } N )$ ，相应的特征向量为 $\\alpha ^ { 1 } , \\alpha ^ { 2 } , \\cdots , \\alpha ^ { m }$ ，并假设特征空间中的单位变换轴分别为 $u ^ { 1 } , u ^ { 2 } , \\cdots , u ^ { m }$ ，则：",
    "context13": "$$\nu ^ { i } = \\frac { 1 } { \\sqrt { \\lambda _ { i } } } \\sum _ { j = 1 } ^ { N } \\alpha _ { j } ^ { i } \\Phi ( x _ { j } ) , i = 1 , 2 , \\cdots , m\n$$",
    "context14": "基于上式变换轴的表达式，可得到特征空间中样本 $\\Phi$ $\\left( { \\boldsymbol { x } } \\right)$ 在 $u ^ { i }$ 上投影的表达式，再将特征空间中前 $m$ 个主分量相应的投影值组成新的向量，即可得到样本 $x$ 在特征空间中特征抽取的结果：",
    "context15": "$$\n\\cdots \\quad { \\frac { 1 } { \\sqrt { \\lambda _ { m } } } } \\sum _ { j = 1 } ^ { N } \\alpha _ { j } ^ { m } k ( x _ { j } , x ) \\Biggr ] ^ { T }\n$$",
    "context16": "能改善的效果更好，且若先对原始数据集进行特征选择，再利用核主成分分析进行特征提取可进一步改善分类器的性能；文献[6]用我国30个省市1993年农民家庭消费状况的数据进行了特征抽取，实验结果表明采用KPCA可获得比PCA更好的降维效果，且不论采用何种核函数，第一主成分的贡献率都在 $9 5 \\%$ 左右。"
  },
  "2.2.3核Fisher鉴别分析": {
    "context1": "核Fisher 鉴别分析即 KFDA（Kernel Fisher discriminantanalysis)，FDA基于寻求“最佳”投影方向（即变换抽）的思想：所有样本投影到“最佳”投影方向后，应该具有最大的类间距离与类内距离比值。并称该方向对应的向量为Fisher最佳鉴别向量，称样本投影到投影方向后具有最大类间距离与最小类内距离的准则为Fisher准则[]。FDA基于最佳鉴别向量的思想，构建最具鉴别性的向量集，从而利用多个正交鉴别向量处理分类问题。",
    "context2": "在FDA算法中引入“核技巧”后即为KFDA，该算法可描述为：",
    "context3": "设输入空间为 $d$ 维空间，相应的非线性映射为 $\\Phi$ ，映射后的特征空间为 $F$ 。在 $F$ 中Fisher准则函数为：",
    "context4": "$$\nJ ( w ) = \\frac { w ^ { T } S _ { b w } ^ { \\Phi } } { w ^ { T } S _ { w } ^ { \\Phi } w }\n$$",
    "context5": "其中， $w$ 为鉴别向量，满足 $w \\in F$ ， $S _ { b } ^ { \\Phi }$ 和 $S _ { w } ^ { \\Phi }$ 分别为 $F$ 中的类间散布矩阵和类内散布矩阵，且求得的鉴别向量 $w$ 应使得式（14）达到最大值。以两类分类为例，可设 $c 1$ 类样本为 $x _ { 2 } ^ { 1 } , x _ { 2 } ^ { 1 } , \\ldots , x _ { l 1 } ^ { 1 }$ ，共 $l _ { 1 }$ 个， $c 2$ 类样本为 $x _ { 1 } ^ { 2 } , x _ { 2 } ^ { 2 } , \\ldots , x _ { l 2 } ^ { 2 }$ ，共 $l _ { 2 }$ 个（ $l _ { 1 } + l _ { 2 } = l )$ 。假设样本各类的先验概率相等，则$S _ { b } ^ { \\Phi }$ 和 $S _ { w } ^ { \\Phi }$ 分别表达为：",
    "context6": "$$\nS _ { b } ^ { \\Phi } { = } ( m _ { 1 } ^ { \\Phi } { - } m _ { 2 } ^ { \\Phi } ) ( m _ { 1 } ^ { \\Phi } { - } m _ { 2 } ^ { \\Phi } ) ^ { T }\n$$",
    "context7": "$$\nS _ { w } ^ { \\Phi } = \\textstyle \\sum _ { i = 1 } ^ { 2 } \\sum _ { j = 1 } ^ { l _ { i } } ( \\Phi ( x _ { j } ^ { i } ) - m _ { i } ^ { \\Phi } ) ( \\Phi ( x _ { j } ^ { i } ) - m _ { i } ^ { \\Phi } ) ^ { T }\n$$",
    "context8": "式中：",
    "context9": "$$\nm _ { i } ^ { \\Phi } = \\frac { 1 } { l _ { i } } \\sum _ { j = 1 } ^ { l _ { i } } \\Phi ( x _ { j } ^ { i } )\n$$",
    "context10": "根据再生核理论：鉴别向量 $w$ 位于特征空间所有训练样本组成的子空间中，因此，鉴别向量可表示为：",
    "context11": "$$\n\\begin{array} { r } { \\boldsymbol { w } = \\sum _ { i = 1 } ^ { l } \\boldsymbol { \\alpha } _ { i } \\Phi ( \\boldsymbol { x } _ { i } ) } \\end{array}\n$$",
    "context12": "引入核后，基于核的Fisher准则函数为：",
    "context13": "$$\n\\begin{array} { r } { J ( \\mathbf { \\theta } \\propto ) = \\frac { \\alpha ^ { T } M \\alpha } { \\alpha ^ { T } N \\alpha } , \\mathbf { \\Theta } _ { \\alpha } = \\left[ \\begin{array} { l l l l } { \\alpha _ { 1 } } & { \\cdots } & { \\alpha _ { \\mathcal { L } } } & { T } \\end{array} \\right. } \\end{array}\n$$",
    "context14": "式中：",
    "context15": "$$\n\\begin{array} { l } { { M { = } ( { M _ { 1 } { - } } M _ { 2 } ) ( { M _ { 1 } { - } } M _ { 2 } ) ^ { T } } } \\\\ { { \\nonumber } } \\\\ { { N { = } \\sum _ { i = 1 } ^ { 2 } { K _ { i } ( I { - } I _ { l _ { i } } ) K _ { i } ^ { T } } } } \\end{array}\n$$",
    "context16": "其中， $M _ { i }$ 为 $l$ 维列向量， $( M _ { i } ) _ { j } = \\frac { 1 } { l _ { i } } \\sum _ { n = 1 } ^ { l _ { i } } k ( x _ { j } , x _ { n } ^ { i } )$ ，$j = 1 , 2 , \\cdots , l$ ， $i = 1 , 2$ ， $I$ 为单位矩阵， $I _ { l _ { i } }$ 为 $l _ { i } \\times l _ { i }$ 阶矩阵且所有元素都为 $\\frac { 1 } { l _ { i } }$ ， $K _ { i }$ 为 $\\mathbf { \\xi } _ { l } \\times \\mathbf { \\xi } _ { l _ { i } }$ 阶矩阵， $K _ { i }$ 为 $c _ { i }$ 类的核矩阵， $( K _ { n } ) _ { i , j } = k ( x _ { i } , x _ { j } ^ { n } ) , i = 1 , 2 , \\cdots , l , j = 1 , 2 , \\cdots , l _ { n } , n $ $= 1 , 2$ 。",
    "context17": "此时，求鉴别向量 $w$ 的问题转化为求向量 $\\alpha$ 的问题，$\\alpha$ 可通过求解广义特征方程",
    "context18": "$$\nN \\alpha { = } \\lambda N \\alpha\n$$",
    "context19": "的最大特征值所对应的特征向量得到。特别地，对于两类问题， $\\alpha { = } N ^ { - 1 } ( ~ M _ { 1 } { - } ~ M _ { 2 } )$ 。",
    "context20": "在得到 $\\alpha$ 后即可设计分类器，该分类器基于最小距离",
    "context21": "的原则。首先算出任意样本 $x _ { t }$ 关于KFDA的鉴别向量 $\\alpha$ 的特征抽取结果：",
    "context22": "$$\n\\begin{array} { r } { f ( x _ { t } ) = \\sum _ { i = 1 } ^ { l } \\alpha k ( x _ { t } , x _ { i } ) } \\end{array}\n$$",
    "context23": "式中， $x _ { i }$ 为训练集中的第 $i$ 个样本。再计算 $f _ { 1 }$ 的 $f _ { 2 }$ ：",
    "context24": "$$\nf _ { i } = \\frac { 1 } { l _ { i } } \\sum _ { j = 1 } ^ { l _ { i } } \\sum _ { n = 1 } ^ { l } \\alpha _ { n } k ( x _ { n } , x _ { j } ^ { i } ) , i = 1 , 2\n$$",
    "context25": "最后采用最小分类器的原则进行分类： $f ( x _ { t } )$ 与 $f _ { 1 }$ 间距离小于与 $f _ { 2 }$ 间距离，则 $x _ { t }$ 的分类为 $c _ { 1 }$ 类，否则，为 $c _ { 2 }$ 类。",
    "context26": "由KFDA方法可以得出：FDA在训练样本的阶段知道并运用了样本的信息，属于有监督学习；相比于无监督学习，在训练样本的学习阶段有效地运用样本的类别信息，可使得变换后的样本有着更好的分离性[7]；FDA 方法中,训练期鉴别向量的个数决定了变换后样本信息的维数;PCA和FDA都可用于降维；KFDA方法可直接用于多类分类，不需构造多个两类分类，具体方法是根据不同的鉴别向量集来抽取不同类别的相应特征信息，从而实现多类分类。"
  },
  "2.2.4 SVM": {
    "context1": "非传统的学习方法如神经网络算法在解决非线性问题方面有一定的优越性，但存在过拟合、局部极小值和网络结构复杂度问题。1995 年，Cortes 和Vapnik 提出了支持向量机（SVM）的概念和算法。这个新的模式识别的方法，主要用于解决小样本、非线性及高维模式识别问题。",
    "context2": "SVM的关键在于核函数。由于低维空间向量集通常难于划分，解决的方法是将它们映射到高维空间，而核函数的展开和计算方法避免了向高维映射后特征空间中的“维数灾难”问题。基于核函数的SVM可有效地解决有限样本条件下的模式分类问题，并且具有很强的推广能力。理论性强、适应性强、全局优化性、推广能力强，这些正是支持向量机在核方法中成为主流方法的原因。SVM算法基于最大间隔分类的思想，在寻找最优分类超平面时，力求不同类别的样本与分类面之间有最大距离[8]。最简单的 SVM模型可描述为：",
    "context3": "设有样本 $l$ 个，其中标记为1类的样本有 $l _ { 1 }$ 个，标记为－1类的样本有 $l _ { 2 }$ 个（ $l _ { 1 } + l _ { 2 } = l \\ ` _ { }$ )。分隔两类样本的超平面数学表达式为：",
    "context4": "$$\nw ^ { T } x ^ { + } b { = } 0\n$$",
    "context5": "式中： $w$ 为分隔超平面的法向量， $^ { b }$ 为超平面与原点间的偏移。构建约束表达式，以求得 $w$ 和 $^ { b }$ ：",
    "context6": "$$\n\\left\\{ \\begin{array} { l l } { \\displaystyle \\operatorname* { m i n } \\frac { 1 } { 2 } \\| \\boldsymbol { \\boldsymbol { w } } \\| ^ { 2 } } \\\\ { \\displaystyle y _ { i } ( \\langle \\boldsymbol { \\textbf { \\em w } } , \\boldsymbol { x } _ { i } \\rangle + b ) \\ge 1 } \\end{array} , \\right. 1 { = } 1 , 2 , \\cdots l\n$$",
    "context7": "最后，通过决策函数：",
    "context8": "$$\nf ( x ) { = } _ { \\mathrm { s i g n } ( \\langle ~ w } , x \\rangle + b )\n$$",
    "context9": "来判定待测样本属于1类还是一1类。求解的过程中，为了提高计算多维特征向量的效率以及解决非线性问题而在算",
    "context10": "法中引入核技巧。",
    "context11": "SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法去发现目标函数的全局最小值；SVM通过最大化决策边界的边缘来控制模型的能力，但用户必须提供其他参数，如使用核函数类型和引入松弛变量等；未改进的SVM一般只能用在二类问题，对于多类问题需对SVM算法进行改进。",
    "context12": "对于以上4种模式识别过程中常规的核方法，可进行研究对比：这四种模式识别的核方法都是由低维向高维映射的非线性关系中引入核技巧，它们都是由线性思想的算法转化来的核算法；核算法的构造思想不同，使得基于核的模式识别模型有很大的差异；KPCA与KFDA主要是对特征抽取过程中引入核，且具有降低样本维数的特点，其变换后样本的最大维数为训练样本的个数；KMSE，KFDA，SVM为有监督学习方法，KPCA为无监督学习方法；应用于分类问题时，KPCA和KFDA能直接处理多类问题（KP-CA和KFDA可直接对多类问题进行相应的特征抽取，并根据特征抽取结果直接实现多类分类)，而KMSE和SVM只能直接处理两类问题，若要多类分类，还需采取一对一或一对多的方案构造多个二值分类器来解决多类分类问题。"
  },
  "2.3核函数及核参数选择的研究": {
    "context1": "SVM和其它核方法的性能在很大程度上取决于核函数的种类及参数，但目前还没有一种对具体问题普遍适用的核函数构造方法。如何进行核函数类别选择和参数优化，一直缺少理论指导，这是核算法研究急需解决的问题。"
  },
  "2.3.1核函数选择研究": {
    "context1": "在算法的建立过程中，核函数至少要满足两个条件：一是满足Mercer条件；二是能反映实际运用中训练数据的分布特性[9]。常用的核函数可分为两类，即平移不变核函数和内积核函数，如：",
    "context2": "(1）高斯核函数(也称径向基核函数)：$K ( \\mathbf { \\Delta } x , x _ { i } ) { = } _ { \\mathbf { e x p } } ( { - } \\parallel \\mathbf { \\Delta } _ { x } { - } \\mathbf { \\Delta } _ { x _ { i } } \\parallel ^ { 2 } / 2 \\sigma ^ { 2 } ) ;$",
    "context3": "(2）多项式核函数：",
    "context4": "$$\nK ( \\mathbf { \\Phi } _ { x } , \\mathbf { \\Phi } _ { x _ { i } } ) = ( \\langle \\mathbf { \\Phi } _ { x } , \\mathbf { \\Phi } _ { x _ { i } } \\rangle + \\theta ) ^ { d } , d { = } 1 , 2 , \\cdots , N ;\n$$",
    "context5": "(3）感知器核函数(也称 Sigmoid 核函数)",
    "context6": "$$\nK ( \\mathbf { \\Psi } _ { x } , \\mathbf { \\Psi } _ { x _ { i } } ) = \\mathop { t a n h } ( \\mathbb { R } \\mathbf { \\Psi } _ { x } , \\mathbf { \\Psi } _ { x \\hat { \\nu } } + \\mathbf { \\Psi } _ { b } ) , \\quad \\mathbb { B > 0 } _ { \\circ }\n$$",
    "context7": "由以上核函数的形式可看出高斯核函数为平移不变核，多项式核函数和感知器核函数为内积核函数，为旋转不变核。",
    "context8": "核函数的选择决定了特征空间的结构。在使用多项式核函数时，若特征空间维数很高则 $d$ 值很大，则核算法的计算量激增甚至有时候不能得到正确的结果；多项式核函数中有两个参数，这对参数的选取带来一定的麻烦；而Sigmoid 核函数中的两个参数 $\\beta$ 和 $^ { b }$ 只对某些值满足 Mercer条件，因此它的使用也受到一定限制；径向基核函数是一个普适的核函数，在合理选择参数 $\\sigma$ 的情况下，径向基核",
    "context9": "函数可用于任意样本的分布。",
    "context10": "当然，核函数之间也不是相互独立的。文献[10］中证明了线性核函数和多项式核函数是径向基核函数的特殊形式；Lin 等在文献[11］中论述了在某些参数情况下，多项式核函数和径向基核函数具有相似性。文献［12］对径向基核函数、多项式核函数、感知器核函数进行了特性的分析，通过仿真实验得出结论：识别的正确率与样本数量以及核函数及其参数的选择有很大关系；相对于径向基函数和感知机函数，多项式核函数参数对样本规模的变化更加敏感；当样本较大时，采用径向基和感知机的收敛效果和识别效果比多项式核函数好；径向基核函数相对于其它两种核函数，无论对低维还是高维，大样本还是小样本的情况都适用，且有较宽的收敛域。",
    "context11": "文献［13]指出核函数主要有两种类型：局部性核函数和全局性核函数。局部核函数仅对测试点附近的小范围内的数据有影响；全局核函数不仅对测试点附近的小范围数据有影响，对远离测试点的数据也有一定影响；结合这两类核函数的特点混合起来可构造一个混合核函数，仿真实验证明，混合核函数确实比普通的单核有更好的性能。",
    "context12": "对于核函数不仅可以使用常见的核函数，还可构造满足Mercer条件的核函数运用于实际问题的解决。文献[14]给出了核函数的基本性质以及核函数的构造方法，并提出把实际问题的知识与核函数的设计集合起来，这对于提高SVM的性能很重要。文献［15］介绍了子波核函数和多尺度核函数，并证明了它们比传统核函数更有效。",
    "context13": "对核函数、核参数选择工作概括为2个方面：（1）针对具体问题改进或构造新的核函数，以提高分类器的推广能力。（2）利用不同的自适应优化策略，选择合适的核函数并控制参数，提高系统的综合性能[16]。"
  },
  "2.3.2核参数选择研究": {
    "context1": "利用不同的自适应优化策略，来提高系统的综合性能是寻找最优参数的最终目标。核参数的选择标准可从两个方面进行考虑：一是仿真实验的对比结果；二是利用理论分析，需找与期望误差最近的边界[9]。",
    "context2": "参数选择问题，其实就是一个优化问题。目前核参数选取方法主要有：经验选择法、实验试凑法、梯度下降法、交叉验证法、Bayesian 法等。（1） $K$ 重交叉验证（k FoldCross Validation)",
    "context3": "将训练数据集分成 $k$ 份相等的子集，每次将其中 $k ^ { - 1 }$ 份数据作为训练数据，而将另外一份数据作为测试数据。这样重复 $k$ 次，根据 $k$ 次迭代后得到的 MSE平均值来估计期望泛化误差，最后选择一组最优的参数。",
    "context4": "(2）留一法误差估计 $( \\mathrm { L e a v e ^ { - } O n e ^ { - } O u t }$ ，LOO)",
    "context5": "LOO是交叉验证法的一个特例。LOO的原理是从 $l$ 个样本中选择其中的 $l ^ { - 1 }$ 个样本进行分类学习，然后用得到的分类器去判断剩下的一个样本，将这个过程重复l次后，统计总的错误个数。基于统计理论中留一法得到的错误率估计是分类器真实错误率的一个无偏估计[17]，由此来判断此参数下分类性能如何。对于基于核的SVM算法，由于LOO 计算量很大，而在SVM中真正影响分类器的是起着决定作用的支持向量，自然可想到在“留一”的过程中若只留下支持向量，可在很大程度上减少计算量。",
    "context6": "对于核函数及其参数的选择：各种实验的观察结果表明，某些问题用某些核函数效果很好，用另一些就很差。但是一般来讲，使用径向基核函数不会出太大的分类偏差，当难以选择核函数时，径向基核函数可作为首选；依据待解决的实际问题具体分析，结合核函数的特点选取适合于样本分布特征的核函数，是可以改善分类器的性能的；核函数及参数的选择可针对具体的算法(如 KFDA 和KMSE)使参数的选择具有相应的明确的意义，这样可以提高分类器的性能；选择合适的核函数和参数，可实现在解决分类问题的前提下大大减少计算量；核函数和参数的选择要综合考虑模型的泛化能力，不能使选出的函数和参数在训练中有很好的效果，但在实际运用中达不到理想的分类精度;使用混合核函数或多核学习，能达到更好的分类性能；利用已有的选择方案用于类似的实际问题中（例如待解决的问题是异常数据的检测问题，可参照单值SVM的核函数选择方案)；根据核函数的性质，可根据当前模式识别模型构造一个新的合适的核函数来解决实际问题。"
  },
  "2.4核方法的问题与改进": "",
  "2.4.1核方法的现状及存在的问题": {
    "context1": "模式识别由一般的线性方法，到一般的非线性方法，再到20世纪90年代出现的基于核函数的模式识别方法，特别是以SVM算法为研究对象的主流算法，攻克了模式识别领域中遇到的各种问题。基于核的方法在理论上已经很成熟，但是在具体的实际应用中还有很对问题待解决，这些问题主要表现在：",
    "context2": "（1）新的核方法的提出：在普通的线性算法中，凡是出现向量内积的运算形式，便可将其核化。",
    "context3": "（2）核函数的选择及其参数的优化：不同的应用问题中，核函数的选择及参数的优化对识别效率有着很大的影响，但目前核函数及参数的选择没有明确的理论指导。",
    "context4": "（3）核算法的正则化：将线性问题转化为非线性问题的这一核化过程中通常会产生不适定问题，正则化技术是处理不适定问题的有效途径。",
    "context5": "(4）高效算法的实现，降低核算法的复杂度：核算法的过程中仍然会遇到大样本对存储空间和时间效率的要求，因此对核算法进行改进以获得对系统资源需要少，效率高的算法是很必要的。",
    "context6": "(5）建立在线动态模性：在解决模式识别的过程中，有很多系统具有时效性，不同时间的模型特点有所改变，如何进行动态的识别技术也是难点之一。",
    "context7": "（6）多类分类的实现：多数核方法与SVM一样，其本质为二值模式分类器，如何利用二值分类算法推广到多值分类实现快速分类，是当前很有实用意义的一个课题。",
    "context8": "（7）核算法与其他模式识别技术相结合：研究核算法的内在共性和联系，构造多种核算法结合运用的新算法以及将核技术与其他模式识别技术相结合，往往会得到更好的效果。"
  },
  "2.4.2基于核的模式识别方法改进研究": {
    "context1": "对核方法的改进主要有两种思路：一是对已有算法进行优化改造，解决核算法中计算量很大的问题或是提高原有方法的分类性能；二是根据实际运用问题的特点，想办法构建新模型，保证分类精度和泛化能力。",
    "context2": "（1）对常规核算法：KMSE，KPCA，KFDA的简单改进",
    "context3": "首先，对于2.2节中提到的算法KMSE，KPCA，KFDA，都有一个较为简单的共通的改进方法：抽取少于原始样本的数量的“节点”，设这些节点为 $x ^ { ' } 1 , x ^ { ' } 2 , \\cdots , x ^ { ' } r$ ，这些节点能代替（几乎）全部训练样本（设有 $l$ 个）中的信息，来表征特征空间中的鉴别向量，即：",
    "context4": "$$\n\\begin{array} { r } { \\boldsymbol { w } \\approx \\sum _ { i = 1 } ^ { r } \\beta _ { i } \\Phi ( \\boldsymbol { x } ^ { \\prime } { } _ { i } ) , \\boldsymbol { r } < \\boldsymbol { l } } \\end{array}\n$$",
    "context5": "这样，在尽量不损失原始样本信息的情况下，很大程度上降低了计算的复杂度。而且虽然选出的节点只是训练样本的一部分，但所有的训练样本和类别标签都参与了训练（因为改进后的模型中，方程个数仍与训练样本数相同)。",
    "context6": "KMSE，KPCA，KFDA在训练时，选取这些节点所采用的原则一般不同：KPCA中，认为对于所有的训练样本，其中一部分占较大的权重，找出这些比较重要的样本。具体选取方法为依据特征方程中特征值的大小来判断训练样本在构造最优变换抽中的重要程度，相应的较大的特征向量组成得到的变换抽对原数据进行变化时包含的原数据的信息最多，于是按降序选取前 $m$ 个较大的特征值相应的特征向量作为变换抽；核Fisher 鉴别分析基于Fisher准则函数（内间距离与内类距离的比值最大）取得最大值，等价于求解过程中求解广义特征方程取得最大值，并取广义特征方程中前若干个最大的特征值对应的特征向量组成鉴别向量集。此外，在多类分类中，不同类别的样本进行不同的特征抽取时，特征向量由不同的特征方程产生；至于KMSE还没有一定的原则标准来选出这些重要的“节点”。"
  },
  "（2）普通SVM算法的简单改进": {
    "context1": "现实中的样本数据往往不是理想的，最典型的问题便是出现非线性问题本可以转换为线性问题进行解决，可是由于若干样本点反常状态而影响了整个学习机的效率。为了解决这几个“顽固的”异常点所引起的问题，引入一个为松弛变量的概念，这种方法叫做“软间隔”。",
    "context2": "此时，优化模型如下：",
    "context3": "$$\n\\left\\{ \\begin{array} { l l } { \\operatorname* { m i n } \\frac { 1 } { 2 } \\| { \\boldsymbol { \\omega } } \\| ^ { 2 } + c \\sum _ { i = 1 } ^ { \\iota } \\xi _ { i } } \\\\ { y _ { i } ( { \\boldsymbol { \\zeta } } \\ { \\boldsymbol { w } } , { \\boldsymbol { x } } _ { \\Dot { \\boldsymbol { \\nu } } } ^ { } + b ) \\geq 1 - \\xi _ { i } } \\end{array} \\right. , \\ \\xi _ { i } \\geq 0 , \\ i = 1 , 2 , \\dots , l\n$$",
    "context4": "其中， $\\boldsymbol { \\mathsf { \\xi } } _ { \\mathrm { \\xi } _ { i } }$ 即为松弛变量，若取松弛变量为 $\\boldsymbol { \\{ \\bar { \\varepsilon } _ { i } ^ { 2 } } $ ，则称作“二阶软间隔”；常数 $C$ 为事先为已经确定的值，称作惩罚因子，根据待解决问题的实际经验对不同的样本点给予不同的重视，赋予不同的 $C$ 值。",
    "context5": "此外，惩罚因子还有另外一个很重要的作用，用于解决“数据集偏斜”的问题。此时，惩罚因子和松弛变量的形式可改进为：",
    "context6": "$$\n\\begin{array} { r } { C ^ { + } \\sum _ { i = 1 } ^ { p } \\zeta _ { i } + C - \\sum _ { j = p ^ { + 1 } } ^ { p ^ { + } q } \\zeta _ { j } } \\end{array}\n$$",
    "context7": "式中， $i = 1 , \\cdots , p$ 都是正类样本， $i = p + 1 , \\cdots , p + q$ 都是负类样本。对赋予 $C -$ 较大值即可对负类引起足够重视，从而在一定程度上解决样本集偏斜问题。文献［18]给出了算法的实现过程，且实验证明了增加了惩罚因子和松弛变量 SVM的有效性。",
    "context8": "文献[9， $1 9 - 2 0 ]$ 等文献，给出了一种孤立点监测（或称异常点检测）的改进支持向量机一—单值SVM（One一class SVM）方法。其学习机构造的思想为：",
    "context9": "单值SVM基于异常样本点的检测只有两个结果“是”或“不是”的特点，以及基于目标样本在特征空间的分布具有相似性，而非目标样本则相对较分散的特点。该方法只建立目标样本的数据分布模型，用正常状态的样本信息作为目标样本的惟一检测信息，并找到此类样本的支撑点描述区域，对于未知样本的检测即看此样本是否在目标样本的描述区域内，即用用正常状态的模型库来检测未知样本是否异常。支持向量的区域描述（Support Vector DomainDescription，SVDD）就是构造一个半径最小超球，在超球的这个高维空间里只包含目标样本，拒绝其他样本进入球内。球表面上的样本点即为支持向量。其模型数学描述为：",
    "context10": "设训练样本集 $\\Omega { = } \\{ \\boldsymbol { \\mathbf { \\Phi } } _ { x _ { i } } \\} , \\ i { = } 1 , 2 , \\ldots , \\ i , \\ \\boldsymbol { \\mathbf { \\Phi } } _ { x _ { i } } \\in R ^ { n } ,$ 向高维映射的非线性线性关系为 $\\Phi$ 。超球的中心用 $\\alpha$ 表示，球体半径用 $R$ 表示。同样在建立模型时，为每个样本加以松弛变量 $\\mathfrak { s } _ { i } \\geq 0$ 以及惩罚因子 $C$ ，以消除噪声对分类结果的影响。 $\\boldsymbol { \\mathsf { \\xi } } _ { i }$ 定义为：",
    "context11": "$$\n\\boldsymbol { \\mathfrak { f } } _ { i } = ( \\parallel \\Phi ( \\boldsymbol { x } _ { i } ) - \\alpha \\parallel ^ { 2 } - \\boldsymbol { R } ^ { 2 } ) +\n$$",
    "context12": "当样本点落在超球体的内部，其值为0；当落在外部，它表示样本点到球心 $a$ 的距离的平方超过 $R ^ { 2 }$ 的程度（离超球体的平方距离)。",
    "context13": "超球体满足如下关系式：",
    "context14": "$$\n\\{ \\begin{array} { c } { \\operatorname* { m i n } \\cal { R } ^ { 2 } + \\cal { C } \\sum _ { i = 1 } ^ { l } \\tilde { \\xi } _ { i } } \\\\ { \\quad  \\frac { \\mathrm { ~ } } { \\mathrm { ~ } } | \\Phi ( x _ { i } ) - a \\| ^ { 2 } \\leq \\mathrm { R } ^ { 2 } + \\bar { \\xi } _ { i } ^ { ~ }  } \\end{array} , ~ \\xi _ { i } \\geq 0\n$$",
    "context15": "根据对偶理论[21]，原始问题求解转化为对偶问题求解：",
    "context16": "$$\n\\left\\{ \\begin{array} { l l } { \\operatorname* { m a x } L ( \\mathbf { \\Theta } \\alpha ) = \\sum _ { i } \\alpha _ { i } k ( x _ { i } , x _ { i } ) - \\sum _ { i , j } \\alpha _ { i } \\alpha _ { j } k ( x _ { i } , x _ { i } ) } \\\\ { \\qquad s \\cdot t \\cdot \\sum _ { i } \\alpha _ { i } = 1 , \\ 0 \\leq \\alpha _ { i } \\leq \\mathbf { C } } \\end{array} \\right.\n$$",
    "context17": "若此二次优化问题 $\\alpha$ 的最优解为 $\\alpha ^ { * }$ ，则所要求的超球体即可解出来。其中，球心由样本点的线性组合构成 $\\alpha ^ { * }$ $\\begin{array} { r } { = \\sum _ { i = 1 } ^ { l } \\alpha _ { i } ^ { * } \\Phi ( x _ { i } ) } \\end{array}$ ，球半径 $R$ 为球面上的支持向量 $x _ { k }$ 到球心 $\\alpha$ 的距离：",
    "context18": "$$\n\\begin{array} { c } { { R ^ { 2 } = \\| \\Phi ( x _ { k } ) - \\alpha \\| ^ { 2 } = k ( x _ { k } , x _ { k } ) - 2 \\displaystyle \\sum _ { i } \\alpha _ { i } k ( x _ { k } , x _ { i } ) + } } \\\\ { { \\sum _ { i , j } \\alpha _ { i } \\alpha _ { j } k ( x _ { i } , x _ { j } ) } } \\end{array}\n$$",
    "context19": "对于未知样本 $z$ ，其到球心的距离为：",
    "context20": "$$\n\\begin{array} { r c l l r } { { \\parallel \\Phi _ { ( z ) } - { } _ { a } \\parallel ^ { 2 } } } & { { = } } & { { k ( z , z ) - 2 \\alpha \\displaystyle \\sum _ { i } \\alpha _ { i } k ( z , x _ { i } ) + } } & { { } } & { { } } \\\\ { { \\sum _ { i , j } \\alpha _ { i } \\alpha _ { j } k ( { } _ { x _ { i } } , x _ { j } ) } } & { { } } & { { } } & { { ( 3 5 ) } } \\end{array}\n$$",
    "context21": "因此可定义判别函数为：",
    "context22": "$$\nf _ { S V D D } = \\operatorname { s g n } ( R ^ { 2 } - \\parallel \\Phi _ { \\left( z \\right) } - \\alpha \\parallel ^ { 2 } ) = \\operatorname { s g n } ( A ) = \\left\\{ \\begin{array} { l l } { 1 } & { A \\geq 0 } \\\\ { 0 } & { A < 0 } \\end{array} \\right.\n$$",
    "context23": "若函数 $f _ { S V D D }$ 输出0，表明测试样本落在超球的外部，从而 $z$ 是异常数据。",
    "context24": "单值 SVM没有用到训练样本类别的信息，隶属于无监督学习；不同于经典的无监督学习方法，单值SVM无需求出所有样本点的密度估计；SVDD方法中不同的核函数导致对应输入空间不同的描述边界，该学习机的分类效果受核函数及其参数的影响。"
  },
  "3大数据在线识别处理的多核学习方法": "",
  "3.1大数据环境": {
    "context1": "当今世界每天所产生的数据浩瀚无比，随着软硬件以及信息技术的飞速发展，信息无论在流量、种类、速度还是活力上都是爆炸式增长。据估算，现在每两天全球产生的数据就相当于从人类文明起源至2003年间全部数据的总和，而新的数据还在以每天 $2 . 5 \\mathrm { E B }$ （约10.7亿GB）的量级高速增长[22]。",
    "context2": "大数据（Bigdata）通常用来形容大量非结构化和半结构化数据，而目前业内流行的一般数据挖掘方法和通用商业数据库无法满足大数据时代的挑战。这些海量、高增长率和多样化的信息资产需要新的处理模式才能挖掘出它无形的价值。",
    "context3": "大数据技术的意义不在于掌握庞大的数据信息，而在于对这些含有意义的数据进行专业化处理。能够在不同的数据类型中进行交叉分析的技术，是大数据的核心技术之一。语义分析技术、图文转换技术、模式识别技术、地理信息技术等等，都在大数据分析时获得应用[23]。"
  },
  "3.2网络实时大数据处理需求": {
    "context1": "大众传媒如此发达的今天，信息浪潮以小众、体验化为特征，以微博、Facebook为代表的网络信息传播形态恰恰把“小众”们组织起来，借助“社会认同”效应迅速放大他们的影响。这些新兴的传播形态颠覆了传统的信息传播路径，使传统的单中心、单向的传播方式，向多中心、网状裂变传播方式转变[24]，具有颠覆性的力量。要对网络信息的传播进行控制以及对不良信息的传播进行实时发现和预警，这对大数据环境下网络实时数据处理提出了更高的要求。",
    "context2": "网络信息的实时变化，即模式识别中的样本总是在变动的，固定的一次性训练不能解决模式识别中问题的不确定性。对于这种现状，利用随时间滚动的新增样本来不断修正分类器的正确性，称作增量学习或在线学习。增量学习的特点表现在：不需要保存所有的历史数据，减少了内存空间；充分利用了历史数据的信息的同时缩短了学习的时间；具有随系统变化而变化的自学习能力；增量学习的过程中，训练样本为满足新的KKT条件，对参数进行调整，这样非支持向量可能变为边界支持向量，边界支持向量可能变为非边界支持向量。"
  },
  "3.3多核学习解决方案": {
    "context1": "核方法是解决非线性模式识别问题的一种数据处理技术，但在一些复杂情形下，尤其是在大数据环境下，数据表现出量大、多样、实时的特点，由单个核函数构成的核机器已不能满足诸如数据异构或不规则、样本规模巨大、样本分布不均匀等实际的应用需求，因此将多个核函数进行组合，以获得更好的结果是一种必然选择[25]。",
    "context2": "设 $k 1$ 和 $k 2$ 是定义在 $X \\times X$ 上的核， $X { \\subseteq } R ^ { n }$ ， $\\alpha { \\subseteq } { R } ^ { + }$ ，$f ( \\cdot )$ 是 $X$ 上的一个实值函数， $\\phi$ ： $X ^ {  R ^ { N } }$ ， $k _ { 3 }$ 是定义在 $R ^ { N }$ $\\times R ^ { N }$ 上的核， $B$ 是一个 的半正定对称矩阵，则下面的函数都是核：",
    "context3": "(1) $k ( \\mathbf { \\boldsymbol { \\mathbf { \\rho } } } _ { x } , z ) = k _ { 1 } ( \\mathbf { \\boldsymbol { \\mathbf { \\rho } } } _ { x } , z ) + k _ { 2 } ( \\mathbf { \\boldsymbol { \\mathbf { \\rho } } } _ { x } , z )$   \n(2) $k ( \\boldsymbol { x } , z ) = \\alpha k _ { 1 } ( \\boldsymbol { x } , z )$   \n(3) $k ( \\boldsymbol { x } , z ) = k _ { 1 } ( \\boldsymbol { x } , z ) k _ { 2 } ( \\boldsymbol { x } , z )$   \n(4) $k ( \\mathbf { \\boldsymbol { x } } , z ) { = } f ( \\mathbf { \\boldsymbol { x } } ) f ( z )$   \n(5) $k ( \\mathbf { \\Phi } _ { x } , z ) = k _ { 3 } ( \\mathbf { \\Phi } \\phi ( \\mathbf { \\Phi } _ { x } ) , \\Phi ( z ) )$   \n(6) $k ( \\mathbf { \\boldsymbol { \\mathbf { \\mathit { x } } } } , z ) { = } f ( \\mathbf { \\boldsymbol { \\mathbf { \\mathit { x } } } } ) k _ { 1 } ( \\mathbf { \\boldsymbol { \\mathbf { \\mathit { x } } } } , z ) f ( z )$   \n（7） $k ( \\boldsymbol { x } , z ) = \\boldsymbol { x } ^ { T } B z$",
    "context4": "由上面的式子可以看出，用简单的核可构造出复杂的核。在核算法中使用不同的核比单核更能提高分类性能。如何运用简单的核构造复杂的核，要根据实际问题给不同的简单核施予不同的权重。",
    "context5": "线形多核学习（MKL）就基于此思想[26]：若有数据集$\\{ ( x _ { i } , y _ { i } ) , \\acute { \\imath } = 1 , 2 , \\cdots , n , \\acute { y } _ { i } \\in \\{ 1 , - 1 \\} \\}$ ，设 $x _ { i }$ 对应的核集为 $\\left\\{ \\mathbf { \\Phi } _ { K _ { m } } \\right\\}$ （ $\\mathbf { \\Phi } _ { m } = 1 , \\cdots , M $ )，每个核对应一个希尔伯特空间$F _ { m }$ 。则经过多核的线性组合后，新核为：",
    "context6": "$$\n\\begin{array} { r } { K ( \\mathbf { \\Phi } _ { x _ { i } } , \\mathbf { \\Phi } _ { x _ { j } } ) = \\sum _ { m = 1 } ^ { M } \\sigma _ { m } K _ { m } ( \\mathbf { \\Phi } _ { x _ { i } } , \\mathbf { \\Phi } _ { x _ { j } } ) } \\end{array}\n$$",
    "context7": "式中 $M$ 是核矩阵的数目， $\\{ \\sigma _ { m } \\} _ { m = 1 } ^ { M } ( \\ \\sigma _ { m } { \\ge } 0 )$ 是要学习的凸组合约束的系数。",
    "context8": "文献[26]的仿真实验证明了线性多核学习SVM不会差于单核学习的SVM，若考虑不同核间的相互作用，用非线性多核组合（NCMK）的方法来提取不同核间相互作用产生的信息，会得到更好的分类性能。文献[27］从合成核、多尺度核、无限核3个角度，系统地综述了多核方法的构造理论，并给出了多核学习的进一步研究方向。多核学习在一定程度上解决了核函数的选择和构造问题，提高了分类性能。此外，超核函数由常见核进行多项式组合而成，具有平移不变性和旋转不变性双重性质，能更好地适应不同问题的需要。多核方法为网络大数据的实时处理提供了一个解决方法。"
  },
  "4总结与展望": {
    "context1": "本文对模式识别中最常见的核方法进行了讨论，从模式识别的传统方法到基于核的方法，并详细介绍了模式识别中核方法应用，特别是基于核函数的支持向量机算法。正是SVM这种基于严格的统计学习理论和数学分析的算法，使核方法成为机器学习的主流算法，并推动了核方法研究与应用的热潮。对于核方法的关键部分，核函数与核参数的选择问题上本文也通过一定的文献分析进行了研究和总结。基于核方法的模式识别会在解决智能化、复杂化问题上发挥效用。",
    "context2": "今后核方法的研究主要会在降低核方法计算的复杂度，核参数的优化，以及多核学习3个方面展开。面对当前的大数据环境，多核学习和在线式学习将是研究的发展方向。如何将核方法中的各种技巧结合起来，合理的运用到具体的问题中，特别是对大数据环境下的网络信息的分析，网络信息情报模式识别的应用，如网络舆情事件的实时分析，这将是笔者进一步研究的课题。"
  },
  "参考文献": {
    "context1": "[1]刘豫均，隔淑芳．移动学习—国外研究现状之综述[J]．现代教育技术，2004，(3)：12—16.  \n[2]张建伟，孙燕青．建构性学习—学习科学的整合性探索[M]．上海：上海教育出版社，2005.  \n[3]武妹婷．移动学习工具含义和种类的探讨[J]·老区建设，2009，(10)：59—60.  \n[4]方海光，吴淑苹．基于EML构建移动学习资源对象单元的研究[J]．现代教育技术，2009，（12)：75-79.  \n[5]陈明明．移动学习资源的创设与评价探究[D]．浙江师范大学.  \n[6]ScottL，Howell．影响远程教育发展的32种趋势[J]．远程教育，2005，(1)：27-32.  \n[7]武妹婷．大学生移动学习现状研究[D]．江西：南昌大学，2009.  \n[8]叶成林，徐福荫，等．移动学习研究综述[J]．电化教育研究，2004，（3)：12—19.  \n[9]庞维国．自主学习—学与教的原理和策略[M]．上海：华东师范大学出版社，2004.",
    "context2": "（本文责任编辑：孙国雷）"
  },
  "（上接第161页）": {
    "context1": "络，加强移动学习平台的建设。在平台中应包括教学系统、终端服务器、无线通讯等，通过相互的协调、控制，可以建立移动学习的学习平台，充分利用互联网络的资源，实现移动学习，并被广大群众所利用。然而，如何开展好移动学习，其核心就是对移动学习平台的建设。而移动学习平台要实现以下几方面功能：一是操作简单，保证学习者很方便地获取平台里的各种学习资料；二是交流顺畅，能使学习者方便通畅自如地进行相互交流；三是保障机制，平台要搭建一套有效的学习方案、评价机制；四是分类清晰，即不同的人群通过平台能有不同的获取资源的方式，以适应不同的经济承受能力。"
  },
  "5.5加大移动学习管理力度": {
    "context1": "对移动学习进行时间管理最根本的目的是对移动学习进行自我监控[9]。首先，大学生们应该对自己的学习有一个系统的规划与设计，让学生们充分的清楚和利用自己的零散时间，计划出在零散的时间内是否可以进行移动学习。这样就能使大学生更有效地利用零散的时间，从而达到提高学习效率的目的。由于移动学习还受外界环境的影响，不同的环境会有不同的影响，十分容易造成对移动学习的间断性，所以，大学生在学习的过程中还要及时做好学习笔记，即对学习内容有条理的做记号，充分发挥移动学习的作用，使其达到预期的效果。",
    "context2": "法[J]．计算机应用研究，2007，24（2)：86—88.  \n[19]王自强，段爱玲，张德贤．基于支持向量数据描述的高效异常数据检测算法［J]．吉林大学学报：工学版，2009，39(2)：499-503.  \n[20]李盼池，许少华．支持向量机在模式识别中的核函数特性分析[J]．计算机工程与设计，2005，26（2)：302—304.  \n[21]黄啸·支持向量机核函数的研究[D]．苏州：苏州大学，2008.  \n[22]李国杰，程学旗．大数据研究：未来科技及经济社会发展的重大战略领域——大数据的研究现状与科学思考[J]·中国科学院院刊，2012，27（6)：647—657.  \n[23]孟小峰，慈祥．大数据管理：概念，技术与挑战[J]．计算机研究与发展，2013，50（1)：146—169.  \n[24]吴金红，张飞，鞠秀芳．大数据：企业竞争情报的机遇，挑战及对策研究[J]．情报杂志，2013，32（1)：5—9.  \n[25]汪洪桥，孙富春，蔡艳宁，等．多核学习方法[J]．自动化学报，2010，36(8)：1037—1050.  \n[26]李晋博．特征提取的核方法与非线性多核学习的研究[D].上海：华东师范大学，2009.  \n[27]贾磊，廖士中．超核函数支持向量机[J]．计算机科学，2008，35(12)：148—155，166.",
    "context3": "（本文责任编辑：孙国雷）",
    "context4": "移动学习的出现，必将会引起教育界新的浪潮，也将会成为未来社会学习方式的主流。随着互联网络的更新，便携式移动设备将会越来越先进，移动理念也将会越来越稳定与成熟，学习者也将会更加方便的获取知识，提高学习效率。"
  }
}