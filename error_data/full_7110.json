{
  "original_filename": "full_7110.md",
  "文本神经语义解析方法研究进展": "",
  "沈凌云²乐小虬1,2": {
    "context1": "1（中国科学院文献情报中心北京 100190)  \n2（中国科学院大学经济与管理学院信息资源管理系北京100190)",
    "context2": "摘要：【目的】对近10年利用神经网络研究文本语义解析的方法进行归纳和评述。【文献范围】以谷歌学术和中国知网为检索平台,分别以\"Neural Semantic Parsing\"和\"神经语义解析\"为关键词,筛选 2010年-2022年的相关文献及其重要引文进行分析【方法】对神经语义解析方法按照技术路径进行分类,剖析各技术路径的基本思路,对比分析各技术方法在数据、性能、应用目标等方面的异同点,归纳文本神经语义解析技术存在的问题及发展趋势。【结果】将现有神经语义解析方法归纳为序列到序列、借助中间形式以及语义单元分解与组合三种类型,后两种方法是对第一种方法的改进。中间表示形式,如语义草图、规范话语和少样本神经语义解析,是当前研究的主要关注点。【局限】主要从方法论上对现有研究思路进行归纳分析,对于神经语义解析模型内部实现机理未做细致阐述。【结论】目前神经语义解析方法在文本语义解析中能够获得最佳性能，面向具体应用设计针对性的神经网络模型是当前主流做法,但语义解析效果与实际应用仍然有一定差距。",
    "context3": "关键词：语义解析神经网络模型语义表示预训练分类号：G350DOI:10.11925/infotech.2096-3467.2022.1074",
    "context4": "引用本文：沈凌云，乐小虬.文本神经语义解析方法研究进展[J].数据分析与知识发现,2023，7(12)：1-21.(Shen Lingyun, Le Xiaoqiu. Review of Text Neural Semantic Parsing Methods[J]. Data Analysis and KnowledgeDiscovery,2023,7(12): 1-21.)"
  },
  "1引言": {
    "context1": "语义解析是机器理解自然语言(NatureLanguage,NL)文本的关键路径之一，它能建立一种自然语言文本到某种结构化逻辑形式或意义表示的映射[1],从而达到理解文本内在含义的目的。语义解析作为自然语言理解领域的重要研究方向，是智能检索、问答系统、知识抽取、自动综述等应用的基础，其性能的提升能够有效提升这类应用的智能化水平。",
    "context2": "语义解析技术的发展大致经历三个阶段[2]：20世纪60年代，主要以规则和模式匹配的方法进行语义解析;20世纪90年代末到21世纪初，以统计机器学习为基础的各类统计模型成为语义解析的主流方法；近年,随着深度神经网络模型性能提升,神经语义解析进入新阶段。从已有研究结果来看，传统的基于规则匹配、统计机器学习的语义解析方法虽然在解决一些具体问题上具有独特的优势,但从整体上神经语义解析已逐步成为主流研究方法。",
    "context3": "神经语义解析通过构造深度学习神经网络，利用深度学习技术学习自然语言文本特征并预测对应的结构化语义表示，通过训练神经网络模型学习自然语言与逻辑形式之间的映射关系，实现从自然语言到逻辑形式的转换。这种语义解析方法在多数情况下能达到超越传统统计机器学习模型的性能水平,大幅提升语义解析的准确率,并在许多自然语言应用领域,如数据库查询[3]、数学解算、代码生成[4]等方面，取得了当前最好结果,显示出巨大的研究价值和应用潜力。",
    "context4": "本文利用谷歌学术(Google Scholar)和中国知网(CNKI)作为文献检索平台，检索2010年-2022年“Neural Semantic Parsing\"和\"神经语义解析\"相关文献,并对这些文献进行整理分析,筛选出重要的、有代表性的研究。对文本神经语义解析方法进行了归纳分析,梳理了神经语义解析的内涵和常用模型结构以及常用的语义表示形式,剖析各类神经语义解析方法的基本思想和技术线路,对比分析各技术方法在数据、性能、应用目标等方面的异同点,归纳了文本神经语义解析技术发展趋势及现存问题。"
  },
  "2神经语义解析与语义表示形式": {
    "context1": "为了使计算机能够理解自然语言,需要借助具有逻辑结构的形式作为语义表示的载体。这种表示载体通常遵循某种语法或规则集合，约束语义表示形成具有统一分布特征和逻辑结构的形式,称为逻辑形式或意义表示。语义表示形式是语义解析的重要组成部分。"
  },
  "2.1神经语义解析的内涵": {
    "context1": "语义解析(Semantic Parsing)是将文本转换为形式化的意义表示或逻辑形式的自然语言处理任务[5]。神经语义解析是指利用神经网络模型实现语义解析[的技术，以神经网络模型构成的语义解析器为核心,完成从自然语言到某种逻辑形式表示的转换任务。"
  },
  "2.2基本构成": {
    "context1": "神经语义解析的基本构成包括：输入自然语言文本、输出逻辑形式以及由神经网络构成的神经语义解析器。一个基本的神经语义解析过程如图1所示。神经语义解析器以编码器-解码器架构的模型为主流,其基本组成包括编码器模块、解码器模块或预测模块,编码器负责将自然语言转换为在向量空间上连续的向量表示,解码器、预测模块负责根据向量表示输出正确的逻辑形式。本节将分别就输入文本的向量表示、神经语义中的网络模型和语义表示形式作进一步阐述。",
    "context2": "![](images/d099ba90afc6bd86fcba034ca7825b38167675fd3f8d13218ee8abd3b67eff06.jpg)  \n图1神经语义解析的基本组成  \nFig.1Basic Components of Neural Semantic Parsing"
  },
  "（1）输入文本的向量表示": {
    "context1": "神经网络模型在自然语言处理(NatureLanguageProcessing,NLP)领域获得成功的关键是将自然语言表示为一定维度的连续向量（又称为分布式表示或嵌入表示）,隐式地表示语言的句法或语义特征。其主要思路是通过训练语言模型[7或使用神经网络模型将语义单元(如单词、句子、段落、篇章等)的语义映射到向量空间中[8]。目前关于词的向量表示的模型研究最多,如Word2Vec[9]、GloVe[10]ELMo[11]、OpenAI GPT[12]、BERT[13]、RoBERTa[14]BART[15]等。关于句子、段落、篇章的向量化表示研究也在积极开展,如段落向量[16]、文档向量[17]、句子嵌入[18] 预训练文档向量生成[19]等,这些方法大大改善了神经网络模型处理语义的效果。使用自然语言的向量表示作为神经网络模型的输入已被深度学习自然语言处理领域普遍接受并广泛应用于语义解析",
    "context2": "任务中。"
  },
  "（2）神经语义中的网络模型": {
    "context1": "神经语义解析所用的基本网络模型与神经网络发展几乎是同步的。随着新的模型不断被开发并应用于NLP领域,神经语义解析领域的研究者也不断将这些模型应用于语义解析任务。语义解析任务中使用的基本模型有循环神经网络(RecurrentNeuralNetwork，RNN）及其变体、卷积神经网络（Convolutional Neural Network,CNN）、指针网络[20]（Point Network）、Transformers 架构模型和基于Transformers架构的预训练模型，如BERT[13]BART[15]、RoBERTa[14]、GPT[12]、GPT-2[21]、GPT-3[2]$\\mathrm { T } 5 ^ { [ 2 3 ] }$ 等,都受到神经语义解析领域的关注。",
    "context2": "早期,前馈神经语言模型[7和对数线性语言模型[24]在NLP任务上的成功,引起语义解析任务研究者对神经网络的关注。为探索深层网络结构在语义解析任务上的性能,Grefenstette等[25]提出深层神经网络体系结构，利用由对数双线性模型构造的条件神经语言模型学习自然语言与数据库查询之间的联合表示，以及这种联合表示与数据库查询之间的映射,试图解决传统文本语义解析建立在语法分析和统计模型上的问题。孔令富等[26使用机器学习和神经网络混合模型进行自然语言到指令的语义解析。此时，神经网络只是作为语义解析系统中的一部分。",
    "context3": "随着RNN在NLP领域大获成功，使用RNN进行语义解析任务的方法得到关注。RNN模型擅长处理长度无法确定的序列任务，通过顺序时间步的联系捕获上下文信息并建立前后的依赖关系，使得该模型能较好地胜任语义解析任务，基于RNN模型的神经语义解析方法的解析性能逐渐超过以往基于规则匹配和统计机器学习方法的结果[8.27-28]。但这种时序的模型结构效率低下。",
    "context4": "RNN基于时序处理的特征无法避免并行性差的问题(模型必须在处理完前一个时间步后，才能处理下一个时间步),并且随着时间步的增加无法避免梯度消失和爆炸的问题,导致长距离依赖信息丢失，于是人们利用CNN能并行处理的特点弥补RNN的缺陷。一直以来,使用卷积神经网络的语义解析模型与RNN一样受到关注。Yih等[29]将自然语句中的关系、实体映射到目标形式中,使用CNN训练语义解析模型,将单词投影到带有上下文信息的特征向量上，利用CNN中的最大池计算显著的局部特征，从而形成全局特征的向量表示输出，最后通过计算与逻辑模式的相似分数确定与输入对应的逻辑形式。Babu等[30]充分利用CNN解码的并行优势,使用改进的CNN解码模块解决了RNN解码无法并行的问题,在编码器模块和解码器模块各添加多层深度卷积层，先预测目标序列长度，再通过解码器逐个预测目标序列字符。相比于基线的RNN序列到序列方法,CNN模型预测速度提高超过 $80 \\%$ 。卷积神经网络虽然加快了运行速度，并且在卷积核的作用下更好地利用了局部特征，但由于缺乏像RNN那样前后时间步信息的传递,不能根据上下文信息调整语义表示，通用性和学习能力仍然有很大局限。",
    "context5": "为了得到结构更加良好的逻辑形式并适用于特定领域,结合RNN与CNN优势,Vinyals等[20]提出一种挑战RNN模型的新架构——指针网络。传统的RNN模型总是需要预设输出序列的词汇表,因此模型的预测能力高度依赖训练数据。然而，随着输入序列的增长,不可能一直扩大词汇表,因此Vinyals等[20]给出了解决方案,利用指针的方式选择性地直接复制部分输入字符到输出序列，大大减少了随着输入长度的增加带来的预测能力下降问题。Ling等[31]提出基于指针网络的语义解析模型,解决RNN模型的单词稀疏问题,用于将自然语言和非自然的结构化语言转换为程序设计语言,这种方法生成的代码在语义上更加准确。Zhong等[32]同样受Vinyals等[20]的启发,将指针网络用于自然语言转换为结构化查询语言（Structured Query Language,SQL)的解析任务中,在大型数据库上的实验证明,基于指针网络生成的代码准确度远超基线RNN模型。",
    "context6": "后来，Vaswani等[33]提出Transformer的序列编码解码架构，改进了基于RNN一类的编码器-解码器架构多次重复的递归操作,使用自注意机制(Self-Attention)替代递归过程。基于Transformer架构[33]的预训练模型能够产生良好的通用单词表示，使得文本向量表示不必像循环神经网络、卷积神经网络和指针网络那样从头学起,从而专注于语义解析等下游任务。因此，预训练模型逐渐替代了RNN、CNN的编码器功能。Hwang等[34]在2019年率先提出将 BERT（Bidirectional Encoder RepresentationsfromTransformers)加入编码表示层以改善语义解析效果，在一个两层双向长短期记忆网络(Bidirectional Long Short-Term Memory，BiLSTM)编码层之前加入BERT,先将自然语言输入和表结构信息一起送入BERT以获得解码器所需的嵌入表示。该方法在解析准确度上第一次超过人类水平。Rongali等[35]在基于指针网络的语义解析模型中直接将BERT作为编码器进行实验,将加入BERT的语义解析器与另一个BERT的变体RoBERTa[14]作为编码器和未加入预训练模型的语义解析器比较，发现加入BERT后准确度提高 $3 . 3 \\%$ ,加入RoBERTa后准确度提高 $7 . 4 \\%$ 。这证明了预训练模型对于提高语义解析性能是显著的。Lyu等[36]在Hwang等[34的基础上直接将编码器替换为单独的BERT模型,他们认为Hwang等[34]设计的编码器过于复杂,堆叠多层的长短期记忆网络（Long Short-Term Memory,LSTM)单元更容易造成信息丢失和错误,实验证明这一方法是可行的。"
  },
  "（3）语义表示": {
    "context1": "语义表示是使用某种结构化形式对语言所传达的意义显式表示，是自然语言语义解析的最终产物[37]。将语义转换为逻辑形式能充分描述语义并能使计算机可理解,在不同的语义解析任务场景中一般会使用不同的形式表示语义，包括逻辑表达式、图的表示、框架语义网络(FrameNet)、数学表达式、程序设计语言等形式,其中逻辑表达式、语义图、框架语义网络是通用的语义表示形式。",
    "context2": "$\\textcircled{1}$ 逻辑表达式",
    "context3": "逻辑表达式是一种使用逻辑连接符将语义直观表述出来的逻辑语言表示形式,主要通过构造句子句义为真时的逻辑条件描述句子语义。Cresswel[38]在20世纪70年代将逻辑语言引入自然语言处理中，促进了一阶逻辑在内的各种逻辑形式,如Lambda演算[39]（λ-Calculus）、马尔可夫逻辑[40]（Markov Logic）、谓词逻辑[41](PredicateLogic)等在自然语言解析领域的应用。一阶逻辑(First OrderLogic,FOL)是这类形式的标准语言[42],Lambda演算式(λ-CalculusExpressions)是基于一阶逻辑的扩展,也是语义解析任务中最常用的逻辑表达形式[42-44]。以下是Liang[45]使用Lambda演算表示句子语义的示例。",
    "context4": "句子:people who have lived in Seattle",
    "context5": "逻辑形式（Lambda 演算）：x.e.PlacesLived$( x , e ) \\wedge$ Location(e, Seattle)",
    "context6": "逻辑表达式具有完整的语法系统，也可以与其他语义表示理论结合，具有很好的普适性[46],在知识库查询、问答、语义表示方面有很大应用价值。",
    "context7": "$\\textcircled{2}$ 基于图的表示",
    "context8": "图是另一种通用语义表示形式，又可称为语义图或语义网。它用带标签的节点表示自然语句中具有实体、事件、概念、属性、状态等含义的词或词组，用边表示节点之间的关系。基于图的表示形式还可进一步分为有向图、树状图、网络图等。基于图的语义表示直观地将语义的层次结构展示出来，抽象意义表示（Abstract Meaning Representation，AMR）是图表示语言的典型代表。",
    "context9": "AMR实现了句法和语义逻辑的分离。如何构建AMR语义图是这类语义解析方法的关键,Wang等[47]使用两步转换框架实现NL到AMR表示的转换,先使用句法依赖构造依赖关系树,再设计一个转换算法将依赖关系树转换为AMR图。这种方法本质上仍然依赖于句法解析。早期的神经AMR转换方法大多使用统计机器学习模型和神经网络模型结合的方法，Flanigan等[48]联合使用半马尔可夫模型(Semi-MarkovModel)和最大生成连通子图算法(Maximum Spanning Connected Subgraph，MSCG)构建自动解析自然语句为AMR图的框架。Konstas等[49]使用RNN模型将NL转换为AMR语言进行语义表示，不仅取得了与人工标注相当的结果，还弥补了许多人工标注容易产生的错误。",
    "context10": "除了AMR,其他基于图的表示语言的语义解析研究也在同时开展。Hershcovich等[50]提出基于普适概念认知标注（Universal Conceptual CognitiveAnnotation,UCCA)表示框架,使用线性模型和RNN模型分类器构造UCCA图，在跨语言适应性和语义评价准确度上优于AMR表示。Shi等[5]设计了一种树结构DOL(DolphinLanguage)表示描述数学问题的无结构文本，从节点和关系中推导问题答案。",
    "context11": "$\\textcircled{3}$ 框架语义网络",
    "context12": "框架语义网络(FrameNet)[52]是在框架语义学[53]"
  },
  "4 数据分析与知识发现": {
    "context1": "理论基础上建立的一个包含大量词汇和句子语义结构信息的知识库，目前FrameNet团队已创建了1000多个语义框架[54]。这一理论认为自然语言的语义能够通过语义框架(事件类型、关系或实体以及其中的参与者)得到最好的理解。受益于各种语义类型属性的清晰定义,近年来基于框架语义的语义解析成为语义理解的一个重要方向。",
    "context2": "Gupta等[55]和Chen等[56]相继创建了框架语义解析的基准数据集TOP及其扩展版本TOPv2,并且Chen等[56]利用TOPv2数据集,对基于Transformers的预训练模型BART进行自适应的模型训练和微调,在少量数据下得到非常好的解析结果。但这两个数据集局限于单意图的语义框架的标记。Desai等[57]分析了使用Transformers架构进行框架语义转换时出现的错误，发现句法结构是模型预测时最常发生的错误。此外,基于RNN模型的序列到序列方法也面临低并发、高延迟的问题。为了解决这些问题,Babu等[30]提出改进的基于卷积神经网络编码-解码的方法，使解码器并行输出，很大程度上提高了预测效率。",
    "context3": "基于图的表示和框架语义网络都试图弥补单一的一阶逻辑无法适用于复杂语言现象[58]的问题，虽然抽象意义表示[59]、普适概念认知标注[60]、通用分解语义[61]等在描述语义能力上更强，但这些表示形式在跨语言表示和通用性上仍有不足，目前尚没有一种完全普适的语义表示形式[62]。"
  },
  "（4）不同场景中的语义表示": {
    "context1": "不同场景中的语义解析任务，需要依赖不同的语义表示形式使计算机解决相应的问题。无论是传统语义解析还是神经语义解析,都需要依赖某种形式的语义表示完成特定场景的问题解决，如求解数学问题需要数学方程式、数据库查询需要查询语句、机器人需要结构化命令等。不同的语义解析任务场景一般会使用不同的形式表示语义,如逻辑表达式、图的表示、框架语义网络(FrameNet）、数学表达式、程序设计语言、向量表示等形式。合理的语义表示形式是完成不同场景的语义解析任务的关键要素。几种具体任务场景用到的语义表示形式如表1所示。如前所述,不少语义表示形式被相继提出,但这些语义表示方案都有其各自优势和不足。",
    "context2": "表1不同语义解析任务场景需要的语义表示形式示例  \nTable1Semantic Representations Required for Different Semantic Resolution Task",
    "context3": "<table><tr><td>场景</td><td>自然语言语句</td><td>语义形式化表示</td></tr><tr><td>数学问题解算[8]</td><td>Dan have 2 pens，Jessica have 4 pens.How many pens do they have in total ?</td><td>x = 4+2</td></tr><tr><td>逻辑查询[5]</td><td>what microsoft jobs do not require a bscs?</td><td>answer(J,（company(J,&#x27;microsoft&#x27;）,job(J）,not((req_deg (J,&#x27;bscs&#x27;)))))</td></tr><tr><td>问答[63]</td><td>who has published the most articles?</td><td>argmax(type.person;R(x:count(type.article u author:x))) public void add(final double argO）{</td></tr><tr><td>程序生成[4]</td><td>Adds a scalar to this vector in place</td><td>for(inti=0;i&lt;vecElements.length()；i++){ vecElements[i] += arg0;}}</td></tr><tr><td>机器人指令生成[64]</td><td>Go away from the lamp to the intersection of the red Turn(), brick and wood</td><td>Travel(steps:1)</td></tr></table>"
  },
  "3神经语义解析方法": {
    "context1": "神经语义解析是指利用神经网络模型实现语义解析任务。本文通过分析现有神经语义解析研究，按照实现语义解析目标的过程和关键技术的共通特点,将神经语义解析方法分为三类：一是序列到序列的方法;二是借助中间形式的方法;三是基于语义单元分解与组合的方法。三类语义解析方法如图2",
    "context2": "所示。"
  },
  "3.1序列到序列的方法": {
    "context1": "序列到序列的神经语义解析方法的基本思路是通过学习自然语句和对应的逻辑形式之间的映射，将文本序列输入神经网络模型,通过编码、解码，预测目标表示序列。这种序列到序列的架构已在神经机器翻译领域获得巨大成功，由于语义解析与机器翻译任务有许多相似之处，因此序列到序列的神经架构方法也受到语义解析领域的广泛关注,因其良好的性能成为目前神经语义解析的主流方法。",
    "context2": "![](images/ec1ff77fffec46000213409941a4b3ff32bde7d905228dc6888e0247e5056e93.jpg)  \n图2不同神经语义解析方法流程  \nFig.2Process of Different Neural Semantic Analysis Methods",
    "context3": "（1）基于编码器-解码器的序列到序列的方法",
    "context4": "输入是文本序列，输出是逻辑形式序列的语义解析方法称为序列到序列（Sequence to Sequence,Seq2Seq)的方法，神经机器翻译和语义解析是应用这种方法的典型代表任务。",
    "context5": "受神经机器翻译成功的影响，基于编码器-解码器架构的序列到序列( $\\mathrm { S e q 2 S e q }$ )范式的语义解析方法被广泛关注。循环神经网络(RNN)在处理文本上的优势,使其成为这类方法主要使用的模型。模型的训练目标是通过学习输入自然语言与对应逻辑形式之间的映射，获得在输入序列条件下目标形式的概率模型，如公式(1)所示。",
    "context6": "$$\np ( a | q ) = \\prod _ { t = 1 } ^ { | a | } p ( y _ { t } | y _ { < t } , q )\n$$",
    "context7": "其中, $a = y _ { 1 } , y _ { 2 } , \\cdots , y _ { | a | }$ 表示逻辑形式序列; $q =$ $x _ { 1 } , x _ { 2 } , \\cdots , x _ { | q | }$ 表示自然语言文本序列。",
    "context8": "序列到序列的模型学习方法由 Sutskever等[65]在2014年首次提出，在跨语言的翻译任务上的表现超过以往统计机器学习最先进的方法。受Sutskever等[65]的启发，Mei等[28]将语义解析问题也视为序列到序列的学习问题，使用带有LSTM架构的RNN模型,通过学习自然语言与对应的机器指令语句对(Pair)的映射,使模型能够预测给定自然语言句子对应的机器指令。该模型在单句和多句NL解析中均取得当时的最佳性能。接着，Dong等[5使用基于LSTM单元的RNN模型造序列到序列语义解析模型,进行从自然语言同时到多种逻辑形式的转换任务，通过训练 $<$ 自然语句，逻辑形式 $>$ 对(Pair)获得如公式(1)所示的概率模型,这一方法第一次取得了能与规则匹配竞争的效果(二者在GEO数据集[上的准确度均在 $70 \\%$ 左右）。此外,Wang等[8使用RNN模型构建数学文本到方程式的语义解析模型,编码器和解码器分别由两层门控循环单元（GatedRecurrentUnit,GRU)和LSTM组成,在数学文本数据集上的表现超过此前所有机器学习方法。",
    "context9": "随着Transformer架构的模型表现出更强的文本信息处理能力，谢德峰等提出基于Transformer[33]的编码器-解码器模型,实验表明，这一结构的语义解析效果超过了基于LSTM的编码器-解码器架构模型。",
    "context10": "此外,使用预训练模型作为编码器的方法也进一步提高了序列到序列模型的语义解析效果。邓庆康等[68使用BERT作为编码层,BiLSTM结合条件随机场(ConditionalRandomField,CRF)作为解码层进行中文语义解析任务，该方法比使用LSTM的编码器-解码器架构的方法在中文语义解析准确度上提高了约 $4 \\%$ 。对于中文语义解析，王鑫雷等[69]、范红杰等[7]发现使用中文知识增强的预训练模型，如ERNIE[],在处理中文语义解析上的效果略优于基线预训练模型。",
    "context11": "基于编码器-解码器架构的序列到序列神经语义解析模型在处理文本上有很大优势,但由于存在训练成本大，需要大量 $<$ 自然语句，逻辑形式 $>$ 标注数据对,并且在处理上下文信息上只能通过前后位置来决定等问题,使得这一架构有很大局限性。于是,相关研究者提出在不改变模型架构的情况下，对已有神经语义解析模型进行增强的方法。这些改进包括数据增强、注意力机制、添加语法约束和知识库以及向编码器加入结构信息等。"
  },
  "（2）数据增强": {
    "context1": "在数据有限时,模型由于缺乏足够数据训练，难以获得理想性能。基于半监督机器学习理论，一些数据增强方法被用来缓解数据不足的问题。例如，先利用已有数据和生成模型生成一些预测数据,即 $<$ 自然语句 $>$ 或 $<$ 逻辑形式 $>$ ,再将这些数据与已有数据中对应的自然语句或逻辑形式配对成新的 $<$ 自然语句,逻辑形式 $>$ 对,补充到原有数据集中。Kocisky等[72]通过半监督的方法，在给定自然语言问题 $x$ 和逻辑形式 $y$ 的数据对 $< x$ ， $y >$ 条件下，先用一个BiLSTM反向训练逻辑形式 $y$ 和自然语言序列 $x$ ，将编码器输出的x和真值 $x$ 与输入的逻辑形式y组成新的数据对，一起输入另一个LSTM模型学习与真值逻辑形式 $y$ 之间的映射，提高了对未知数据的学习能力。Shao等[73]通过整合其他NLP任务,如问题分类任务，利用问题类型和实体标签信息重新描述自然语句的问题,并将这些新的问题描述与对应逻辑形式组成新的 $<$ 自然语句，逻辑形式 $>$ 对。这一方法不仅扩大了原有数据集的规模，而且带有类型标签的自然语句能够被编码为携带更多信息的向量表示。"
  },
  "（3）注意力机制": {
    "context1": "语义解析与机器翻译的一个相似之处在于自然语句与目标形式之间同样存在一定程度的软对齐机制,即在预测目标形式序列的某个字符时，该字符可能与源序列中某些字符高度相关。而RNN模型预测输出序列时,往往高度依赖与当前字符相连的内容。这对于序列的转换是不利的。注意力机制（AttentionMechanism)[74]提供了一种软对齐机制（Soft-Alignment）,使得解码器在预测每个Token 时按照与该Token相关性大小分配给源自然语句中每个词汇不同的权重，大大提高模型预测目标序列的准确度。添加注意力机制的解码器预测过程如图3所示。将注意力机制加入编码器-解码器架构的语义解析模型中被广泛使用。为了得到格式更加良好的逻辑形式,Dong等[5]分别在编码器和解码器层添加注意力机制,并将逻辑符号加入词汇表。该方法在多个领域的语义解析数据集上的解析结果均超过不带注意力的基线模型。受Dong等[5研究的启发,Wang等[75]提出以Bi-LSTM和LSTM分别作为编码器和解码器单元并添加注意机制的模型,解决描述复杂数学问题文本的方程式转换问题。Huang等[76]发现基于RNN的序列到序列模型存在很严重的长尾问题,于是在序列到序列模型的基础上同时增加复制机制和注意力机制,有效减少了长尾导致的错误。Kocisky等[72]在研究半监督神经语义解析模型时，直接将带注意力机制的LSTM模型作为基线模型。此后，大多数基于RNN的编码器-解码器架构通常都会添加注意力机制。"
  },
  "（4）注入外部知识": {
    "context1": "另一种提高模型性能的直接想法是向模型中添加先验知识。在神经语义解析方法中，有许多向模型注入外部知识的尝试,如添加语法约束、领域知识等,都在一定程度上提高了解析性能。Yin等[7在RNN模型上增加了语法约束,以生成格式更加严谨的程序代码。Wang等[75]对Seq2Seq模型表示多变量方程式时的能力下降问题进行改进,在LSTM和注意力机制的模型中增加了模式约束,指导模型生成符合规范的方程表达式以描述数学问题,通过预设模式和知识辅助,进一步提高了描述复杂数学问题文本向方程式转换的解析准确度。"
  },
  "（5）增加结构信息": {
    "context1": "基线序列到序列模型的另一个缺点是只考虑输入输出序列的上下文,忽略了逻辑形式对应的结构信息，导致预测的逻辑形式在结构上存在很多不规范的问题,尤其是在程序语言和数据库、知识库查询相关的语义解析任务上情况更加严重(程序运行的前提是需要格式严谨的代码)。Dong等[5认为基线的序列到序列模型忽略了逻辑形式的层次结构信息,于是设计了一个能输出结构信息的序列到结构的解码器。Bogin等[78]使用图神经网络(GraphNeuralNetworks,GNN)模型[79-80]将自然语言和对应的数据库结构共同编码为向量表示，解码器根据图的结构类型决定下一个输出字符是逻辑形式的关键词还是数据库的表名、列名。Wang等[81]认为Bogin等[78]生成的数据库结构表示在解码表名时有歧义现象,于是在编码器中加入自注意力层,强化输入文本和数据库表名称之间的链接,之后在大型跨领域语义解析数据集Spider[82]上进行实验，其解析准确度比Bogin[78]等方法提高 $10 \\%$ 。",
    "context2": "![](images/f15bd624a5057c0aa15cc5f241a4c9cfe7398b88cd61b9d89ae7cec92fd05ea9.jpg)  \n图3带注意力机制的解码器解码过程  \nFig.3Decoder Decoding Process with Attention Mechanism",
    "context3": "另外,Yavuz等[83]则将实体信息加人LSTM编码器-解码器模型解决知识问答问题,向编码器中同时输入自然语言问题句和其中的实体序列,之后输出带有类型参数的抽象形式用于知识库查询。这种方法比直接在知识库上寻找实体答案的方式准确度更高,尤其是针对复杂问题的问答任务。Shaw等[84则基于Wang等[81]的模型架构,向图神经网络模型中添加潜在的实体关系节点，又使用BERT获得输入的嵌入表示，优化了基线GNN模型。"
  },
  "3.2借助中间形式的方法": {
    "context1": "受制于自然语言与结构的逻辑形式之间存在巨大的表达形式的差距,或者模型本身不擅长生成结构化的序列,以端到端的方式使用神经网络模型直接进行自然语言到逻辑形式的转换,在效果的提升方面存在瓶颈。因此,有研究者提出借助一种介于自然语言和逻辑形式之间的形式弥补二者之间结构上的差距,通过先生成中间形式进而借助中间形式得到格式更加良好的逻辑形式。神经语义解析方法中，常见的中间形式有语义草图(Sketch)和规范话语（Canonical Utterance）。"
  },
  "(1)语义草图": {
    "context1": "语义草图是一种在保留语句的结构和框架基础上忽略底层变量和参数细节的不完整的逻辑形式。基于草图的神经语义解析器，可以专注于粗粒度转换,能更加关注全局信息。基于草图的神经语义解析是一种重要的语义解析方法。",
    "context2": "早在2006年,Solar-Lezama等[85]就提出借助语义草图将自然语言转换为程序语言的想法。Nye等[8]设计了一种英语与代码之间的中间表示形式，使用神经网络先生成中间形式,再利用搜索算法填充形成完整的程序代码,解决了自然语言向具体代码转换的问题。Dong等[87提出一种基于草图的神经语义解析方法，使用编码器-解码器模型生成粗略的语义草图，再利用更深层的网络模型进行细节填充以生成SQL语句。这种方法同时保留了语言结构和语义。赵睿卓等[88]改进了Yavuz等[83]的语义草图,将线性的语义草图改为树状结构，进而指导解码器生成结构信息更完整的逻辑形式。Xu等[89通过实验证明,借助中间形式的神经语义解析方法在比直接使用基线序列到序列模型生成SQL语句的方法准确度提高 $10 \\%$ 左右。Li等[9以序列到序列的RNN模型为基础,使用基于草图的方法开发了一套完整的自然语言转换为逻辑形式的系统,在NLPCC2019语义解析任务测评中排名第一。之后，Ye等[91]利用Li等[90的方法,通过设计正则草图模板,再通过搜索、填充算法实现NL向正则表达式的转换。这些研究充分证明草图是一个有效缓解自然语言和逻辑形式之间结构差距的解决方案,对于将自然语言转换为格式要求更严谨的代码的任务来说，借助中间形式是必要的。"
  },
  "（2）规范话语": {
    "context1": "规范话语不同于草图,具有一定的逻辑结构，它是将自然语言进行规范化处理,形成语法正确、描述统一的类自然语言形式。起初是为了缓解实际场景中自然话语出现的各种语法和拼写错误给神经模型理解造成的困难。随着T5、GPT等文本到文本的生成模型的兴起[22],利用规范话语使预训练模型应用到语义解析等下游任务成为一种有效方法。",
    "context2": "Berant等[92]重新定义了语义解析任务，将语义解析视为释义问题,利用生成模型同时将候选逻辑形式和输入自然语言都转换为规范话语，再利用规范话语作为参照,筛选出语义等价的自然语句和逻辑形式。使用这一方法训练模型驱动的问答系统在WebQuestions数据集上[93]准确度比序列到序列的方法提高超过 $50 \\%$ 。Shin等[94]在使用GPT-3模型进行语义解析任务时,同样发现直接用预训练模型生成逻辑形式难度很大，于是借鉴了Berant等[92]的思想，先利用GPT-3生成包含一定逻辑信息的规范话语，再利用语法规则转换为逻辑形式，成功将该模型应用于语义解析。由于规范话语数据标注困难,且缺少可用语料,近年,基于规范话语和生成式预训练模型的小样本语义解析方法受到关注。",
    "context3": "借助中间形式的语义解析方法解决了序列到序列方法所生成的逻辑形式结构不完整、格式错误多的问题。但这种方法一般以分阶段的方式实现，仍需结合序列到序列神经模型或预训练模型获得最终逻辑形式。基于草图的方法示例如图4所示，“and”后面的\"thechocolate factory”一词在缺少中间形式的类型信息指导时，很容易被忽略，造成输出的逻辑式不完整。",
    "context4": "![](images/b8a9a04d41bf724edc04ffcfc10ecca94c3cfff074a0572ccfc62aff38db7e70.jpg)  \n图4基于草图的语义解析与端到端语义解析方法对比",
    "context5": "Fig.4Comparison Between Sketch Based Semantic Parsing and End-to-End Semantic Parsing"
  },
  "3.3基于分解与组合的方法": {
    "context1": "对于复杂的长句来说,直接生成逻辑形式或语义草图对神经网络模型来说难度都很大且容易产生更多错误,如信息丢失、长距离依赖问题等。由于逻辑形式本身具有良好的结构层次并且由注意力机制[74]可知，自然语句的片段与相应子逻辑形式存在对应关系。为了降低语义解析的难度,在现有模型基础上，许多研究者使用分解语义单元和组合子逻辑式的方法解决复杂语义解析问题。例如,为了改进单个序列到序列模型效果,Zhong等[32]利用 SQL固有的结构属性，分别预测逻辑形式中不同类型的子句。SQL结构以SELECT、WHERE和聚合函数三种从句类型组成，用一个增强的指针网络模型[20]分别输出聚合操作符、SELECT列名和WHERE条件序列,在训练过程中,先训练三种类型子句的概率分布再进行加权求和，最后将三类子句组合得到完整SQL形式。在解决复杂文本的SQL解析时，李青等[95-96]也采用了分解的思路,将原始问题拆分为子问题后，再进行SQL子句的生成。这种方法在复杂语义解析数据集中的解析准确度大大超过了序列到序列的方法。Yang等[3]发现,对于复杂长难句来说,使用规范话语作中间形式的语义解析方法，反而增加了任务的复杂性，于是同样使用子问题分解的方法，将预测完整的逻辑形式改为预测每个子逻辑形式,使用微调的预训练模型(BART)逐个预测每个子句，最后组成完整逻辑形式。Lindemann等[97]在研究自然语句转换为AMR图的意义表示时，发现直接预测完整的图难度过大且错误很多，于是将问题分解为预测每个子图,使用依赖解析算法[98]将AMR图分解为Token级的依赖图，将输入句子中的每个Token替换为AMR的相应子图，再通过组合和修正的方式逐步扩充成完整的AMR图。该方法不仅减少了直接预测AMR时产生的错误，还增强了扩展到其他图表示语言的通用性。",
    "context2": "Li等[99提出一种简化的自然语言问题分解和子逻辑形式预测方法，称为移进-规约方法。该方法更关注输入自然语言文本本身的语义结构，将自然语句划分为若干个包含完整语义的文本片段，分别生成对应子逻辑形式。利用队列和堆栈(Stack)的递进、规约来筛选、合并语义片段,之后用一个带有门控循环单元(GRU)的双向RNN对语义片段进行编码和解码,但实验准确度相比于使用图神经网络的分解方法[84]仍有 $3 . 8 \\%$ 的差距。",
    "context3": "将复杂长文本的语义解析问题分解为若干个子逻辑形式的预测问题，是基于分治思想解决问题的方法。由于深度学习和神经网络都处于发展之中，还有许多不足之处，在现有神经语义解析模型性能瓶颈下,基于分解与组合的方法,是进一步提高神经语义解析性能的有效策略。"
  },
  "3.4少样本语义解析问题": {
    "context1": "利用大规模未标记语料库训练的预训练模型在捕获自然语言语义、句法结构等通用的语言知识上具有很大优势[100]。虽然以BERT为编码器的神经网络架构对于语义解析性能的提高是有效的,但仅在原有序列到序列或借助中间形式的神经语义解析架构中直接加入预训练模型作为编码或解码的一部分[35-36,101]并没有降低神经语义解析模型的复杂度，基本的神经语义解析架构并没有实质改变。于是，研究者试图使用合理的微调方式，以较小的代价直接使用预训练模型实现语义解析任务。",
    "context2": "以BERT基于序列标注模型、GPT生成式模型为代表的两类预训练语言模型几乎占据了目前预训练模型应用研究的全部。因此，现有模型微调方式Fine-Tuning和Prompt-Tuning也是针对这两类模型提出的。"
  },
  "（1）Fine-Tuning": {
    "context1": "Fine-Tuning是使用一定数量的标记数据对预训练模型参数进行微调,将预训练模型改造为神经语义解析模型。Rongali等[35]的工作证明BERT、RoBERTa用于语义解析任务是有效的。在Rongali等[5]工作的基础上,Chen等[5将RoBERTa编码器换成另一预训练模型BART并对模型加以少量领域数据的预训练,实验结果比加入RoBERTa作为编码层的效果约有 $1 \\%$ 的提升。 $\\mathrm { X u }$ 等[102]通过微调BERT,并在解码层加入指针生成网络，训练了一个通用神经语义解析框架,该模型在Overnight数据集[63]上的准确度超过其他所有基线神经网络模型。",
    "context2": "BERT虽然大大提高了语义解析的性能，但Fine-Tuning的过程需要调整的参数相当多， $\\mathrm { X u }$ 等[102]在实验中需要调整的参数量高达128M,并且针对具体任务的模型训练仍然需要大量的数据作支撑。虽然使用数据扩充、注意力机制等方法在一定程度上缓解了数据不足的问题，但仍然无法有效解决高昂的数据成本问题。"
  }
}