{
  "original_filename": "full_4491.md",
  "基于循环神经网络的网络舆情趋势预测研究": {
    "context1": "孙靖超¹,周睿²,李培岳,芦天亮1",
    "context2": "(1.中国人民公安大学信息技术与网络安全学院,北京100076；2.清华大学工程物理系公共安全研究院,北京100084；3.中国人民公安大学研究生院,北京100038)",
    "context3": "摘要：【目的/意义】网络舆情预测由于对指导政府工作,维护社会稳定具有很高的现实意义，一直是网络安全领域研究工作的关注重点。但是网络與情演化趋势复杂,影响因素众多，前人工作多考虑了单变量因素，忽视了多因素对趋势的影响,且前人采用的传统模型由于自身的局限性,针对非线性场景的预测很难收到较好效果【方法/过程】为解决前人研究的不足,本文设计了一种基于循环神经网络的自适应学习率的网络舆情模型,根据舆情数据特点选取了多种特征构建了循环神经网络序列生成模型,针对循环神经网络模型收敛困难的问题,通过连续最优掷币策略自适应调节学习率来提高训练速度和预测精度【结果/结论】实验结果表明,与传统方式和普通神经网络相比,本文方法有着更好的舆情预测效果。",
    "context4": "关键词：网络舆情;循环神经网络;时间序列;舆情预测 中图分类号：G206.3 DOI:10.13833/j.issn.1007-7634.2018.08.020"
  },
  "Research on the Prediction of Network Public Opinion Based on Recurrent Neural Network": {
    "context1": "SUN Jing-chao', ZHOU Rui², LI Pei-yue', LU Tian-liang1 (1.SchoolofInformationTechnology&NetworkSecurity,People'sPublicSecurity UniversityofChina,Beijing0006, China;2.InstituteofPublicafetyResearchDepartmentofEngineeringPhysics,Tsinghua UniversityBeijing0084, China; 3.Graduate School,People's Public Security University ofChina,Beijing 10oo38,China)",
    "context2": "Abstract:【Purpose/significance]Prediction of network public opinion has high practical significance for guiding government workandmaintaining socialstability,andithasalways been thefocusofresearch work inthefieldofcybersecurity. However,theevolution trendof Internet publicopinion is complexand has many influencing factors.The predecessors' work considersunivariatefactorsandignorestheinfluenceof multiplefactorsonthe trend.Duetothe limitationsof traditionalmodels,itisdiffculttohavegoodpredictionefectonthenonlinearscenario.【Method/processInordertosolvethe deficiencies of previous work,this paperdesigns anetwork public opinion model ofadaptive learning ratebasedon recurrentneural networks.Basedonthecharacteristics of publicopiniondata,avarietyoffeatures are selectedtoconstructaRecurrent Neural Networks Seq2seq model.Tosolve the problemof diffcultyin theconvergenceof therecurrentneural networks model,the training speed and prediction accuracyare improved by continuously adjusting the learning rate by theoptimal coin beting strategy.【Result/conclusion】Theexperimental results show thatcompared with the traditionalmethod andthecommon recurrent neural network,the proposed method has better performance inthe predictionof publicopinion.",
    "context3": "Keywords : network public opinion; Recurrent Neural Networks ; time series; prediction of network public opinion",
    "context4": "互联网对于日常社交而言变得越来越重要，在过去的十年中,涌现出许多类似微信、微博的线上社交媒介，人们在其中建立自己的交际圈,分享见解、经验和观点。在线社交网络便于公众就政治、经济、娱乐、生活等不同话题发表意见，带动着网络舆情在维护社会稳定的地位的逐步提高。因此，如何处理好网络舆情成为新时代一个重要的挑战,没有做好应对舆情的充足准备、引导措施不及时都可能导致政府的安全危机。",
    "context5": "收稿日期:2016-05-14",
    "context6": "网络舆情的定义为通过互联网表达和传播的各种不同情绪、态度和意见交错的总和，其主要特点为自由性可控性、交互性即时性、非理性、丰富性多元性等特点。现有的网络舆情预测研究主要是在时序分析的基础上，采用数学模型进行拟合。高辉等通过层次聚类建立类库，应用最小二乘法从所属事件类模型中选取恰当模型,可获取舆情的整体趋势。杜智涛等构建了研判指标体系,运用灰色预测方法建立网络舆情预测模型。吴鹏等[4基于AGENT建模技术，对突发事件中网络舆情演变过程中网民群体行为进行建模与仿真,揭示了突发事件网络舆情演变的规律。苏创等[5考虑网络舆情传播过程中存在的不确定因素,提出了基于不确定微分方程的网络舆情传播模型。王新猛[通过划分状态空间、构建状态转移矩阵、开展舆情热度趋势预测等步骤建立了基于马尔科夫链的政府负面网络舆情热度趋势预测模型。李彤等提出基于模型集成的微博情感分析与预测模型,对突发事件微博舆情进行了情感分类与趋势预测。魏德志等[8在证明网络舆情发展趋势具有混沌特征的基础上，提出了EMPSO_RBF神经网络对网络舆情进行预测。郭淼等[9]通过改进的SEIR模型和PageRank算法结合贝叶斯网络很好的提高了微博舆情预测的精度。兰月新等[通过分析公共危机事件特征，构建出了网络舆情热度模型，研究了模型稳定性并用化解度细致分析了舆情化解程度。游丹丹等[通过收集百度指数作为衡量舆情发展的基准，构建了基于改进的粒子群算法和BP神经网络的网络预测模型。徐敏捷等[2将逻辑回归模型、指数平滑模型和灰色模型结合，用AHP赋值来作为网络舆情预测模型。陈福集[13]等利用模拟退火算法对RBF神经网络进行优化，构建了SAPSPO_RBF模型来进行网络优化。李倩倩等[4通过多种维度提取特征体系，通过比较多种机器学习算法找出了随机森林作为最佳预测模型。张和平等通过集合平均弱化缓冲算子处理后建立改进的灰色Verhulst模型，并用马尔可夫模型对其结果进行修正,提高了预测准确率。黄亚驹等[16-17]通过构建信息熵和遗传算法的混合算法,构建混合算法优化的BP神经网络,提高预测稳定性和准确率。通过构建模糊逻辑的事件序列预测模型,同时构建BP神经网络预测模型，以组合预测的方式提高了整体的预测精度,实现了对舆情的早期预测。",
    "context7": "前人工作的研究工作在舆情预测时多使用了传统的线性模型,而网络舆情问题并不是个线性的问题,其因素和结果主要呈非线性关系,且在特征选取时仅考虑了单变量因素，忽视了多因素的综合影响，因此很难收到良好的预测效果。为了解决前人工作的不足,本文在特征工程方面引入了更多影响因素,针对传统方法无法应对非线性情况以及难以适应多变量输入的问题,本文采用了循环神经网络序列生成模型对网络舆情进行建模，与传统机器学习方法或是其他神经网络相比,循环神经网络模型可以记忆输入数据，是时序数据的处理首选,尤其在非线性模型时序数据预测中具有很大优势。在对网络的训练过程中,学习率的设置是关键点，其决定了权值更新的速度,其值太大或太小则都会使随机梯度下降过程异常，无法得到好的训练效果。针对此问题，采用了连续最优掷币策略方法，将学习率优化问题转变为一个随机掷币最优化问题,通过优化掷币策略来逐渐寻找随机梯度函数最优解使其达到最好的预测效果，更准确的拟合舆情趋势。"
  },
  "1相关理论": "",
  "1.1 循环神经网络": {
    "context1": "循环神经网络RNN(RecurrentNeuralNetworks)在许多机器学习任务中都有非常优秀的表现,尤其是当处理输入、输出是可变长度变量时序时,性能十分突出。循环神经网络是常规前馈神经网络的扩展，其能够处理可变长度的序列输入。给定一个可变长度的序列 $\\boldsymbol { x } = \\left( x _ { 1 } , x _ { 2 } , x _ { 3 } . . . . x _ { T } \\right)$ ， $x _ { T }$ 表示第T步的输入,第 $t$ 步的隐藏态 $h _ { \\mathbf { \\Phi } ( t ) }$ 会通过公式(1)来得到更新，其中 $f$ 是个非线性的激活函数如logistic函数,RNN的输出$\\boldsymbol { y } = \\left( \\boldsymbol { y } _ { 1 } , \\boldsymbol { y } _ { 2 } , \\boldsymbol { y } _ { 3 } , . . . , \\boldsymbol { y } _ { t } \\right)$ 同样是一个可变长度的序列。",
    "context2": "$$\nh _ { _ { ( i ) } } = f ( h _ { _ { ( t - 1 ) } } , x _ { _ t } )\n$$",
    "context3": "循环神经网络可以通过训练来获得序列上的概率分布，每步 $t$ 的输出都是条件分布 $p ( x _ { t } | x _ { t - 1 } , . . . . , x _ { 1 } )$ ,例如,可以用公式(2)的激活函数来输出多项分布,其中 $w _ { j }$ 是权重矩阵 $W$ 的行，通过公式(3)可以计算出序列 $x$ 的概率。",
    "context4": "$$\np ( x _ { { t } , j } = 1 | x _ { { t } - 1 } , . . . , x _ { \\mathrm { i } } ) = \\frac { \\exp ( w _ { j } h _ { { t } _ { i } } ) } { \\sum _ { j ^ { \\prime } = 1 } ^ { K } \\exp ( w _ { j } h _ { { t } _ { i } } ) }\n$$",
    "context5": "$$\np ( x ) { = } \\prod _ { t = 1 } ^ { T } p ( x _ { t } | x _ { t - 1 , \\cdots } { , } x _ { 1 } )\n$$",
    "context6": "Bengio等人[8]发现由于梯度下降很容易趋向消失或爆炸,且梯度幅度的变化过大易导致长期相关性影响被短期相关性影响所覆盖使基于梯度下降的优化方法有很大的局限性。有许多研究人员试图解决这个问题，一种方法是设计比简单的随机梯度下降更好的学习算法,例如使用简单的梯度裁剪。另一种方法是设计比通常的激活函数更复杂的函数，其中包括仿射变换,然后使用门控单元进行简单的单元非线性操作。在这个方向上最早的尝试称为长期短期记忆(LSTM)。在LSTM出现之后,Cho等人[9提出了另一种类型的循环单元，称之为门控循环单元(GRU)。"
  },
  "1.2 门控循环单元": {
    "context1": "门控循环单元Gated Recurrent Unit(GRU)是由 Cho提出[19]的使每个循环单元能够自适应地捕捉不同时间尺度依赖关系的一种神经网络单元。与LSTM类似，GRU具有调节单元内部信息的方式的门控单元,但其没有单独的存储单元。",
    "context2": "时刻 $t$ 处GRU的激活函数 $h _ { i } ^ { j }$ 由先前激活 $h _ { t - 1 } ^ { j }$ 和候选激活 $\\tilde { h } _ { \\iota } ^ { j }$ 之间的线性插值得到：",
    "context3": "$$\nh _ { \\iota } ^ { j } = ( 1 - z _ { \\iota } ^ { j } ) h _ { \\iota - 1 } ^ { j } + z _ { \\iota } ^ { j } \\tilde { h } _ { \\iota } ^ { j }\n$$",
    "context4": "其中更新门(update gate) $\\boldsymbol { z } _ { t } ^ { j }$ 决定该单元是否更新其激",
    "context5": "活函数的值，通过公式(5)计算得到。",
    "context6": "$$\nz _ { \\iota } ^ { j } = \\sigma ( W _ { z } x _ { \\iota } + U _ { z } h _ { t - 1 } ) ^ { j }\n$$",
    "context7": "在现有状态和新状态之间取得线性和的过程类似于LSTM,候选激活 $\\tilde { h } _ { \\iota } ^ { j }$ 的计算与传统的递归单元相似，如公式(6)所示。",
    "context8": "$$\n\\tilde { h } _ { \\iota } ^ { j } = \\operatorname { t a n h } ( W x \\iota + U ( r _ { \\iota } \\bigodot h _ { \\iota - 1 } ) )\n$$",
    "context9": "其中 $r _ { t }$ 是一组重置门(reset gate),关闭时 $r _ { t } ^ { j }$ 接近于0,重置门能有效地使单元如读取输入序列的第一个符号,从而允许它遗忘之前计算的状态。",
    "context10": "重置门 $r _ { t } ^ { j }$ 的计算与更新门类似：",
    "context11": "$$\nr _ { \\iota } ^ { j } = \\sigma ( W _ { z } x _ { \\iota } + U _ { r } h _ { \\iota - 1 } )\n$$",
    "context12": "更新门可以控制过去的隐含状态在当前时刻的重要性，如果更新门直近似1,过去的隐含状态将直通过时间保存并传递至当前时刻,这个设计可以应对循环神经络中的梯度衰减问题,并更好地捕捉时序数据中间隔较的依赖关系。总体而言，重置门有助于捕捉时序数据中短期的依赖关系，更新门有助于捕捉时序数据中长期的依赖关系。"
  },
  "2 预测模型构建": "",
  "2.1循环神经网络序列生成舆情预测模型": {
    "context1": "深度学习模型已在视觉、语音、医学等众多领域有了非常好的表现，由于RNN具有短期记忆能力，对于处理时序数据具有天然优势。文章[20]中提出一种用长时间短期记忆LSTM(Long Short-Term Memory)结构的方法来应对序列问题,首先是使用一个LSTM读取输入序列以获得较大的固定维矢量表示，然后使用另一个LSTM从该矢量提取输出序列。该模型基于LSTM网络,通过读入输入序列,来预测输出序列。但是LSTM结构复杂，本文使用了门控循环单元GatedRecurrentUnit(GRU)作为编码和解码层的基本单元，文章【21】详细对比了LSTM和GRU以及传统的RNN的异同,发现GRU在依赖更少参数、需要更少训练时间的同时，有着更好的预测效果。",
    "context2": "预测模型结构如图1所示：",
    "context3": "![](images/608f552a9ff2c63ed7b80ff255e100ea0fe218bcc18faa43f2ce49d57e1ad74b.jpg)  \n图1序列生成模型结构",
    "context4": "模型的构建包括Encoder编码器层与Decoder解码器层两大部分。本文的编码器和解码的基本单元均为GRU,连接两者的为状态向量 $c$ 。编码器的作用是将输入序列压缩成指定长度的状态向量,该状态向量包含了输入序列的信息,在本模型中编码器的输入为特征组合向量 $x _ { t }$ ，$t = 1 , 2 . . . , T$ 。其隐含层变量为 $h _ { \\iota } = f ( x _ { \\iota } , h _ { \\iota - 1 } )$ ，状态向量$c = q ( h _ { 1 } , . . . , h _ { T } )$ ,其包含了输入序列 $x _ { 1 } , x _ { 2 } , . . . , x _ { T }$ 的信息。输出序列为 $y _ { 1 } , y _ { 2 } , . . . , y _ { T ^ { * } }$ ,其中每个 $t$ 时刻的输出 $\\boldsymbol { \\mathcal { Y } } _ { t }$ 既由 $\\boldsymbol { \\mathcal { Y } } _ { t - 1 }$ 之前的输出有关又取决于状态向量,之后最大化输出序列的联合概率 $P ( y _ { 1 } , . . . , y _ { r } ) = \\prod _ { t ^ { \\prime } = 1 } ^ { T ^ { \\prime } } P ( y _ { t } , | y _ { 1 } , . . . , y _ { t ^ { \\prime } - 1 } , c )$ 并得到损失函数为$- \\mathrm { l o g } P ( y _ { 1 } , . . . . , y _ { T ^ { \\circ } } )$ ，用函数 $p$ 表示单个输出 $\\boldsymbol { y } _ { t ^ { \\prime } }$ 的概率$P ( y _ { t } . | y _ { 1 } , . . . , y _ { t ^ { * } - 1 , } c ) = p ( y _ { t ^ { * } - 1 } , s _ { t ^ { * } } , c )$ 。其中 $\\boldsymbol { s } _ { t ^ { \\prime } }$ 为 $t ^ { \\prime }$ 时刻的解码器的隐层变量。该隐层变量 $s _ { t ^ { \\prime } } = g ( y _ { t ^ { \\prime } - 1 } , c , s _ { t ^ { \\prime } - 1 } )$ ,其中 $g$ 是循环神经网络单元GRU。"
  },
  "2.2利用连续最优掷币策略优化舆情模型": {
    "context1": "在构建的舆情模型训练过程中,学习率的调整是在学习中的主要性能瓶颈,其决定了权值更新的速度,其值太大会发生梯度爆炸或梯度震荡，其值太小则会使随机梯度下降过慢或引起过拟合，无法达到很好的学习效果。为了解决此问题,学习率的选择采取了连续最优掷币策略优化方法[2,该方法不需要调整学习率也不需要利用目标函数的假定曲率，可以为每一步训练预测学习率。",
    "context2": "优化的基本思路是将解决随机次梯度下降问题转化为掷币问题。首先介绍掷币问题,假设掷币开始时的初始金额$\\varepsilon > 0$ 。在每一轮 $t$ ,对掷币结果 ${ \\boldsymbol { g } } _ { t } \\in \\{ - 1 , 1 \\}$ 下注,其中 $+ 1$ 表示硬币正面，-1表示硬币反面。每一轮可以下注任何金额但是不能借钱，如果输了，则输掉下注的金额;如果赢了，会获得两倍的下注金额。将t轮赌注编码为 $w _ { t }$ 来表示投注结果。用绝对值编码下注金额，将 $W e a l \\operatorname { t h } _ { t }$ 定义为第t轮结束时的赌徒财富，Reward,定义为赌徒的净收益(财富与初始资金的差额），即 $W e a l \\operatorname { t h } _ { \\iota } = \\varepsilon + \\sum _ { i = 1 } ^ { \\iota } w _ { i } \\mathbf { g } _ { i }$ ， $\\mathrm { R e } w a r d _ { t } = W e a l \\operatorname { t h } _ { t }$ $- \\varepsilon = \\sum _ { i = 1 } ^ { t } w _ { i } g _ { i }$",
    "context3": "赌注 $\\beta _ { \\iota }$ 由 $w _ { \\iota } = \\beta _ { \\iota } W e a l \\operatorname { t h } _ { \\iota - 1 } $ 得到,绝对价值 $\\beta _ { \\iota }$ 是当前财富的下注比例,符号代表投注正反。不能借钱的限制意味着$\\beta _ { \\iota } \\in [ - 1 , 1 ]$ 。通过将掷币结果 $\\boldsymbol { g } _ { t }$ 为[-1,1]中的任何实数来泛化一般问题。",
    "context4": "接下来介绍该方法如何用到随机次梯度下降里，首先将掷币结果 $g _ { t }$ 等同于 $w _ { t }$ 中需得到最小值的函数 $F$ 的负次梯度，即 ${ \\cal g } _ { t } \\in \\partial [ - { \\cal F } ( w _ { t } ) ]$ ,其中 $w _ { t }$ 是下注金额。在第一次迭代中， $w _ { 1 } = 0$ ,用函数 $H ( x )$ 用来确保投注策略T轮之后的财富对于任意序列 $_ { g 1 , \\ldots , g T }$ 至少为 $H ( \\sum _ { t = 1 } ^ { T } g _ { t } )$ 。用 $x ^ { * }$ 表示函数F$\\left( { \\bf { \\sigma } } _ { \\bf { X } } \\right)$ 的最小值，可以得出以下推导：",
    "context5": "$$\n\\begin{array} { l } { \\displaystyle F ( \\frac { 1 } { T } \\sum _ { \\iota = 1 } ^ { T } w _ { \\iota } ) - F ( \\boldsymbol { x * } ) \\leq \\frac { 1 } { T } \\sum _ { \\iota = 1 } ^ { T } F ( w _ { \\iota } ) - F ( \\boldsymbol { x * } ) } \\\\ { \\leq \\frac { 1 } { T } \\sum _ { \\iota = 1 } ^ { T } g _ { \\iota } \\boldsymbol { x } \\ast - \\frac { 1 } { T } \\sum _ { \\iota = 1 } ^ { T } g _ { \\iota } w _ { \\iota } \\leq \\frac { 1 } { T } + \\frac { 1 } { T } \\sum _ { \\iota = 1 } ^ { T } g _ { \\iota } \\boldsymbol { x } \\ast - H ( \\sum _ { \\iota = 1 } ^ { T } g _ { \\iota } ) ) } \\\\ { \\leq \\frac { 1 } { T } + \\frac { 1 } { T } \\operatorname* { m a x } v \\boldsymbol { x } ^ { \\ast } - H ( v ) = \\frac { H \\ast ( \\boldsymbol { x } ^ { \\ast } ) + 1 } { T } } \\end{array}\n$$",
    "context6": "第一个不等式使用了Jensen的不等式,第二个不等式是次梯度的定义，第三个不等式为对 $H$ 的假设,最后一个不等式是 $H$ 的Fenchel共轭的定义，如此证明了投注的均值$\\frac { 1 } { T } \\sum _ { t = 1 } ^ { T } w _ { t }$ 即收敛于最小值优化问题,可以通过掷币算法找到通过次梯度来寻找非平滑目标函数的最小值。",
    "context7": "连续最优掷币策略的有效学习率为 $\\tilde { \\eta } _ { i } \\colon = w _ { t } \\sqrt { \\sum _ { i = 1 } ^ { t } g _ { i } ^ { 2 } }$ 。在训练过程中学习率不是恒定的，也不是单调递增或递减，而是自适应的,当距最优较远时较大,接近最优时则逐渐变小，这保证了其有良好的逼近最优解的能力。",
    "context8": "经过优化后的预测模型流程图如图2所示：",
    "context9": "![](images/48bea47bf4506d37bffcc2fcfd0bfee2a981868c43960aab63bef68395fe5756.jpg)  \n图2预测模型流程图",
    "context10": "模型流程的关键步骤如下：",
    "context11": "步骤1：从互联网爬取所需的数据,给定模型的训练数据集,包含输入和输出样本集。",
    "context12": "步骤2：对收集的数据进行特征工程,根据舆情情况，选定适合学习的变量特征。",
    "context13": "步骤3：对提取的特征进行数据标准化和归一化处理，然后将特征转变为tensor输入模型。",
    "context14": "步骤4：构建好模型的拓扑结构，确定编码器和解码器模型单元和层数。",
    "context15": "步骤5：对模型的超参进行初始化。",
    "context16": "步骤6：通过编码器对时间序列进行编码,生成状态向量,通过解码器对其进行解码,在此过程中通过ASGD(Aver-aging Stochastic GradientDescent)等方式进行性能调优，并通过连续最优掷币策略保证学习率。在每轮迭代时,通过$G _ { t - 1 , i } + | g _ { t , i } |$ 来更新次梯度和 $G _ { t , i }$ ，通过$\\mathrm { R e } w a r d _ { t - 1 , i } + ( w _ { t , i } - w _ { 1 , i } ) g _ { t , i }$ 更新 $\\mathrm { R e } w a r d _ { t , i }$ ，用 $\\theta _ { t - 1 , i } + g _ { t , i }$ 来更新梯度和0,；，β $\\beta _ { t , i } = \\frac { 1 } { L _ { i } } ( 2 \\sigma ( \\frac { 2 \\theta _ { t , i } } { G _ { t , i } + L _ { i } } ) - 1 )$ 计算投注，其中$\\sigma ( x ) = \\frac { 1 } { 1 + \\exp ( - x ) }$ ,然后通过 $w _ { 1 , i } + \\beta _ { \\iota , i }$ 来更新参数 $w _ { t + 1 , i }$ ,最终返回计算结果 $\\bar { w } _ { T } = \\frac { 1 } { T } \\sum _ { t = 1 } ^ { T }$ 0。",
    "context17": "步骤7：利用模型对实验数据进行训练，得出预测结果。"
  },
  "3实验及结果分析": "",
  "3.1样本数据的获取与特征预处理": {
    "context1": "预测实验部分采用2014-2017年热点话题‘两会'作为预测数据从百度指数上收集网络舆情发展趋势的时序数据，作为实验样本,时间跨度选为舆情指数波动明显的二月二十号到三月三十号，其中2016年为闰年，数据从二月21日开始收集到三月三十日，4年共156个数据点。以2017年的数据作为测试数据，以2014-2016年的数据作为训练数据，使用前10天的数据预测舆情发展趋势,选取了时间、热度、年份之间的自相关性,流量规模,历史热度作为数据特征，其中时间上选择了每年两会的召开和闭幕时间作为重要的时间节点，年份之间的自相关性表示在序列不同时期的相关程度，计算结果由公式(8)得到：",
    "context2": "$$\nr = { \\frac { \\displaystyle \\sum _ { i = 1 } ^ { n } \\bigl ( X _ { i } - { \\bar { X } } \\bigr ) \\bigl ( Y _ { i } - { \\bar { Y } } \\bigr ) } { \\sqrt { \\displaystyle \\sum _ { i = 1 } ^ { n } ( X _ { i } - { \\bar { X } } ) ^ { 2 } } \\sqrt { \\displaystyle \\sum _ { i = 1 } ^ { n } ( Y _ { i } - { \\bar { Y } } ) ^ { 2 } } } }\n$$",
    "context3": "由于热度经过了处理，规模信息发生了缺失，将热度规模信息的值作为特征加入到数据中，取值为热度数据的中位数。历史热度选用了39天和78天前的数据作为特征，为了减少数据噪声在取了这两个时间点的3天范围内的值，当天和前后两天的数据各占一半权重。",
    "context4": "所有特征都经过了0均值标准化(Z-score standardiza-tion)方法将原始数据集归一化为均值为0、方差1的数据集，对热度数据使用 $\\log 1 0$ 函数进行处理以减少偏态,提升训练效果。0均值标准化方法如下：",
    "context5": "对序列 $m _ { 1 } , m _ { 2 } , . . . . . , m _ { n }$ 进行变换,变换公式如公式(9)：",
    "context6": "$$\n\\gamma _ { i } = \\frac { x _ { i } - \\mu } { \\sigma }\n$$",
    "context7": "其中 $\\mu \\ , \\sigma$ 为数据集的均值和标准差， $\\mu { = } \\frac { 1 } { n } { \\sum _ { i = 1 } ^ { n } m _ { i } }$ $\\sigma = \\sqrt { \\frac { 1 } { n - 1 } \\sum _ { i = 1 } ^ { n } ( m _ { i } - \\mu ) ^ { 2 } } \\textrm { o }$",
    "context8": "趋势预测时用5天的数据预测后5天的走势，使用2017年的数据作为测试集。"
  },
  "3.2模型参数设置": {
    "context1": "模型采用了一层编码层和一层解码层,编码和解码单元均为GRU,方向为单向,编码器的单元数量为5,编码器drop-out值设为0.5。"
  },
  "3.3仿真实验及结果分析": {
    "context1": "本文模型使用python构建,使用了TensorFlow进行网络架构搭建,为了验证本文循环神经网络序列生成舆情预测模",
    "context2": "型模型的有效性，构建了普通循环神经网络预测模型和ARIMA预测模型作为对照组以更好比较预测结果，模型预测结果的拟合曲线如图3所示。",
    "context3": "![](images/65e9ae0a92b2245393c8b17a98022d0a61c947819684ced5e8b64bdd73f7f6d4.jpg)  \n图3模型预测结果对比图",
    "context4": "表1模型预测相对误差对比",
    "context5": "<table><tr><td>时间段</td><td>ARIMA预测</td><td>普通循环神经 网络预测</td><td>序列生成 模型预测</td></tr><tr><td>2月25号</td><td>0.944854477</td><td>-0.013786381</td><td>0.872023395</td></tr><tr><td>2月26号</td><td>0.390889319</td><td>0.47160464</td><td>-0.174601257</td></tr><tr><td>2月27号</td><td>-0.22091195</td><td>-0.055670204</td><td>-0.604068396</td></tr><tr><td>2月28号</td><td>-0.176648127</td><td>-0.218255508</td><td>-0.015176993</td></tr><tr><td>3月1号</td><td>-0.550662614</td><td>-0.457677812</td><td>-0.210066869</td></tr><tr><td>3月2号</td><td>-0.262387229</td><td>0.289986229</td><td>-0.152721958</td></tr><tr><td>3月3号</td><td>-0.141651964</td><td>0.115841699</td><td>-0.024583337</td></tr><tr><td>3月4号</td><td>0.483950507</td><td>0.57605577</td><td>0.269109208</td></tr><tr><td>3月5号</td><td>-0.117132166</td><td>-0.129545488</td><td>-0.021671344</td></tr><tr><td>3月6号</td><td>-0.126918213</td><td>-0.158101378</td><td>-0.257473812</td></tr><tr><td>3月7号</td><td>-0.242235666</td><td>0.278745376</td><td>-0.013043311</td></tr><tr><td>3月8号</td><td>-0.033098623</td><td>-0.157668197</td><td>-0.055933251</td></tr><tr><td>3月9号</td><td>-0.105078093</td><td>-0.199852809</td><td>0.01866757</td></tr><tr><td>3月10号</td><td>0.118971891</td><td>-0.010683412</td><td>-0.010180503</td></tr><tr><td>3月11号</td><td>0.179240953</td><td>0.182400706</td><td>0.056593116</td></tr><tr><td>3月12号</td><td>0.252496839</td><td>0.038890645</td><td>0.08835335</td></tr><tr><td>3月13号</td><td>-0.372317814</td><td>-0.253656377</td><td>-0.097836538</td></tr><tr><td>3月14号</td><td>0.077462095</td><td>0.0944294</td><td>0.046434418</td></tr><tr><td>3月15号</td><td>0.126988865</td><td>0.055504534</td><td>0.116565262</td></tr><tr><td>3月16号</td><td>0.836166496</td><td>0.859023517</td><td>0.794627641</td></tr><tr><td>3月17号</td><td>0.86661195</td><td>0.675344714</td><td>0.370814183</td></tr><tr><td>3月18号</td><td>-0.296630232</td><td>0.19594016</td><td>0.368256687</td></tr><tr><td>3月19号</td><td>-0.396341766</td><td>-0.008598837</td><td>-0.097569462</td></tr><tr><td>3月20号</td><td>-0.521027826</td><td>-0.316414393</td><td>-0.165548808</td></tr><tr><td>3月21号</td><td>0.045976392</td><td>-0.409367757</td><td>0.067967445</td></tr><tr><td>3月22号</td><td>0.110892578</td><td>-0.300052062</td><td>-0.005611153</td></tr><tr><td>3月23号</td><td>-0.132467532</td><td>-0.311147186</td><td>-0.176244589</td></tr><tr><td>3月24号</td><td>-0.297518862</td><td>-0.114408009</td><td>-0.039103308</td></tr><tr><td>3月25号</td><td>-0.156963061</td><td>0.136089225</td><td>0.10549473</td></tr><tr><td>3月26号</td><td>-0.336183206</td><td>0.015368957</td><td>-0.180254453</td></tr><tr><td>3月27号</td><td>-0.163789903</td><td>-0.298997773</td><td>-0.177060134</td></tr><tr><td>3月28号</td><td>0.03004973</td><td>-0.39286848</td><td>0.027616125</td></tr><tr><td>3月29号</td><td>-0.291139241</td><td>-0.289938891</td><td>-0.047904845</td></tr><tr><td>3月30号</td><td>-0.247983628</td><td>-0.138798604</td><td>-0.01191766</td></tr><tr><td>平均误差</td><td>0.2839</td><td>0.2418</td><td>0.1689</td></tr></table>",
    "context6": "为更好的理解模型预测结果,根据公式(10)来计算真实值与计算值的相对误差进行结果误差的对比,根据公式计算",
    "context7": "其平均值,其中 $y _ { i }$ 为预测值, $t _ { i }$ 为期望值,根据公式(11)计算平均相对误差,结果如表1所示。",
    "context8": "$$\nR E { = } ( { \\gamma } _ { i } { - } t _ { i } ) / t _ { i }\n$$",
    "context9": "$$\nM R E = \\frac { 1 } { n } { \\sum _ { \\mathrm { ~ \\scriptsize ~ 1 ~ } } ^ { n } } \\lvert R E \\rvert\n$$",
    "context10": "从拟合性能和预测精度来看，本文模型相较传统模型和普通神经网络有着非常显著的提升，在舆情趋势预测上，本文方法有着优异表现,究其原因,在于循环神经网络对于处理时序数据的记忆功能对于处理时序数据与传统方法相比有着天然的优势，与传统方法相比，预测稳定性好，浮动小，精度更高，而本文采用的基于GRU的序列生成模型架构和连续最优掷币策略的应用使其与传统循环神经网络相比有更好的预测优势，能够更好的拟合现实舆情。"
  },
  "4结语": {
    "context1": "网络舆情的发展演化复杂,影响因素众多,传统基于统计或普通神经网络的方法对非线性数据和多变量特征的预测效果较差。并且为了改进以往工作的不足，本文选取了多种特征以更好的捕捉舆情趋势,提出了一种基于循环神经网络的序列生成模型,利用循环神经网络处理时序数据的强大优势，使用门控循环单元作为序列生成模型的编码器和解码器,针对在训练过程中学习率过大导致的梯度爆炸或局部震荡,或是学习率过小引起的收敛速度慢、易过拟合的情况，用连续最优掷币策略解决拟合过程中梯度下降遇到的学习率调优问题,通过实验与普通神经网络和传统方法相比，本文方法均有着更好的预测精度，可以较好的预测舆情的趋势。下一步的工作重点在于对舆情影响因素的调研,理清各因素对舆情的作用过程及影响力大小，根据各影响因素作为特征建立舆情库,使在新舆情发生时能够到更及时准确的预测舆情态势，有效帮助政府部门辅助决策。"
  },
  "参考文献": {
    "context1": "1 钟秉林,方芳.“慕课\"发展与大学人才培养模式改革[J].中国高等教育,2015,(21):24-28.  \n2LiyanagunawardenaTR， AdamsAA，WilliamsSA.MOOCs:Asystematic study of the published litera-ture 2008-2012[J]. The International Review of Re-search in Open and Distance Learning，2013,14(3):  \n202-227.  \n3曹婷,论慕课的发展现状及其对现代远程教育的启示[J].西部素质教育,2015,(17)24-26.  \n4 韩凤霞.高校\"慕课\"平台建设方法研究[J].科技资讯,2015,  \n13(22):116-117.  \n5林雪治,苏考辉.高校创新创业慕课平台建设的思路及策略[J].浙江树人大学学报(自然科学版),2016,16(1):49-53.  \n6周玲玲.高校图书馆慕课建设的理念理解偏差解读[J].图书馆学刊,2015,37(9):86-89.  \n7鄂丽君,张雪红,王启云.高校图书馆开展慕课服务的现状与对策[J].图书情报工作,2015,59(24):78-82,38.  \n8 刘峰峰.慕课环境下的高校信息素养教育[J].情报探索,2015,(3):52-55.  \n9艾兵.高校图书馆 $^ { \\dots } 3 + 2 + 1 ^ { \\dots }$ ”慕课化信息服务模式探索[J].图书情报工作 $, 2 0 1 6 , 6 0 ( 5 ) { : 2 5 - 3 0 }$   \n10 裴小琴,夏春明,杜龙兵,范圣法.基于MOOC的混合式教学模式初探——以华理校园慕课平台建设为例[].化工高等教育,2015,32(3):10-13,39.",
    "context2": "(责任编辑：孙晓明)"
  },
  "（上接第122页）": {
    "context1": "2015,(7):161-164.  \n7李彤,宋之杰.基于模型集成的突发事件舆情分析与趋势预测研究[].系统工程理论与实践,2015,35(10):2582-2587.  \n8 魏德志,陈福集,郑小雪.基于混沌理论和改进径向基函数神经网络的网络舆情预测方法[).物理学报,2015,64(11):44-51.  \n9 郭淼,焦垣生.网络舆情传播与演变背景下的微博信息转发预测分析[].情报杂志,2016,35(5):46-51.  \n10 兰月新,王芳,董希琳,等.公共危机事件网络舆情热度模型研究[J].情报科学,2016,35(2):32-36.  \n11 游丹丹，陈福集.基于改进粒子群和BP神经网络的网络舆情预测研究[].情报杂志,2016,35(8):156-161.  \n12 徐敏捷，兰月新,刘冰月.基于组合预测的网络舆情数据预测模型研究[].情报科学,2016,34(12):40-45.  \n13 陈福集,黄亚驹.基于SAPSO_RBF神经网络的网络舆情预测研究[].武汉理工大学学报(信息与管理工程版),2017,39(4):422-426.  \n14 李倩倩,姜景,李瑛,等.我国政务微博转发规模分类预测[].情报杂志,2018,(1):95-99.  \n15 张和平，陈齐海.基于灰色马尔可夫模型的网络舆情预测研究[].情报科学,2018,(1):75-79.  \n16 黄亚驹,陈福集,游丹丹.基于混合算法和BP神经网络的网络舆情预测研究[].情报科学,2018,(2):24-29.  \n17黄亚驹，陈福集.基于熵值法的网络舆情组合预测研究[].情报科学,2018,(3):70-74.  \n18 Bengio Y, Simard P,Frasconi P.Learning long-term de-pendencies with gradient descent is difficult.[J]. IEEE Trans-actions on Neural Networks,2002,5(2):157-166.  \n19Cho K,Van Merrienboer B,Gulcehre C,et al. LearningPhrase Representations using RNN Encoder-Decoder forStatistical Machine Translation[J].Computer Science,2014,(6):3130-3145.  \n20Sutskever I,Vinyals O,Le Q V. Sequence to SequenceLearning with Neural Networks[J].2014,(4):3104-3112.  \n21 Jozefowicz R,Zaremba W, SutskeverI.An empirical explo-ration of recurrent network architectures[C]//InternationalConference on International Conference on MachineLearningJMLR.org,2015:2342-2350.",
    "context2": "(责任编辑：孙晓明)"
  }
}