{
  "original_filename": "full_4307.md",
  "基于LDA的微博文本主题建模方法研究述评": {
    "context1": "张培晶」　宋　蕾²"
  },
  "中国人民公安大学办公室北京100038²北京警察学院公安科技系北京 102202": {
    "context1": "（摘要）在介绍概率主题模型发展过程以及概率主题模型的代表性模型LDA基本原理的基础上，分析LDA模型的特征及其用于微博类网络文本挖掘的优势;介绍和评述微博环境下现有的基于LDA模型的文本主题建模方法,并对其扩展方式和建模效果进行总结和比较;最后对微博文本主题建模的发展方向进行展望。",
    "context2": "(关键词）LDA 概率主题模型微博主题建模 （分类号）G356"
  },
  "Overview on Topic Modeling Method of Microblogs Text Based on LDA": {
    "context1": "Zhang Peijing'Song Lei²   \n1 Office of Chinese People’s Public Security University，Beijing 100038   \n²Department of Police Technology，Beijing Police College，Beijing 102202",
    "context2": "(Abstract）Basedonthedevelopmentprocesof probabilitytopic modelandbasicprincipleof isrepresenting model-LDA，thispaperanalyzesthecharacteristicsofLDAandadvantageofmicroblogs networktextmining.Thenitintroducesandcommentstheexisting texttopicmodelingmethodsbasedonLDAimicroblogsenvironment，andcomparestheirexpandedmodeandmodelingefectFialy it prospects the development direction of microblogs text topic modeling.",
    "context3": "(Keywords）Latent Dirichlet Alocation(LDA）probability topic modelmicroblogstopic modeling"
  },
  "1引 言": {
    "context1": "以LDA(latent dirichlet allocation)模型为代表的主题模型是近年来文本挖掘领域的一个热门研究方向。主题模型具有优秀的降维能力、针对复杂系统的建模能力和良好的扩展性。利用主题建模挖掘出的主题可以帮助人们理解海量文本背后隐藏的语义，也可以作为其他文本分析方法的输入，完成文本分类、话题检测、文本自动摘要和关联判断等多方面的文本挖掘任务。LDA主题模型已经在文本挖掘及其相关领域中得到了广泛应用，并且在以新闻类数据为主的传统网络文本挖掘方面获得了很大成功。",
    "context2": "微博(microb log)是基于Web2.0实现的社会媒体（socialmedia)的一种形式,近年来得到了爆发式的发展，已经成为人们表达思想、传播信息和交流学习的重要工具，被越来越多的用户和机构所关注。微博与传统网络文本有着显著不同，主要表现为：微博帖子的文本长度被限制在140个字符，文本多是只言片语，数据稀疏性问题严重;书写较为随意，使用语法不规范，网络用语、符号语言和新生词大量出现,数据噪声大；微博支持用户通过手机和网络等多种方式进行即时信息发布，具有更新速度快和文档数据规模庞大的特征。微博的上述特点给其主题建模带来了挑战。目前，有关主题模型在微博类社会媒体上的应用研究相对较少,尚处在探索阶段。但有关研究已经证明,不论是作为独一无二的使用特征,还是作为多重真实世界任务的补充特征，主题建模对于微博文本的挖掘都是非常有用的。"
  },
  "2 LDA 模型概述": "",
  "2.1概率主题模型的提出": {
    "context1": "海量文本的复杂性导致了基于主题的分层次统计模型的研究，产生了以LDA为代表的概率主题模型。相对于可以观察到的文档和词,主题是一个抽象的概念,代表了一个潜在的语义主旨(subject）。概率主题模型本质是通过对文本中词的分布规律的观察，实现对相似分布规律词集的聚类。主题相当于聚类中的簇，文档以不同的概率属于不同的主题。",
    "context2": "概率主题模型的前身可以追溯到LSA（latent se-mantic analysis）潜在语义分析。LSA打破了人们以往基于“词典空间”进行文本表示的思维模式，创新地引入了语义维度，实现了文档在低维的隐含语义空间上的表示。但是，LSA的方法论基础来源于线性代数，是基于奇异值分解 SVD( singular value decomposition)的单词计数原始矩阵的近似分解。由LSA获取的概念层表示无法处理“一词多义\"问题。同时,SVD涉及到矩阵运算，计算复杂度较高，而且矩阵运算的结果在很多维度上为负数，没有对应的物理解释。霍夫曼针对LSA的缺陷提出了一个构建在可靠的概率统计学基础之上的新方法 PLSA(probabilistic latent semantic analy-sis）。此前,每个语义维度对应一个特征向量,而在概率模型中，每个语义维度对应一个词典上的概率分布。PLSA可以明确地区分单词使用的不同意思和不同类型,解决了“一词多义”问题。但PLSA并没有在文档层提供概率模型，这导致了模型中待估参数的数量随着语料库的大小呈线性增长，容易出现过度拟合问题。2003年,D.Blei等人通过引入一个Dirichlet先验分布扩展了PLSA模型,克服了PLSA参数随着文档集增长而线性增长的不足，形成了当前被广泛应用的产生式概率主题模型LDA。至此，主题模型被第一次正式提出。"
  },
  "2.2 LDA 模型的描述": {
    "context1": "LDA 模型是一个分层的贝叶斯模型,包含文档、主题和词三个层次。LDA 模型基本思想是每个文档都可以表示成若干潜在主题的混合分布，每个主题是词汇表中所有单词的概率分布。LDA模型将主题混合权重θ视为 $\\mathrm { T }$ 维参数的隐含随机变量，并对主题混合权重θ引进Dirichlet先验。Dirichlet分布的公式为：",
    "context2": "$$\n\\mathrm { D i r } ( \\mathbf { \\alpha } _ { \\alpha _ { 1 } } \\cdots , \\mathbf { \\alpha } _ { \\alpha _ { \\mathrm { r } } } ) \\mathbf { \\beta } = \\frac { \\Gamma ( \\sum _ { \\mathbf { \\alpha } _ { \\mathrm { j } } } \\mathbf { \\alpha } _ { \\alpha _ { \\mathrm { j } } } ) } { \\prod _ { \\mathbf { \\alpha } _ { \\mathrm { j } } } \\Gamma ( \\mathbf { \\alpha } _ { \\alpha _ { \\mathrm { j } } } ) } \\prod _ { \\mathbf { \\alpha } = 1 } ^ { \\mathrm { T } } \\mathbf { \\alpha } _ { \\mathbf { \\alpha } } ^ { \\alpha _ { \\mathrm { i } } - 1 }\n$$",
    "context3": "参数 $\\alpha _ { 1 } \\bullet \\cdots \\bullet \\alpha _ { \\mathrm { T } }$ 均为多项式 $\\boldsymbol { \\theta } = \\mathrm { P } ( \\mathrm { \\theta } _ { 1 } , \\cdots , \\mathrm { \\theta } _ { \\mathrm { T } } )$ 的超参数，每个超参数 $\\alpha _ { \\mathrm { j } }$ 可以被理解为从一个文本中抽样主题j的次数的一个先验。考虑不同的主题被利用的方式几乎没有变化,为方便计算,LDA假定文本中的主题是可交换的，采用具有相同超参数 $\\alpha$ 的对称Dirichlet分布,这里 $\\alpha _ { 1 } = \\alpha _ { 2 } = \\cdots = \\alpha _ { \\mathrm { T } } = \\alpha \\alpha _ { \\mathrm { T } }$ 。Dirichlet 分布和多项式分布是共轭分布。若 $\\mathrm { T }$ 维随机向量服从",
    "context4": "Drichlet分布,则的 $\\mathrm { T }$ 个分量 $\\Theta _ { 1 } \\bullet _ { 1 } \\bullet _ { 2 } \\bullet \\cdots , \\bullet _ { \\mathrm { T } }$ 都是连续的非负值,且Σ $\\mathbf { \\rho } _ { \\mathrm { j } } \\Theta _ { \\mathrm { j } } = 1 \\mathbf { \\rho } ^ { \\perp }$ 。D.Blei最早提出的原始 LDA文献中只对文档－主题分布θ增加了Dirichlet先验,并没有对主题－词汇分布 $\\boldsymbol { \\Phi }$ 进行先验假设。为了充分利用共轭概率分布的特性，便于推理运算，Giffiths等人在后续的研究中对 $\\boldsymbol { \\Phi }$ 加上了Dirichlet先验,并假定主题中的单词是可交换的，其超参数为 $\\beta$ 。至此，LDA中有了两组先验。一组是文档－主题的先验，来自一个对称的Dirichlet(α)；另一组是主题-词汇的先验,来自一个对称的Dirichlet（β）。",
    "context5": "2.3 LDA 模型的特征及其用于微博文本建模的优势",
    "context6": "LDA主题模型具有优秀的降维能力，可以将原来高维的单词空间降维到由一组主题构成的相对较小的主题空间上。降维技术不仅可以有效降低文本相似性计算的复杂度,还可以避免传统文本建模方法存在的数据稀疏性等问题。对于微博类的短文本而言，文本中单词非常有限,同一个词共现在两篇不同短文本中的概率较小，使用传统以词或短语为特征的向量表示方法，很难准确计算文本间的相似度。利用主题模型可以将每个短文本表示成低维的主题空间上的一个向量。低维的主题向量的空间则不会是稀疏的，有助于隐含的相关性被挖掘出来，从而减少数据稀疏性对度量文本之间相似度的影响，更充分地挖掘文本集合的内在信息。",
    "context7": "LDA主题模型具有扎实的概率理论基础。一方面,LDA模型使用概率统计方法来分析文本，是建立在词袋(bagofwords)假设之上的。词袋假设认为一篇文章可以表示为一组单词的无序组合,而忽略了文章中的语法和单词的次序。与基于语法规则的语言学的文本分析方法相比，LDA通过对文本中词汇共现次数的概率统计来挖掘潜在的语义结构，更适合用于存在着大量语法不规范问题的微博类网络文本的建模和分析;另一方面,LDA模型使用概率分布方法来表示文本,主题是单词表中词的随机分布概率，文本以不同的概率属于不同的主题，服从真实数据的概率分布。概率分布表达方法能够较好地表达和量化文本中存在的不确定性，减小噪声干扰。对于语言组织规范性差、新生词汇大量出现的微博类网络文本而言，以LDA为代表的概率主题模型更适合微博类网络文本挖掘过程中不确定性环境下相对准确的计算。",
    "context8": "LDA主题模型具有良好的扩展能力。LDA模型属于贝叶斯网络模型,可以方便地把各种元数据、结构化信息甚至领域知识作为贝叶斯网络中的随机变量加入到主题模型中;也可以将两个主题模型合并,以搭积木的方式形成一个新的主题模型。微博具有多种非文本特征信息。例如,不同类型微博(按照不同的消息发布方式,微博可以分为广播类（broadcast）、对话类（conversion)和转发类(retweet)3种类型)的结构化特征,使用哈希标签(Hashtag)（微博文本中的Hashtag标签是一种用于简化搜索、索引和趋势发现的用户自定义标签,格式为“#话题名#\"（中文微博)或“#话题名”（twitter)）,具有用户特征和时间特征等。此外,微博也是一种社会关系网络,带有一些结构化的社会网络方面的信息。利用LDA模型良好的扩展性，可以将微博中的这些特征信息引入到LDA模型中,实现更准确的微博文本建模和主题挖掘。",
    "context9": "综上所述,相对于其他文本建模方法,应用LDA主题模型进行微博类网络文本的建模和分析具有很大的优势。但是直接使用传统LDA模型对微博进行主题建模，一定程度上仍然受到篇幅过短、内容和格式散乱、数据噪声较大等方面的影响。目前，已经有一些基于LDA的改进方法用于微博环境下的主题建模。"
  },
  "3 微博文本主题建模方法介绍": {
    "context1": "3.1直接使用LDA模型进行主题建模",
    "context2": "3.1.1直接使用LDA模型对微博帖子进行主题建模将每个微博帖子对应为单个文档，可以直接使用LDA模型对其进行主题建模。由于LDA模型是无监督的,此方法实现过程最为简捷。但有关研究证明，LDA主题模型的效力很大程度上受文档长度的影响，一个短文本缺少足够的词出现次数,无法帮助判断是否这些词是相关的。因此,这种方法的微博主题建模效果并不理想。",
    "context3": "3.1.2基于聚集的LDA 主题建模方法社会媒体中本身存在着一定的“聚集策略”，按照这些策略可以将一系列存在某种关联关系的微博帖子聚集为一个长文档,然后使用传统的LDA模型在此聚集后的长文档上进行主题建模。常用的聚集策略是按照发布者进行聚集,即将同一微博用户的所有帖子聚集为一个文档。该方法得到的主题－单词分布 $\\boldsymbol { \\Phi }$ 和文本上的主题分布θ都是微博用户层面的,可以用于微博用户及其感兴趣主题的分析。WengJianshu等人使用这种方法有效实现了敏感主题有影响力微博博主的发现"
  },
  "3.1.3基于训练的LDA 分步建模方法一些情况下,用户希望实现针对微博帖子的分析,例如进行帖子": {
    "context1": "过滤、帖子分类等。如何在解决短文本信息不足的前提下,实现这一目标呢？HongLiangjie等人提出了通过预先训练来分步完成的方法，本文将其总结为基于训练的LDA分步建模方法，其中具有代表性的实现模式有用户模式（user scheme）和术语模式（termscheme）。基于训练的LDA分步建模方法的基本思想是先在聚集后的长文本数据集上进行LDA模型的训练,学习得到合理的主题，也就是挖掘出主题在词汇上的分布 $\\boldsymbol { \\Phi }$ ；然后再用训练好的模型去推导训练集上原始的短文本或等待测试的新文本的主题分布θ,在此推导过程中，并不再进行主题 $\\boldsymbol { \\Phi }$ 的重新训练，令长文本,训练=φ帖子,测试=φ帖子,训练=φ长文本,测试=φ。笔者认为,这种方法本质上是将LDA模型的 $\\boldsymbol { \\Phi }$ 和的求解过程通过先期训练的方式进行人为的拆分,从而在应用中可以根据文本的实际情况，选择合理的解释性强的主题进行统一的文本主题分步推导,同时也简化了 $\\boldsymbol { \\Phi }$ 的求解。但这种方法也存在主题模型僵化的问题，当有新的文档来到时，若要更新模型仍需重新训练。另外,建模过程也需要大量的文本预处理工作和人工干预。",
    "context2": "HongLiangjie等人提到的用户模式是将训练数据集中相同用户的帖子聚集为一个长文本，然后进行LDA模型的训练;术语模式是将训练数据集中具有某个相同术语的帖子聚集为一个长文本,再进行LDA 模型的训练。术语模式设计的出发点是考虑到微博用户经常使用自定义的Hashtag标签来标示主题或事件,构建术语聚集文本可以使我们直接获得与这些Hashtag标签相关的主题。"
  },
  "3.2使用ATM模型进行主题建模": {
    "context1": "ATM(author-topic model）d 是一种基于 LDA 的扩展模型，旨在通过引入文章的作者信息来指导LDA主题生成,最初用于对维基百科进行建模。实验表明，当测试文档中仅仅有小数量的单词被观察到时，该模型胜过LDA。不同于LDA的是,模型中“文本-主题\"的分布被“作者－主题\"分布所取代。由于每个微博帖子只有一名作者,微博上的ATM主题建模实际上是ATM模型的一个特例。ATM建模得到的是微博用户层面的主题混合分布，而不是微博帖子层面的主题混合。经过某些扩展或特殊的处理,扩展后的ATM也可以支持同时对微博帖子和用户进行主题建模[ 。"
  },
  "3.3使用Twitter-LDA模型进行主题建模": {
    "context1": "ATM模型较好地解决了微博数据稀疏性问题，但同时也降低了针对单个帖子的文本分析能力。考虑到一个较为普遍的观测规律：一个单独的微博帖子通常只有一个单一主题,Zhao Wayne Xin 等人对ATM模型进行了扩展,提出了Twitter-LDA 模型[。Twitter-LDA模型与ATM模型的主要区别是、Twitter-LDA不仅可以在微博用户层面进行主题建模，而且可以对单个的微博帖子进行主题建模。",
    "context2": "Twitter-LDA模型在ATM基础上有两处变化：一是引入了背景模型 $\\varphi _ { \\mathrm { B } }$ ,可以有效地降低高频词汇的影响。类似“love”、“thanks\"这些出现频率极高的词汇如果不被处理,会对文本挖掘造成严重干扰;二是给每个微博帖子内部的所有单词赋予一个统一的主题,可以实现对用户和帖子两个层面的同时建模。此外,Twit-ter-LDA还可以方便地开展帖子层面的数据统计。例如,可以计算语料集合中有多少帖子与某主题相关以及一对单词在某主题相关的帖子中共现的次数。"
  },
  "3.4使用LabeledLDA模型进行主题建模": {
    "context1": "考虑微博作为一种网络文本，很多数据已经被读者贴上了标签(tags），利用这些已经存在的标签资源，有助于更好地进行主题挖掘,D.Ramage等人提出了使用Labeled LDA 对微博文本进行主题建模[。LabeledLDA最早是D.Ramage等人[在LDA基础上提出的一个受监督的主题模型，可以描述一个标签化文档集的产生过程。与LDA不同的是,LabeledLDA 通过对主题模型的一个简单约束来引入监督，即仅仅使用那些与一个文档可以观察到标签集合相对应的主题。该模型学习得来的主题直接与每个标签对应关联。La-beledLDA的优势在于通过提供一个标签集（LabelSet)到其学习过程中，使其学习得到的主题更具有解释性，也可以用于解决文本分类中的可信归属问题。",
    "context2": "D.Ramage等人将微博中的哈希标签对应为La-beledLDA中标签(Label）,使用LabeledLDA模型在微博上进行主题建模,有效利用了微博上已有的用户自定义主旨信息，较为准确地找出了与每个哈希标签密切关联的词汇。另外,还可以使用主题模型对包含大量简易符号的微博文本进行建模，实现了符号标识的主题挖掘。例如，挖掘出符号标识：)”与 thanks、thank、much、too、hi、following、love 等单词存在密切关联。还需指出,Labeled LDA 克服了 Supervised LDA 和DiscLDA[等早期引入监督的主题模型限制一个文档只可以与一个单一的标签关联的缺点,允许对多标签的语料集进行建模。因此,LabeledLDA也支持在微博帖子的聚集文档之上的主题建模，聚集后的文档允许包含多个哈希标签。D.Quercia 等人[将基于微博帖子聚集文档上的LabeledLDA主题建模方法用于微博用户文档(Twitterprofiles)的分类,并通过实验证明，该方法能够在较少训练数据的情况下，实现较为准确的微博用户分类,其分类性能远远胜过支持向量机(sup-port vectormachines,SVM)的线性分类方法。"
  },
  "3.5使用 MB-LDA模型进行主题建模": {
    "context1": "国内中文微博类似于国外Twitter的引入，但是二者除使用文字和语言结构的不同之外,在其他方面也存在差异:一方面,中文微博在Twitter基础上进行了简单的扩展。例如,较早支持了对话评论,并且转发微博的同时也可以加入评论,而Twitter只能转发，不能同时加入评论;另一方面,国内微博媒体的“转发”类微博所占比例相当大，频次要高出Twitter几个数量级。考虑到中文微博的上述独有特征,结合微博文本内蕴含的社会网络方面的结构化信息，张晨逸等人在LDA基础上提出了专用于中文微博主题建模的MB-LDA模型 。",
    "context2": "MB-LDA在原有LDA的基础上作了两方面的扩展，分别将微博的文本关联关系和联系人关联关系引入到微博主题建模中。对于转发类微博(一般格式为“评论 $\\mathrm { R T } @$ 作者原文”）,将微博的转发部分和原文部分关联起来进行主题建模;对于对话类微博（以 $^ { 6 6 } @$ 联系人\"格式开头），将本次发布内容与联系人的微博内容关联起来进行主题建模。",
    "context3": "张晨逸等人通过对衡量主题模型性能和推广性的perplexity指标的计算发现,MB-LDA模型在中文微博上的主题挖掘功能明显优于传统的LDA模型。另外，通过MB-LDA模型不仅可以挖掘出每条微博可能所属的主题以及每个主题代表性的词，还可以推导出联系人的主题概率分布0。,从而挖掘出每个联系人感兴趣的主题。"
  },
  "4 微博文本主题建模方法比较": {
    "context1": "上述微博文本主题建模方法主要表现为LDA模型两个方向的扩展：一个方向是纵向的基于操作过程的扩展;另一个方向是横向的基于模型的扩展。"
  },
  "4.1基于操作过程的扩展": {
    "context1": "在基于操作过程的扩展方面，不论是基于聚集的LDA主题建模方法,还是基于训练的LDA分步建模方法，都需要预先进行短文本的聚集，与利用ATM模型在微博上进行主题建模的基本思想是类似的。但这几种主题建模方法产生的主题彼此间却存在很大的不同。HongLiangjie等人对微博帖子上的LDA建模（文献中称为消息模式)、ATM的扩展模型建模(同时获得用户和帖子两个层面）、基于训练的用户模式建模和术语模式建模共4种方式产生的主题进行了对比实验分析。实验表明,在主题的一致性方面,不同建模方法学习得到的主题并不直接相一致。对于不同建模方法得到的相似主题,使用 JS(Jensen-Shannon divergence)算法计算相似主题构成单词的概率是各自不同的，并且这种差异性会随着主题数量的增加而略微增加;使用肯德尔(Kendall)等级相关系数去测量相似主题构成单词的排名也发现存在一定差别。通过上述两项指标的测量,可以推导出,通过不同模式或模型学习的主题,它们之间是存在明显区别的。相比较其他模式,消息模式和用户模式之间的分歧更加突出，而通过术语模式和ATM模型学习的主题更接近消息模式学习的主题。在建模性能方面，在聚集后的长文本上训练的模型能够产生更好的性能，其中基于训练的用户模式能够实现训练过程更快和训练质量更好。但当文本长度过于大时，主题模型反而变得低效。D.Quercia等人在使用Label-LDA用于微博用户分类时也曾指出，中等活跃度用户的帖子聚集后形成的微博用户文档(TwitterProfile)长短适中，其分类性能和效果最理想。在主题数量方面，消息模式由于微博帖子数量巨大，与其他三种方式相比,会获得更大数量的主题。另外,通过ATM的扩展模型学习产生的针对帖子的主题,通常数量很少，并且"
  },
  "比消息模式获得的结果还糟糕。": "",
  "4.2基于模型的扩展": {
    "context1": "在基于模型的扩展方面，ATM和Twitter-LDA都是利用微博文本的用户特征来实现LDA模型的扩展。只是Twitter-LDA在ATM基础上作了进一步扩展,引入背景模型，并实现了用户层面和帖子层面的同时建模。本质上,MB-LDA利用文本结构化信息也实现了微博短文本的扩展，只是这种扩展是依据微博的具体类型而区别对待的。Labeled-LDA是通过引入微博中结构化的标签信息,将原先无监督学习的LDA模型变为监督型的模型，有效提高了主题建模效率和主题可解释性。在主题挖掘效果方面，从已有文献资料推断，相比直接在微博帖子之上的LDA主题建模，上述扩展后的主题模型更具优势;但多数扩展后的模型相互之间并无绝对的孰优孰劣，其建模效果与应用目标有着很大关系。例如，对于用户感兴趣主题的挖掘，ATM模型较为简单实用;对于包含足够标签信息的微博主题挖掘和文本分类而言，Labeled-LDA更有优势;对于转发频次很高的中文微博的主题建模，MB-LDA适用性更强;而Twitter-LDA可以同时实现对用户和帖子两个层面的建模,并且Zhao Xin等人通过实验证明,与传统LDA模型和ATM模型相比较，该模型挖掘的某个主题的前几个单词具有更好的关联性和可解释性[。上述讨论的各种建模方法的比较情况,如表1所示:",
    "context2": "表1微博上的主题建模方法比较",
    "context3": "<table><tr><td rowspan=2 colspan=1>模型/模式</td><td rowspan=1 colspan=2>主题分布层面</td><td rowspan=2 colspan=1>扩展方式</td><td rowspan=2 colspan=1>实现方式</td><td rowspan=2 colspan=1>优势</td><td rowspan=2 colspan=1>局限性</td></tr><tr><td rowspan=1 colspan=1>微博用户</td><td rowspan=1 colspan=1>微博帖子</td></tr><tr><td rowspan=1 colspan=1>LDA（消息模式）</td><td rowspan=1 colspan=1>香</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>无</td><td rowspan=1 colspan=1>直接使用</td><td rowspan=1 colspan=1>无需监督</td><td rowspan=1 colspan=1>主题挖掘不理想</td></tr><tr><td rowspan=1 colspan=1>基于用户聚集的LDA</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>否</td><td rowspan=1 colspan=1>过程扩展</td><td rowspan=1 colspan=1>文本聚集</td><td rowspan=1 colspan=1>解决短文本问题</td><td rowspan=1 colspan=1>只限微博用户层面建模,需要人工干预</td></tr><tr><td rowspan=1 colspan=1>基于训练的USER模式</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>否</td><td rowspan=1 colspan=1>过程扩展</td><td rowspan=1 colspan=1>文本聚集、分步求解</td><td rowspan=1 colspan=1>解决短文本问题,简化推导</td><td rowspan=1 colspan=1>需要事先训练和人工干预，若要更新模型需重新训练</td></tr><tr><td rowspan=1 colspan=1>基于训练的TERM模式</td><td rowspan=1 colspan=1>否</td><td rowspan=1 colspan=1>否</td><td rowspan=1 colspan=1>过程扩展</td><td rowspan=1 colspan=1>文本聚集、分步求解</td><td rowspan=1 colspan=1>解决短文本问题，简化推导，提高主题可解释性</td><td rowspan=1 colspan=1>需要事先训练和人工干预,若要更新模型需重新训练，要求文本具有标签信息</td></tr><tr><td rowspan=1 colspan=1>ATM[[g</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>否</td><td rowspan=1 colspan=1>模型扩展</td><td rowspan=1 colspan=1>文本聚集</td><td rowspan=1 colspan=1>解决短文本问题</td><td rowspan=1 colspan=1>只限微博用户层面主题建模</td></tr><tr><td rowspan=1 colspan=1>ATM扩展模型</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>模型扩展</td><td rowspan=1 colspan=1>文本聚集</td><td rowspan=1 colspan=1>解决短文本问题</td><td rowspan=1 colspan=1>挖掘出的帖子层面主题少且不理想</td></tr><tr><td rowspan=1 colspan=1>Twitter-LDA[[g</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>模型扩展</td><td rowspan=1 colspan=1>文本聚集，引入背景模型</td><td rowspan=1 colspan=1>解决短文本问题和高频词汇问题</td><td rowspan=1 colspan=1>一个帖子只能对应一个主题</td></tr><tr><td rowspan=1 colspan=1>Labeled-LDA</td><td rowspan=1 colspan=1>否</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>模型扩展</td><td rowspan=1 colspan=1>引入标签信息</td><td rowspan=1 colspan=1>提高主题可解释性</td><td rowspan=1 colspan=1>要求文本具有足够的标签信息</td></tr><tr><td rowspan=1 colspan=1>MB-LDA [</td><td rowspan=1 colspan=1>部分可以</td><td rowspan=1 colspan=1>是</td><td rowspan=1 colspan=1>模型扩展</td><td rowspan=1 colspan=1>引入结构化信息</td><td rowspan=1 colspan=1>解决短文本问题，提高主题可解释性</td><td rowspan=1 colspan=1>主要针对会话类和转发类中文微博</td></tr></table>"
  },
  "5展望": "",
  "微博环境下的主题建模研究仍处于开始阶段，还": {
    "context1": "有许多需要研究解决的问题和难点。本文通过对微博环境下主题建模方法进行梳理发现，现有的微博文本主题建模方法主要研究解决微博文本主题建模的适应性问题。例如,LDA 基于过程的扩展和ATM、Twitter-",
    "context2": "LDA等模型解决了微博的短文本数据稀疏问题,La-beled-LDA一定程度上降低了微博文本语法不规范的干扰。然而,面对海量微博文本信息的快速更新,如何实现主题模型在微博环境下的大规模部署和在线学习训练,有待进一步研究解决。为此,引入和探索更加高效的主题模型训练算法具有现实意义，有助于微博环境下主题建模由理论性向实用性的转变。例如，可以借鉴R.Nallapati等人提出的并行变分期望最大化算法、A.Asuncion 等人提出的LDA 模型分布式Gibbs采样算法等主题模型推导方法，将其应用于多处理器和分布式环境,加速微博环境下主题模型的训练。",
    "context3": "另外,从功能性角度来分析微博环境下主题建模能够发现，现有的微博文本主题建模主要用于微博的内容分类、信息过滤、用户推荐等方面的商业应用，以提高内容质量，改善用户体验;而对于社会媒体舆情监测方面的应用研究仍较少涉及。主题模型最主要的功能是抽取出文本的语义主题，可以直接用于文本话题挖据。话题检测和话题跟踪紧密关联，是网络舆情监测的重要内容。路荣等人已经对主题模型应用于微博类新闻话题的自动识别做了一些研究，但尚未涉及微博文本的话题跟踪演化分析。目前,已经有一些成熟的应用于话题演化分析的LDA扩展模型。例如，Wang等人通过在LDA模型中引入了作为观测值的时间随机变量，得到了一个主题随时间变化的主题模型TOT(topic over time）;D.Blei等人将当前时刻的模型参数后验作为下一时刻模型参数的条件分布引入主题模型，提出了动态主题模型DTM（dynamic topicmode）。将此类LDA模型的扩展应用于微博类新闻话题的演化分析，从而实现从海量微博文本信息中快速准确地追踪话题的演化，并根据演化及时做出相应的预测，为安全机构的领导决策提供支持，将会是微博环境下主题建模的一个重要研究方向。此外,在网络舆情监测领域的另一个研究热点-文本情感(倾向性)分析方面，也有主题模型的扩展研究。例如,LinChenghua等人提出的联合情感话题模型JST(joint sen-timent/topic model）能够在无监督的情况下,抽取出文本主题,并同时实现文档级的情感检测;MeiQiaozhu等人提出的主题情感混合模型TSM（topic-sentimentmixture）可以将抽取出的主题有关的单词分为中性、正面和负面三类，支持话题级的情感检测和情感随时间的动态演化分析。研究者也可以将此类主题模型应用到微博信息的舆情监测和分析中。总而言之，面对LDA模型良好的扩展能力和已有的丰硕研究成果，"
  },
  "微博类社会媒体环境下的文本主题建模还有很多方面需要改进。": "",
  "今考义：": {
    "context1": "[1]Blei D,Ng A,Jordan M.Latent Dirichlet allocation $[ [ ]$ .Journal of Machine Learning Research,2003(3) :993-1022.   \n[2]Hong Liangjie,Davison B.Empirical studyof topic modeling in Twitter [// Proceedingsof the First Workshopon Social Media Analytics( SOMA'10).New York: ACM Press,2010:80 -88.   \n[3] Deerwester S,Dumais S,Landauer T,et al. Indexing by latent semantic analysis ]．Journal of the American Society for Information Science,1990,41(6) :391 -407.   \n[4]Hofmann T.Unsupervised learning by probabilistic latent semantic analysis [．Machine Learning,2001,42(1) :177 -196.   \n[5]Steyvers M,Grfiths T.Probabilistic topic models $\\mathbb { M }$ / /Landauer T,McNamara D,Dennis S,et al. Latent Semantic Analysis:A Road to Meaning. Mahwah: Lawrence Erlbaum Associates,2007: 424 -440.   \n[6]Grffths T,Steyvers M.Finding scientific topics//Proceedings of the National Academy of Sciences.Washington D.C.:United States National Academy of Sciences,2004:5228 -5235.   \n[7]Tang Jie,Jin Ruoming,Zhang Jing.Atopic modeling approach and its integration into the random walk framework for academic search [//Proceedings of the 2O08 Eighth IEEE International Conference on Data Mining( ICDM‘O8）．Washington: IEEE Computer Society,2008:1055-1060.   \n[8]Lu Yue，Zhai Chengxiang.Opinion integration through semi -supervised topic modeling $\\mathbb { H }$ //Proceedings of the 17th International Conference on World Wide Web.（WWW‘O8)．New York: ACM Press,2008:121 - 130.   \n[9] Weng Jianshu,LimEe-Peng,Jiang Jing,etal.Twiterrank:finding topic- sensitive influential Twitterers $\\mathbb { H }$ // Proceedings of the 3rd ACM International Conference on Web Search and Data Mining （WSDM'10).New York:ACM Press,2010:261-270.   \n[0]Zvi M,Griffths T,Steyvers M,etal.Theauthor-topic model for authorsand documents[C]//Proceedings of the 2Oth Conference on Uncertainty in Artificial Inteligence（UAI'O4）．Arlington: AUAI Press,2004: 487 -494.   \n[1]Zvi M,Chemudugunta C,Grifths T,etal.Learning author-topic models from textcorpora[].ACM Transactions on Information Systems,2010,28(1) :1-38.   \n[2] Zhao Wayne Xin，Jiang Jing，Weng Jianshu，et al.Comparing Twiter andtraditional media using topic models[d// Proceedings of the 33rd European Conference on Information Retrieval(ECIR ' 11).Berlin，Heidelberg: Springer- Verlag，2011:338 -349.   \n[3]Ramage D,Dumais S,Liebling D. Characterizing microblogs with topic models [C // Proceedings of International AAAI Conference on Weblogs and Social Media （ICWSM‘10).Menlo Park，CA: AAAI,2010:130-137.   \n[14]Ramage D,Hall D,Nallapati R，et al.Labeled lda:A supervised topic model for credit attribution in multi-labeled corpora $\\mathbb { C }$ $/ /$ Proceedings of the 2Oo9 Conference on Empirical Methods in Natural Language Processing(EMNLP'O9).Stroudsburg:Association for Computational Linguistics,2009:248 -256.   \n[5]Lacoste - Julien S,Sha F,Jordan M. DiscLDA:Discriminative learning for dimensionality reduction and classification $\\mathbb { C }$ //Proceedings of Neural Information Processing Systems Conference (NIPS'08)．Vancouver:NIPS,2008:897-904.   \n[6]Quercia D,Askham H,Crowcroft J. TweetLDA:Supervised topic classification and link prediction in Twitter [C]//Proceedings of the 3rd Annual ACM Web Science Conference.New York:ACM Press,2012:247 -250.   \n[7] Yu L,Asur S,Huberman B.What trends in chinese social .edia [C] //Proceedings of the Fifth International Workshop on Social Network Mining and Analysis(SNA -KDD’11)．San Diego: KDD Press,2011:37."
  },
  "[18]张晨逸,孙建伶,丁轶群.基于MB-LDA 模型的微博主题挖掘[.计算机研究与发展,2011（10)：1795－1802.": {
    "context1": "[19]Zhao Xin,Jiang Jing，He Jing，etal.Topical keyphrase extraction from Twiter [Cl// Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies（ACL-HLT'11），Portland:ACL Press,2011:379 - 388.   \n[o]Nallapati R,Cohen W,Lafferty J.Paralelized variational EMforlatent dirichlet allocation:An experimental evaluation of speed and scalabiliy//Proceedings of the Seventh IEEE International Conference on Data Mining Workshops（ICDMW'O7)．Washington D. C.:IEEE Computer Society,2007:349 -354.   \n[1]Asuncion A,Smyth P,Weling M.Asynchronous distributed learning of topic models [] $/ /$ Proceedings of the Twenty- Second Annual Conference on Neural Information Processing Systems.New York: Curran Associates,Inc,2008:81 -88.   \n[2]路荣,项亮,刘明荣,等.基于隐主题分析和文本聚类的微博客 新闻话题发现研究[//中国中文信息处理学会.第六届全国 信息检索学术会议论文集.北京：中国中文信息处理学会， 2010:291-298.   \n[3] Wang X，McCallum A. Topic over time: A non- markov continuous -time model of topical trends [C] //Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD'06).New York:ACM Press,2006:424- 433.   \n[4]Blei D,Laferty J.Dynamic topic model [C] //Proceedings of the 23rd International Conference on Machine Learning.New York: ACM Press,2006:113 -120.   \n[5] Lin Chenghua，He Yulan. Joint sentiment/topic model for sentiment analysis [C // Proceeding of the 18th ACM conference on Information and Knowledge Management.New York:ACM Press, 2009:375 -384.   \n[26]Mei Qiaozhu,Xu Ling，Wondra M,et al.Topic sentiment mixture: Modeling facets and opinions in weblogs [Cl // Proceedings of the 16th International Conference on WorldWide Web.New York: ACM Press,2007:171-180.",
    "context2": "(作者简介）张培晶，男，1979年生，讲师，发表论文10余篇。宋蕾，女,1980年生，讲师,发表论文10余篇。"
  },
  "(上接第76页)": {
    "context1": "[14] Xu W X. Zipf's law and mechanism of distribution of Chinese term frequency [Cl //Proceedings of the 2nd International Conference on Bibliometrics，Scientometrics and lnformetrics,London，1989:332 -338.",
    "context2": "[15]Le Quan Ha,Hanna Philip Ming Jik,Simth F J.Extending Zipf's law to n-grams for large corpora [.Artificial Intelligence Review, 2009(32):101 -113."
  },
  "[6]杨波,阎素兰．齐普夫定律的汉语适用性研究及其在自动标引中的应用[.情报理论与实践,2004(3):252-255.": {
    "context1": "[7]Hill B. The rank-frequency form of Zipf's law [] .Journal of the American Statistical Association，1987,69(348):1017 - 1026. [18] Cancho RF,Solé RV.Two regimes in the frequency of words and",
    "context2": "the origins of complex lexicons:Zipf's law revisited $\\mathbb { I }$ . Journal of Quantitative Linguistics, $2 0 1 0 , 3 \\mathopen { } \\mathclose \\bgroup \\left( \\mathrm { ~ 8 } \\aftergroup \\egroup \\right) : 1 6 5 - 1 7 3$   \n[9] Cancho RF,Sole RV. Zipf's law and random texts[]．Advances in Complex Systems,2002, $5 ( 1 ) : 1 - 6$   \n[0] Adamic L A．Zipf，power-law，pareto-a ranking tutorial [EB/ O1]．[2011 -02-22]．http://www.hpl.hp.com/research/idl/ papers/ranking，2011.   \n[1]Rouseau R,Zhang Qiaoqiao. Zipf's data on the frequency of Chinese words revisited []．Scientometrics，1992，24(2):201 - 220.   \n[22]游荣彦．Zipf定律与汉字字频分布[].中文信息学报，2000, 14(3) :60 -65.",
    "context3": "(作者简介）路高飞，男，1988年生，硕士研究生，发表论文3篇。韩普,男,1983年生,博士研究生,发表论文10 篇。沈思，女，1983年生，博士研究生，发表论文7篇。"
  }
}