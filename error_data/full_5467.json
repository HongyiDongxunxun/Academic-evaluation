{
  "original_filename": "full_5467.md",
  "nothing": {
    "context1": "$\\bullet$ 陈果，姚游倩（南京理工大学经济管理学院，江苏南京210094)"
  },
  "情报学研究的可靠性评估：内涵、进展与对策\"": {
    "context1": "摘要：［目的/意义」合理地评估一项情报学研究的可靠性，是情报学科对内建设自我优化体系、对外提高学术话语权的基础支撑，亦是适应情报学朝向模型化、智能化、工程化等新趋势发展的必然要求。对研究理论、方法和结果的可靠性评估一直是情报学研究中的关键环节，忽视可靠性评估会导致情报学研究质量参差不齐、应用性不足等问题。[方法/过程］厘清情报学研究可靠性评估的内涵，梳理该方面研究进展，正视现有情报学研究中可靠性评估环节的不足，并探索有针对性的解决之道。［结果/结论]在情报学科未来发展中，急需在学科范围内强化研究人员的可靠性评估意识，加强学界和业界联动，从资源、流程、方法等层面推进情报学研究可靠性评估体系规范化的建设，探索情报学可靠性研究评估工作的“特色化”，以形成高效的自审、自证、自优的情报学研究机制，提升学科话语权。",
    "context2": "关键词：情报学研究；可靠性评估；情报评估；情报学学科发展",
    "context3": "DOI:10.16353/j.cnki.1000-7490.2023.04.010",
    "context4": "引用格式：陈果，姚游倩．情报学研究的可靠性评估：内涵、进展与对策［J]．情报理论与实践，2023，46（4)：74-81."
  },
  "Reliability Assessment of Information Science Research: Implications，Progress ，and Countermeasures": {
    "context1": "Abstract:[Purpose/significance]Areasonableassssmentof thereliabilityof an intellgence research studyisthe basicsupportfortheintellgencedisciplinetobuildaself-optimizingsysteminterallyandimproveitsacademicdisouseexteally，andit isalsoaninevitablerequirementtoadapttothedevelopmentofintellgencesciencetowardsnewtrendssuchasmodeling，iteligence，andenginering.Theassessmentofthereliabilityofresearchmethodsandresultshasalwaysbenakeyaspectof iteligenceresearch，andtheneglectofeliabilityassessment willeadtoproblemssuchasunevenqualityandinsuficientapplicability of intelligenceresearch.Method/process]Thispaperclarifiesthemeanngofeliabilityassessmntinintellgenceresearch，reviewsthe progressofreseachintisarea，confrontsthshorcomingsofeliabilityassessmentinexistingintellgecersearh，and explorestargetedsolutions.Result/conclusion]Inthefuturedevelopmentofthedisciplieofintellgenc，itisurgenttotegt entheawarenessofreliabiltyasessmentamongresearchers withinthediscipline，strengthenthelinkagebetweenacademiaandindustry，promotetesandardzaioftheeliabilitysssmetsystmofiteligeneresarchfrothelevelsofsous，procesesandmethods，andexplorethe“specialization”ofthereliabilityassssmentofintellgenceresearch，toheimistoforman efcientmechanismofself-examiation，self-evidence，andself-excellenceinintellgenceresearch，andtonancethediscourse of the discipline.",
    "context2": "Keywords:information scienceresearch；reliabilityassessment；intellgenceassessment；intellgence discipline develop ment"
  },
  "0引言": {
    "context1": "近年来，情报学研究中理论、方法等迅捷发展，但随着大数据爆发，情报学研究陷入“数据泛滥、信息淹弱、知识饥荒、情报低质”的困境[1]，一些研究存在无评估、应付式评估、主观自夸式评估等现象，由此引发了学者对部分情报学研究理论、方法和结果可靠性的质疑。情报学研究急需合理的评估以自证其可靠性，从而推动情报学的良性发展。情报学科的话语体系建立在完善的学科体系、深厚的学术体系之上[2]。而情报学研究的可靠性评估正是对情报学科的理论、方法等的合理评判和保障，也是情报学科的研究水平、研究进程的直观展示。推进情报学研究的可靠性评估是优化情报学科理论方法体系建设、提升情报学话语权的重要切入点，尤其是面对人工智能时代的新需求，通过可靠性评估研究形成高效的自审、自证、自优的情报学研究机制，促使学科体系向系统化、智能化、工程化等趋势的可持续创新[3]。",
    "context2": "在此背景下，急需摸清情报学研究中的可靠性评估现状，探索有针对性的解决之道，以增强情报学科的话语体系。因此，本文将立足于情报学新时代背景，明晰情报学研究中评估内涵，梳理可靠性评估进展，提出推进情报学研究评估工作发展的对策。"
  },
  "1情报学研究可靠性评估的内涵": {
    "context1": "科学研究的可靠性，是指在不同主体和时空下的科学实验要遵循可重复性原则，且得出的科研成果要满足可验证性要求。评估是指根据既定的目标，对某一事物的价值、地位、作用和影响等进行有效分析与可靠判断的研究[4]。科学研究的可靠性评估包括两个方面：一是外部评估，即由外部相关人员（同行、使用方）对某项科学研究成果的可靠性再评估；二是内部评估，即由研究者对自身研究的可靠性评估，这是科学研究的基础环节。其中,内部评估是元科学（Metascience），要求学者使用严格的方法来审视其研究实践过程和科学结论的可靠性[5]，以便其及时调整研究思路、采取必要措施，使得研究流程以一个良性有序的循环过程传递下去。由此可见，内部评估是保障科学研究可靠性的基准点。",
    "context2": "本文所关注的情报学研究可靠性评估中，“情报学研究”是指基于学科理论与实践探索的泛指性的情报学研究，而非旨在辅助决策支持的特指性的“情报研究”，而“可靠性评估”则侧重内部评估。换言之，本文将关注点聚焦为：情报学者在完成一项研究时，对其研究要素（数据、指标、方法和结果）的可靠性进行自审性、自证式评估。聚焦这一关注点的原因在于：在情报学研究中，可靠性评估环节保障了在科研过程中所使用的研究方法、模型等的严谨性，亦是对研究结果质量的可靠性评判[6]。因此，其是情报学研究过程中的重要步骤，推动着情报分析工作的自我优化式发展[7]。从这个意义上看，情报学研究可靠性评估的概念有别于科学评价。科学评价注重外部人员的他方评价，在决策管理与利用中外部人员对情报产品、情报信息等的价值评判[8]。实际上，情报学研究可靠性评估是保障科学评价有效性的前提。情报学研究者只有对其研究工作开展可靠性评估，才能促进后续外部人员高效、科学地评价情报成果和产品。",
    "context3": "尽管情报学研究可靠性评估不可或缺，但当前学界对此问题的关注却并不充分，对于其内涵也缺乏讨论。要理解情报学研究可靠性评估的内涵，关键在于评估原则，可以参照与情报学紧密相关的其他学科内对可靠性进行评估。在实践中，情报学者常使用统计方法或计算机指标作为评估工具。近年来，统计学和计算机科学中某些传统评估方法的可靠性正遭受质疑，学者们开始反思评估本身应遵循的基本原则。在统计学中，近年来反思传统评估方法的一个典型例子，是关于P值作为检验决策的依据是否可靠的争议[9]。针对相关质疑，美国统计协会 ASA发布了关于使用P值的基本原则的声明，强调了任何单一指标都无法取代科学推理[10]。在计算机领域，学者从评测数据集、评测模型以及指标等方面对评估方法的可靠性进行了广泛讨论，提出相应的可靠性评估原则。该评估原则包括信度、难度、效度等方面。信度是指评测的可靠程度，即无偏性、鲁棒性、无悖性、科学性；难度是指评测的能力，即挑战性、区分性、量化性、生命性；效度是指评测的效力，即深层次、可解释、易用性、综合性[1]。",
    "context4": "在总结过往研究并汲取情报学相关学科评估经验的基础上，本文认为在情报学研究中，可靠性评估的内涵应包括以下内容。 $\\textcircled{1}$ 客观性：衡量研究结果的准确率，并说明该结果所依据的前提假设、不确定性因素的来源以及程度； $\\textcircled{2}$ 针对性：选择合适的评估数据集、方法并恰当地展开评估分析； $\\textcircled{3}$ 全面性：设置多层次、多维度的分析视角等； $\\textcircled{4}$ 透明性：搭建公开评估交流平台，披露研究数据集、方法、实验过程等评估细节； $\\textcircled{5}$ 可再现性：直接对原有实验过程进行重复性实验，或者通过更换评估数据集、方法等进行稳健性检验等。"
  },
  "2情报学研究可靠性评估的进展": {
    "context1": "当前科学研究的3个关键要素为：研究数据集的采集、研究方法的设计和研究结果的发现。其中，对于情报学科的研究方法，一方面，指标设计和指标体系构建是具有学科特色的研究方法，其相关研究一直备受情报学者的关注和探讨；另一方面，由于情报学科的交叉性，统计学和计算机领域的研究模型、方法等在情报学研究中广泛应用，但评估此类方法本身的可靠性主要是统计学者和计算机学者的研究任务，而情报学者则侧重于评估该方法的应用结果可靠性。因此，本文从研究数据集、研究指标和研究结果3个要素总结情报学研究的可靠性评估进展，具体评估维度如图1所示。除了图1中所列举可靠性评估维度外，少部分学者还探讨了软件工具[12]、评价系统[13]和政策设计[14]等要素的可靠性问题。"
  },
  "2.1情报学研究数据集的可靠性评估": {
    "context1": "随着大数据时代科研数据井喷式增长，情报学研究中的数据质量问题也变得日益突出。在数据集的采集、处理过程中可能会出现不完整、无标注、含噪声等质量问题，从而导致基于数据的情报学研究产生偏差。从以往的相关研究来看，数据集的可靠性评估维度可划分为数据集的外在形式和内在质量。",
    "context2": "![](images/26909b622b80a4f5b517bc3cfa0fc13c8511c6b47e6e15471636b0ad3e1d0d7a.jpg)  \n图1情报学研究的可靠性评估维度  \nFig.1Reliability assessment dimensions in information science",
    "context3": "1）情报学研究数据集的外在形式评估维度。数据集的外在形式评估指的是针对数据集的外在表现形式的评估，主要包括数据集来源的一致性、数据集规模大小的合理性等维度。数据集来源的一致性评估维度是指评估不同的数据集之间的差异大小对研究的影响。情报科技文献通常来源于各大数据库，而有研究表明各数据库的覆盖范围存在很大差异[15]，甚至达到所分析数据集的 $40 \\%$ \\~$50 \\%$ [16]，这种差异主要受研究领域、出版国家和出版时间等因素影响所致，由此得出的研究结果可能产生严重偏差。如 Teixeira[17]使用不同数据库获取作者的相关信息会导致不同的 $\\mathrm { h }$ 指数值，因此数据库间的差异可能会扭曲基于此数据库的分析结果。那么如何应对指标在不同数据集间的结果差异？为此，Pech等[18]评估了基于百分位数和随机的方法是否能有效地在两个数据库之间转换引用计数，同时保证引用计数的标准化。此外，数据集规模大小的合理性评估维度是指评估研究所设计的数据集规模大小对研究的影响。数据集规模是否足够满足实验的数据需求，对于实验设计至关重要，若实验选取的数据集样本少，得出的评价结果将不尽如人意。如 Schulz等[19]探究了实验需要构造多大规模的样本数据集才能准确反映类别标准化引用影响指标（CNCI）的真实平均值和正确的相对状态；Jochen等[20]也提到了用于评估研究中数据集的“最小可评估单位”（LEU），并解释在LEU 标准以下进一步分解的主要障碍是相关文献数量少，指标会失去统计有效性，而导致细分单位的绩效无法独立衡量。",
    "context4": "2）情报学研究数据集的内在质量评估维度。数据集的内在质量评估指的是针对数据集本身具体内容的评估，主要包括数据集的真实性、合规性等维度。在大数据时代，数据造假、数据失真、数据遗漏等问题接踵而来，在数据质量方面应当更重视数据“真实可信”的要求，即是否“合规”（有效性、唯一性）以及是否“真实”（准确性、及时性)[21]。如在文献计量分析中，Meho等[22]比较了Web of Science（WoS）、Scopus 和 Google Scholar（GS）引文数据库的数据质量，发现GS的准确性较低。Franceschini等[23]对遗漏引用进行分析，发现WoS数据库的遗漏率为 $6 \\%$ 。学者也进一步探讨了数据集的内在质量问题对研究结果的影响。Donner[24]对比了准确的与包含一定错误信息的文献类型数据集对引文索引的影响，发现不准确的数据集会导致有偏差的引文计数值。Moreno等[25]指出完整的文献数据集通常难以获得，而由不完整的记录计算所得的跨学科指数存在很大偏差。Moed 等[26]发现由于数据集中缺少一篇高被引论文，导致了错误的学者学术影响排名。",
    "context5": "总体而言，数据集的可靠性是决定研究可靠性的直接前提，即“garbage in，garbage out”，且数据集的收集和处理工作应占总研究工作量的 $5 0 \\% \\sim 7 0 \\% ^ { [ 2 7 ] }$ 。当前情报学研究已形成一系列惯用的数据收集方法（如文献搜集、问卷调查)，这些方法本身的操作性较为灵活，不同研究者面临同一问题采用相同方法收集的数据也可能存在较大差异[28]。此外，情报学研究问题场景繁多，所使用的数据集呈现出多样化，同一个研究问题可用到多种不同数据或是其多种组合。已有研究表明，不同情报学研究问题对数据采集方式[29]、数据集规模[30]的要求不同。“遵循惯例”构造的数据集的可靠性还有待商榷。目前，研究者对数据集可靠性评估的自主意识较为欠缺。后续学科发展中有必要关注学者数据质量评估意识的提升和情报学研究数据质量评估体系的探索。"
  },
  "2.2情报学研究指标的可靠性评估": {
    "context1": "在情报学研究中，指标的选择是否合适、其本身的可靠性如何，都将直接影响着研究结果的准确性。因此，情报学中存在大量基于特定指标的定量研究，是情报学研究可靠性评估的重要组成部分。从以往的相关研究来看，研究指标的可靠性评估维度可划分为学者级指标、文章级指标、期刊级指标。",
    "context2": "1）学者级研究指标的可靠性评估。学者级指标是指从微观层面（学者、研究团队）对学者个人研究成果的量化评估因子，该指标能有效帮助判断学者的创造力及其成果的价值，亦可提供个人学术成就的客观数据[31]。当前，关于学者级指标的研究众多，但并非所有指标都可靠地评估个人学术表现。如Orduna等[32]研究了学术网站上计算的ResearchGate 分数是否能够有效衡量学者的学术声誉。Salman 等[33]将多个学者影响力指数的排名结果与领域内权威排名对比，以此分析指数的可靠性。此外，学者关于指标的适用性也有诸多探讨。如Wildgaard[34]对44位学者绩效指标进行聚类分析和比较，指出选择指标时应考虑不同学者的学术背景。Leydesdorff 等[35]指出在评估学者的高被引论文时，综合影响指标（I3）和百分位（PR6）评价个人绩效比影响因子（IF）更合理。",
    "context3": "2）文章级研究指标的可靠性评估。文章级指标是衡量其学术影响力在学术交流与传播中的广度与深度，是文章对相关研究领域所产生的积极影响的范围和深度的度量[36]。传统的同行评议在面对大量文章数量时适用性降低，因此学者先后提出引用频次、影响因子、共被引、h指数等指标，从不同角度分析论文质量，如Yan 等[37]以26 种H型指数和其他3种传统指标(平均引用次数等)为研究对象，通过指标间的相关性分析等来探讨这些指标在评估单篇文献时的表现。但这类指标也存在时滞性、片面性等问题，不利于全面、及时地体现论文的学术影响力[36]。对此，Kwok[38]认为新型的Altmetrics 在一定程度上能补充传统引文指标的不足，这也吸引了众多学者对Altmetrics与引文分析的相关性探讨，如Waltman等[39]指出F100 推荐和引用量之间存在显著相关性。",
    "context4": "3）期刊级研究指标的可靠性评估。期刊级指标是指期刊在一定时期内发表的学术研究成果在某段时间里促进相关学术研究与应用发展的能力[40]，反映了学术期刊在相关学科领域的学术地位，是学术期刊生存和发展的根本[41]。当前，期刊级指标的种类和方法繁多，张慧玲等[42]从传统指标、影响因子系列评价指标、h指数以及衍生指标、类PageRank 及其衍生指标、基于多因素综合评价方法、基于社交媒体的期刊影响力评价指标以及跨学科期刊评价方法出发，指出现有期刊评价指标的融合度较差，较少考虑引文偏态与自引问题；程结晶等[43]进一步将定量指标细分为期刊论文被引频次、引用和被引用质量、被引时间、被引离散度，也指出已有的期刊指标融合性较差等问题。",
    "context5": "总体而言，研究指标因其实用性备受情报学者关注，其可靠性评估相较于其他要素（数据集、结果等）的研究更为丰富。由于情报学研究问题的复杂性、单一指标的片面性等，多指标融合逐渐成为解决相关情报学研究问题的主流方法。学者基于不同的评估目标，构建了包含不同评价维度的指标体系，使之互补，得出较为可靠的结论。但当前的研究指标体系不够清晰，指标间存在交叉或者重叠，其全面性与适用性未取得较好的平衡。研究者在设计指标体系时，应结合应用场景，综合考虑研究指标的针对性与适用性[44]。"
  },
  "2.3情报学研究结果的可靠性评估": {
    "context1": "情报学研究结果的多样化导致难以有统一的评估工具和准则。在实践中，对情报研究结果的可靠性评估的核心在于实践中是否存在标准结果，以此作为评估依据设计可靠性评估的整体方向。其次，结果可靠性评估的具体方法又可划分为定性、定量和二者结合法。",
    "context2": "1）情报学研究结果的可靠性评估依据为实践中是否存在标准结果。当实践中存在标准结果时，学者常通过对比研究结果与标准结果来开展评估工作。这类评估方法主要用于预测研究中，如前沿预测、技术预测和引文预测等。标准结果的类型主要包含真实数据类和报告文件类。相应的评估方式可细分为： $\\textcircled{1}$ 将研究的预测结果与真实数据进行对比，并衡量其差异大小。如Colladon 等[45]将所提出的引文预测模型随机重复了300次实验后得出论文被引次数的预测值，而后与真实值对比统计获得准确率为$7 9 . 2 \\%$ 。此外，许学国等[46]用均方根误差（RMSE）和平均绝对百分比误差（MAPE）两个定量指标，衡量研究前沿的识别结果与真实结果的差异大小。类似地，Tay 等[47]综合采用了MSE、MAE、NMSE、准确率等指标，评估股票价格预测模型的可靠性。 $\\textcircled{2}$ 将研究的预测结果与现有的专家报告、政策文件等进行对比分析。如李静等[48]结合文献调研法与领域专家咨询法，证明其研究热点识别以及趋势分析方法具有较好的准确度。曾海娇[49]通过对比生物农药领域的前沿识别结果与科技政策数据，证明其提出的前沿识别模型的可靠性。然而，在实践中，很多研究并不存在标准的、真实的或者可形式化比较的结果。在该情况下，学者往往将多种相似方法进行横向比较开展评估工作。如Korytkowski等[50]使用了不同的出版物计数方法（完全、整体、分数、平方根分数）计算国家间研究绩效的排名结果，观察计数方法的改变对绩效评估结果的影响。类似地，Zhao 等[51]对比分析了传统的引文计数法与按位置过滤的引文计数法，评估其是否对作者的引文排名结果有显著影响。其在之后的研究[52]中，又通过比较4种加权引文计数方法的结果，探索了加权作者共引分析法（ACA）的有效性。此外，Forthmann 等[53]通过比较泊松、负二项式和Conway-Maxwell-Poisson 这3种统计模型，评估这些模型对于衡量研究人员能力的可靠性。",
    "context3": "2）情报学研究结果的可靠性评估具体方法可分为定性法、定量法以及二者结合法。定性法多用于尚未开拓的研究领域，主要结合领域专家知识或权威报告来主观评估研究结果的可靠性。如徐路路[54]等采取专家咨询法（石墨烯领域）对其实验设计的合理性以及分析结果的可靠性进行评估。定量法多用于较为成熟的研究领域，其客观性地克服了定性法的专家依赖性[55]，因此近年来受到学者们的青睐。如王菲菲等[56]利用肯德尔和谐系数验证了3种方法在医学信息学前沿探测结果的一致性；Abrishami等[57]在预测论文被引次数时，选择使用RMSE 和 $R ^ { 2 }$ 指标来评估预测结果与真实结果的差距。除了定量评估指标外，还有学者使用统计类分数对所提出的方法或模型进行可靠性评估。如张慧玲等[58]通过计算指标间的相关性系数等，探究其设计的评价指标体系的合理性。严炜炜等[59]计算信度和效度值来评估所设计的调查问卷的科学性。随着情报学的高速发展，研究问题逐渐呈现多面、多层次的复杂性，定量法已不能满足评估新需求，因此更适应新科研环境的定性法、定量法相结合法的重要性日益凸显。如Chen 等[60]将准确度、精确度、召回率和 $F$ 指标作为分类模型的评价指标，并在领域专家的帮助下验证了扩展候选提示词的有效性。类似地， $\\mathrm { L i } ^ { [ 6 1 ] }$ 通过专家咨询法验证了作者关键词的排名结果，并计算Jaccard 相似系数进一步比较了新方法与其他方法在主题词分类、排序结果上的差异。另外，Halim等[62]使用4个聚类有效性指数（即DBI、DI、SC 和CHI指数）评估聚类实验结果，并将其设计的学术期刊分类系统与现有系统对同一组期刊获得的分类结果进行比较，以此评估新系统的可靠性。",
    "context4": "总体而言，由于情报学的社会科学研究性质具有一定的不确定性，这使得其研究结果在本质上难以被证明或复制。在此背景下，当同一项评估任务有多种的评估方案，且其评估结果是相互矛盾的情况下，倘若实践中缺乏标准结果，导致其研究结果的优劣缺乏可供对比的评估基准，学者倾向于采用对自己最有利的评估方案并声称其研究已达到了最佳结果。诸如此类缺乏标准结果的评估研究，往往会出现评估本身失真的情况，这将导致后续研究难以客观比较和超越。因此，情报学也急需针对一些常用的情报学研究问题和通用方法，参照计算机学科研究中“公开评测集”的做法，引入领域专家构造评估标准，从而保证可靠性评估研究的透明性、科学性。"
  },
  "3推进情报学研究可靠性评估工作的对策": {
    "context1": "近年来，大数据、人工智能等先进技术迅速发展的同时，情报学研究越来越趋向于计算化和智能化发展[63]。而通过可靠性评估研究形成高效的自审、自证、自优的情报学研究机制，既能有效保障情报学研究的质量，又能促进建设情报领域自我优化的可持续发展路径。因此，在时代潮流背景下，推进和完善情报学可靠性研究有其必要性和紧迫性。本文总结情报学研究中数据、指标、结果的可靠性评估研究进展及其不足可知，在微观层面情报学研究者个体的评估意识、在中观层面情报学科整体的评估体系规范、在宏观层面情报学研究可靠性评估理论等方面均有一定程度的改进空间。对此，结合现状提出如下对策，以探寻适合我国情报学研究的可靠性评估思路。",
    "context2": "第一，在学科范围内强化学者的可靠性评估意识，拓展评估的分析视角和方法。情报学研究的可靠性是构建和优化情报学基础理论、方法体系的基本途径，也决定了其提出的理论方法在实践中的应用效果。可靠性评估工作不宜后置到实践应用环节，而更重视在理论方法的提出和探索研究中考察其是否合理，将学界的研究可靠性评估作为保障业界实践应用质量的起始点。现阶段情报学研究中，仍存在缺少可靠性评估环节的现象。因此，学科内要强化可靠性评估环节是情报学研究的天然属性和必要环节，在项目、论文等评审鉴定中提高对研究评估环节的关注度，强调对研究可靠性的常态化评估要求。情报学研究人员应转变“我用的是惯用方法，不需要评估”“我提出的新方法看起来是可行的，真的是否可行留待实践应用的检验”等心态，尽己所能地借助科学的评估方法自证研究工作的可靠性。另外，评估环节是一个对后续研究具有指导性的重要过程，但当前评估环节多从一个维度去衡量研究结果的优劣，而每种方法都存在一定的局限性，这样是否能够真正全面、准确地反映评估结果的可靠性还有待商榷。已有学者尝试使用多维视角或多种方法相结合的研究方式来得到一个综合化的评估结果，为决策者提供更高质量的情报信息。如Kim等[64]从信度和效度这两方面来评估所提出的转化研究影响指标：一是可靠性测试，通过观察该指标随时间变化的稳定性；二是有效性测试，与当前主流指标比较评价结果的准确性。此外， $\\mathrm { S t e k } ^ { [ 6 5 ] }$ 在评估其所提出的识别空间技术集群方法时，首先通过比较多种方法效果以评估其有效性，再进一步将其应用于18个领域以评估该方法的普适性。",
    "context3": "第二，加强学界和业界联动，从资源、流程、方法等层面推进情报学研究可靠性评估体系规范化的建设。情报学研究问题往往较为复杂，其结果存在模糊性，因而可靠性评估具有不确定性，不同方式的评估结果甚至可能相互矛盾。尤其是在缺乏公认有效的评估数据集、指标和方法时，研究者自行构造的评估数据、采用的评估指标和方法存在随意性，或采用对自己最有利的评估方案以自夸其研究达到最佳结果。缺乏规范性可靠性评估体系带来的评估失真将导致后续同类研究难以客观比较和螺旋式提升。解决诸如此类应付式评估、主观自夸式评估的问题，关键是从评估原则确立、评估数据集构造、评估指标体系构建、评估方法流程设计等入手建立规范化的可靠性评估体系。这就需要学界和业界进一步加强联动。在具体操作中遵循由易到难、由确定到模糊的原则，先从以解决确定性问题为主的自然科学类研究范式人手[66]。如论文支撑数据的开放共享是论文结论得到客观检验、重复验证和可靠应用的保障，可通过推拉结合的多措并举，促进共享数据的激励措施[67]。当前最为紧迫的任务是投入一定的资源支持从业界实践应用中提炼构造一批带有理想结果（以最能拟合真实状况为准）的、公开的评估数据集，建立透明化的可靠性评估过程机制，从而促进模型实时计算与反馈、评估结果自动修正与超越等的情报学模型化、自优化发展。如可参照计算机学科相关会议的做法，选取业界中一些代表性的任务，构造相应的数据集和理想结果，制定相应的结果评估指标，开展情报研究方法测评工作。",
    "context4": "第三，探索情报学可靠性研究评估理论与实践的“特色化”。情报学研究的可靠性评估不同于统计学检验评估、计算机技术模型评估，其特色根植于情报学研究的领域性、预知性、决策性特点。情报学研究的领域性特点要求其评估不应片面追求量化甚至简单指标评估，而应考虑领域知识驱动下的综合评估，尤其是在评估体系尚未完成规范化建设时期，应重点强调领域专家在情报学研究可靠性评估中的关键性作用。情报学研究的预知性特点，要求其可靠性评估基准立足于对未知、未来的感知能力，而非对历史和现状的揭示能力，这一导向对评估数据集构造、评估指标设计将有重要的影响。情报学研究的决策性特点，就是要求其评估层次从数据层面的准确、拟合等维度，上升到对决策影响层面。实际上，这是情报学研究评估的理想状态，尽管较为困难，但已有研究者尝试评估其研究结果与政策计划文件的一致性[50]，这是一种较好的开头。推进情报学研究可靠性评估工作“特色化”的最终目标，是构建具有鲜明情报特色的评估理论体系。其首要任务是要回归到理论源头的探讨，探索出原创性的、内生性的评估理论、方法，以区别于其他学科，凸显情报学科的价值。同质化的评估研究必将导致个体研究、整体学科表现的平庸化。因此，情报学研究人员应敏锐洞察时代发展新趋势，不断完善构建契合情报智慧、情报价值的可靠性评估理论体系。"
  },
  "4结束语": {
    "context1": "情报学研究可靠性评估有助于建设情报领域自我优化的可持续发展路径。对研究可靠性的合理评估能够反向推动情报研究向更深层次开展，评估环节与其他研究步骤的良性互动形成了情报学研究闭环，即研究 $\\longrightarrow$ 评估 $\\longrightarrow$ 再研究$\\longrightarrow$ 再评估。情报学研究的可靠性评估也有助于提高对外学术话语权。只有建立扎实、深厚的学科理论基础、知识体系，才能保证学科话语的权威性和认可度[2]，而评估工作是其关键的实现路径之一。可靠性评估既是对实验设计、分析过程、研究结果等可靠性的直观说明，也是情报学科的研究水平、研究进程的直观展示。同时，情报学研究可靠性评估为相关方向的研究发展提供了标杆，激励着学者更加积极持续地参与推进研究的深入发展。",
    "context2": "本文系统梳理了情报学研究可靠性评估的内涵、进展后发现，当前研究虽取得了一定的成果，但领域内诸多问题尚未得到充分讨论，在一定程度上导致了部分研究难以回应外界对研究的复现性、可靠性等质疑。针对当前情报学可靠性研究评估的困境，本文立足于新时代情报学发展需求，探寻了适合我国情报学发展的推进研究可靠性评估工作的对策，而在未来研究中，急需强调对情报结果等的常态化评估要求，明确评估原则以规范化研究流程，构建具有鲜明情报特色的评估理论体系。□"
  },
  "参考文献": {
    "context1": "［1］雷帅，李晓松，陈敬一．基于WSR-RDJF国防科技战略情报研究的方法框架［J]．情报理论与实践，2020，43(10):25-30.  \n［2］苏新宁．中国特色情报学学科体系、学术体系、话语体系论纲［J]．中国图书馆学报，2021，47（4)：16-27.  \n［3］戴国强．推进竞跑阶段的创新情报研究［J]．情报学报,2019，38(8)：771-777.  \n[4] 耿蓓，王斌．情报评估体系建设研究［J]．图书馆学研究，2014（23)：7-11.  \n[5] WOOLSTON C.A blueprint to boost reproducibility of results[J]．Nature，2014（513)：283.  \n[6] 邱均平，韩小林，周子番．大数据时代我国评价科学的研究进展与分析［J]．情报理论与实践，2021，44（9）：1-7.  \n[7] 靳娟娟．基于案例的情报评估分析之研究［J]．情报理论与实践，2012，35（1)：45-47，74.  \n[8] 谢晓专．美国情报产品标准与质量控制机制研究［J]．图书情报工作，2019，63（18)：87-98.  \n[9] AMRHEINV，GREENLANDS，MCSHANEB，etal．Scien-tists rise up against statistical significance [J].Nature,2019，567:305-307.  \n[10］郝丽，刘乐平，申亚飞．统计显著性：一个被误读的P值——基于美国统计学会的声明［J]．统计与信息论坛，2016，31(12):3-10.  \n[11] 董青秀，穗志方，詹卫东，等．自然语言处理评测中的问题与对策［J]．中文信息学报，2021，35（6)：1-15.  \n[12] PAN Xuelian，YAN E，CUI Ming，et al.Examining the us-age，citation，and difusion patterns of bibliometric mappingsoftware：a comparative study of three tools [J]．Journal ofInformetrics，2018，12（2)：481-493.  \n[13] HAUER M P，HOFMANN XC R，KRAFFT T D，et al.Quantitative analysis of automatic performance evaluation sys-tems based on the h-index ［J]．Scientometrics，2020，123(2):735-751.  \n[14］RICHTER A，NG K TW，FALLAH B.Bibliometric and textmining approaches to evaluate landfill design standards[J].Scientometrics，2019，118（3）：1027-1049.  \n[15]MONGEON P，PAUL-HUS A. The journal coverage of Web ofScience and Scopus：a comparative analysis [J]． Scientomet-rics，2016，106（1)：213-228.  \n[16]HARZING A W， ALAKANGAS S. Gogle Scholar， Scopusandthe Webof Science：alongitudinaland cross-disciplinarycomparison[J]．Scientometrics，2016，106（2）：787-804.  \n[17]DA SILVA JA T，DOBRANSZKI J. Multiple versions of theh-index：cautionary use for formal academic purposes[J].Scientometrics，2018，115（2）：1107-1113.  \n[18]PECH G，DELGADO C.Percentile and stochastic-based ap-proach to the comparison of the number of citations of articlesindexed in diferent bibliographicdatabases[J].Scientomet-rics，2020，123（1)：223-252.  \n[19]SCHULZ J. Using Monte Carlo simulations to assessthe impactof author name disambiguation quality on diferent bibliometricanalyses[J]．Scientometrics，2016，107（3）：1283-1298.  \n[20] GLASER J,SPURLING T H，BUTLER L. Intraorganisationalevaluation：are there“least evaluable units”?［J]．ResearchEvaluation，2004，13（1)：19-32.  \n[21] 符山，邓正保，于鹏．论数据质量的“真实性”与相关融合计算策略［J]．信息通信技术与政策，2022（2)：8-15.  \n[22] MEHO L I,YANG K. Impact of data sources on citation countsand rankings of LIS faculty:Web of Science versus Scopus andGoogle Scholar[J]．Journal of the American Society for Infor-mation Science and Technology，2007，58（13）：2105-2125.  \n[23] FRANCESCHINI F，MAISANO D，MASTROGIACOMO L.Scientific journal publishers and omitted citations in bibliometricdatabases：Any relationship?［J]．Journal of Informetrics,2014，8(3):751-765.  \n[24] DONNER P.Document type assignment accuracy in the journalcitation index data of Web of Science[J].Scientometrics，2017，113(1):219-236.  \n[25] CALATRAVA MORENO M C，AUZINGER T，WERTHNERH. On he uncertainty of interdisciplinarity measurements due toincomplete bibliographic data [J]． Scientometrics，2016,107 (1):213-232.  \n[26] MOED HF，PLUME A. Studying scientific migration in Scopus[J]．Scientometrics，2013，94（3）：929-942.  \n[27] 经济合作与发展组织，欧盟统计署．奥斯陆手册：创新数据的采集和解释指南［M]．北京：科学技术文献出版社，2011: 46.  \n[28］FIGUEROLA C G，GARCIA MARCO F J，PINTO M. Map-ping the evolution of library and information science（1978-2014）using topic modeling on LISA[J]． Scientometrics，2017，112(3)：1507-1535.  \n［29］陈果，邵雨，王日芬．科技领域情报分析中文献集构造方式比较研究：一致性与可靠性问题［J]．情报学报,2020，39（10)：12.  \n［30］陈果，王盼停，王曰芬．文献集规模对科技领域情报分析的影响：多种任务场景下的实证分析［J]．情报学报,2021 (8): 869-878.  \n[31］徐佳宁，孙婧．学者个人学术成就评价指标研究述评[J]．情报探索，2018（11)：122-128.  \n[32] ORDUNA-MALEAE，MARTIN-MARTINA，THELWALL M,et al. Do ResearchGate Scores create ghost academic reputa-tions？［J]．Scientometrics，2017，112（1）：443-460.  \n[33]SALMAN M，AHMED M M，AFZAL M T.Assessment of au-thor ranking indices based on multi-authorship［J]．Sciento-metrics，2021，126（5)：4153-4172.  \n[34] WILDGAARDL.A critical cluster analysis of 44 indicators ofauthor-level performance ［J]．Journal of Informetrics，2016,10(4):1055-1078.  \n[35] LEYDESDORFF L， BORNMANN L. Integrated impact indica-tors compared with impact factors：An alternative research de-sign with policy implications ［J]．Journal of the American So-ciety for Information Science and Technology，2011，62（11)：2133-2146.  \n［36］王毅萍，马建玲．国外科学数据影响力研究进展［J]．图书情报工作，2017，61（7)：118-126.  \n[37]YAN Zhenbin，WU Qiang，LI Xingchen.Do Hirsch-type indi-ces behave the same in assessing single publications? An empir-ical studyof 29 bibliometric indicators[J]．Scientometrics,2016，109：1815-1833.  \n[38]KWOK R. Research impact：Altmetrics make their mark [J].Nature，2013，500（7463）：491-493.  \n[39]WALTMAN L，COSTAS R.F1000recommendations as a po-tential new data source for research evaluation：a comparisonwith citations[J]．Journal of the Association for InformationScience and Technology，2014，65（3）：433-445.  \n［40］蒋勇青，齐萍．学术期刊影响力评价方法研究［J]．中国软科学，2017（3)：178-185.  \n[41］张铁明，颜帅，佟建国，等．关于提高我国科技期刊学术影响力的思考［J]．编辑学报，2010，22（2）：99-102.  \n[42] 张慧玲，董坤，许海云．学术期刊影响力评价方法研究进展［J]．图书情报工作，2018，62（16)：132-143.  \n[43］程结晶，李秀霞．学术期刊影响力评价指标及改进方案[J]．情报理论与实践，2020，43（6)：56-61，48.  \n[44］刘智锋，王继民，李倩．元数据质量评价研究综述［J].情报理论与实践，2022，45（7）：42-48.  \n「45]COIIADON AFD'ANGEIOCA GIOOR PA.Predic-ting the future success of scientific publications through socialnetwork and semantic analysis[J]．Scientometrics，2020,124(1):357-377.  \n[46] 许学国，桂美增．基于深度学习的技术预测方法—以机器人技术为例［J]．情报杂志，2020，39（8)：53-62.  \n[47] TAY FE H,CAO L. Application of support vector machines infinancial time series forecasting［J]．Omega，2001，29(4):309-317.  \n［48］李静，徐路路．基于机器学习算法的研究热点趋势预测模型对比与分析—BP神经网络、支持向量机与LSTM模型[J]．现代情报，2019，39（4)：23-33.  \n[49］曾海娇．基于专利与论文关联的领域潜在前沿识别研究［D]．北京：中国农业科学院，2020：67-84.  \n[50] KORYTKOWSKI P，KULCZYCKIE.Publicationcountingmethods for a national research evaluation exercise ［J]． Jour-nal of Informetrics，2019，13（3）：804-816.  \n[51］ZHAO Dangzhi， STROTMANN A.Deep and narrow impact:Introducing location filtered citation counting ［J]．Scientomet-rics，2020，122（1)：503-517.  \n[52] ZHAO Dangzhi，STROTMANN A.Telescopic and panoramicviews of library and information science research 2O11-2018：a comparison of four weighting schemes for author co-citationanalysis[J]．Scientometrics，2020，124（1）：255-270.  \n[53] FORTHMANN B，DOEBLER P.Reliability of researcher ca-pacity estimates and count data dispersion：a comparison ofPoisson， negativebinomial，andConway-Maxwell-Poissonmodels［J]．Scientometrics，2021，126（4）：3337-3354.  \n[54] 徐路路，王芳．基于支持向量机和改进粒子群算法的科学前沿预测模型研究［J]．情报科学，2019，37（8）：22-28.  \n[55] TICHY G.The over-optimism among experts in assessment andforesight[J]．Technological Forecasting and Social Change,2004，71(4):341-363.  \n[56］王菲菲，刘明.Altmetrics 视角下的交叉学科研究前沿探测-以医学信息学领域为例［J]．情报学报，2020，39(10):9-20.  \n[57]ABRISHAMI A，ALIAKBARY S.Predicting citation countsbased on deep neural network learning techniques ［J]．Jour-nal of Informetrics，2019，13（2）：485-499.  \n[58] 张慧玲，许海云，岳增慧，等．学科交叉期刊的影响力评价方法研究［J]．情报学报，2019，38（10)：1030-1040.  \n[59] 严炜炜，陈若瑜，张敏．基于元分析的在线知识付费意愿影响因素研究［J]．情报学报，2021，40（2)：204-212.  \n[60] CHEN C，SONG M，HEO G E.A scalable and adaptive meth-od for finding semantically equivalent cue words of uncertainty[J]．Journal of Informetrics，2018，12（1）：158-180.  \n[61] LI Munan. Classifying and ranking topic terms based on a novelapproach：Role differentiation of author keywords ［J]．Scien-tometrics，2018，116（1）：77-100.  \n[62] HALIM Z,KHAN S.A data science-based framework to catego-rizeacademic journals［J]．Scientometrics，2019，119(1):393-423.  \n[63] 赵志耘，曾文，王忠军，等．需求导向的中国科技情报研究方法探索与思考［J]．情报学报，2022，41（1)：1-9.  \n[64] KIMYH，LEVINE AD，NEHLEJ，et al.A bibliometricmeasure of translational science［J]．Scientometrics，2020,125(3):2349-2382.  \n[65] STEKPE. Identifying spatial technology clusters from patentingconcentrations using heat map kernel density estimation [J].Scientometrics，2021，126（2）：911-930.  \n[66] 杨建林．关于重构情报学基础理论体系的思考［J]．情报学报，2020，39（2)：125-134.  \n[67］张晓林．让论文支撑数据的可靠检验成为可信赖研究的保障［J]．数据分析与知识发现，2022，6（2/3)：1-2.作者简介：陈果（通信作者），男，1986年生，博士，副教授。研究方向：领域知识分析，科技情报分析。姚游倩，女，1998年生，硕士生。研究方向：信息智能处理，信息分析。  \n作者贡献声明：陈果，提出论文思路，理论框架设计，修改论文定稿。姚游倩，收集整理资料，撰写论文。  \n录用日期：2022-10-28"
  }
}