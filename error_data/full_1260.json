{
  "original_filename": "full_1260.md",
  "nothing": {
    "context1": "编者按：2019年7月10-11日，第二届“数据分析与知识发现\"学术研讨会在兰州召开。会议评选出25篇优秀论文，在《数据分析与知识发现》期刊以会议专辑形式出版。"
  },
  "专辑": "",
  "中文分词技术研究综述\\*": "",
  "唐琳郭崇慧陈静锋": {
    "context1": "(大连理工大学系统工程研究所 大连116024)",
    "context2": "摘要：【目的】梳理中文分词领域的关键问题、算法和模型,为研究人员提供理论基础和实践指导。【文献范围】使用知网数据库、万方数据知识服务平台和计算机科学文献库DBLP检索中文分词相关文献,共选择109篇代表性文献进行综述【方法】归纳中文分词的发展历程及关键问题,分类总结中文分词的算法和模型,并详述近期的热点研究问题【结果】使用多个标注数据集的多准则分词模型是中文分词的研究难点,解决中文分词和自然语言处理其他子任务的多任务联合模型是当前研究的热点。【局限】没有深人对比分析中文分词的无监督学习方法。【结论】虽然现有的中文分词方法能在一定程度上满足诸多应用的需求,但是在大数据环境下多视角、多任务和多准则的联合模型研究仍存在挑战。",
    "context3": "关键词：中文分词分词算法多准则学习联合模型分类号：TP393DOI: 10.11925/infotech.2096-3467.2019.1059",
    "context4": "引用本文：唐琳，郭崇慧，陈静锋.中文分词技术研究综述[J].数据分析与知识发现，2020,4(2/3)：1-17.(Tang Lin,Guo Chonghui, Chen Jingfeng. Review of Chinese Word Segmentation Studies[J]. Data Analysis andKnowledge Discovery, 2020, 4(2/3): 1-17.)"
  },
  "1引言": {
    "context1": "中文文本中词与词之间没有明确的分割标记,而是以连续字符串形式呈现。所以,任何中文自然语言处理任务都必须解决中文序列切分的问题中文分词。中文分词是通过某种方法或方法的组合，将输入的中文文本基于某种需求并按照特定的规范划分为\"词\"的过程[。由于任务不同、视角不同、准则不同,不同人对\"词\"的定义持有不同意见，这也成为中文分词需要解决的一个难题。中文分词已有30余年的研究历史，相应的中文分词研究成果被应用到自然语言处理的不同任务中,包括信息检索、机器翻译、语音识别、文本错误识别、中文繁简体",
    "context2": "自动转换、自动问答等。",
    "context3": "1987年梁南元[2撰写中文分词综述，介绍当时主流的中文分词方法一—机械分词(也称为词典匹配法)。该方法需要预先构建一个词典,再使用匹配算法实现分词。初期的机器词典质量不佳、匹配方法研究尚且粗浅等原因导致机械分词的切分结果偏差较大。1997年刘开瑛[3对汉语自动分词测评技术进行总结。随着研究的深入，机械分词方法逐步走向成熟。2001年孙茂松[4基于清华大学的相关工作,介绍中文分词研究的若干最新进展,对机械分词方法中的词典、匹配算法、以及如何解决歧义切分问题进行总结归纳。2007年黄昌宁等[5]归纳中文分词的研究难题：未登录词识别和歧义消解。未登录词识别是机械分词方法无法解决的难题，限制了分词准确率的提升。该研究根据Bakeoff测试数据总结说明,对中文分词的后续研究产生了深远影响。2008年何莘等[从自然语言检索角度对中文分词进行讨论。2011年奉国和等基于专家经验的方法对中文分词的中文文献总结归纳,发现相关算法聚焦在机械分词和传统机器学习方面。这一时期基于字粒度的机器学习算法已经能够一定程度上解决未登录词识别的问题。但是，人工特征选择是影响传统机器学习方法分词结果的重要因素。近年,基于深度学习的中文分词方法无需人工选择特征，且有较高的分词准确率,对中文分词算法的进一步发展产生了巨大影响。赵芳芳等[8]、梁喜涛等[9]对中文分词和词性标注两个密切关联的自然语言处理任务进行探讨。赵海等[10]对机器学习中监督学习和深度学习两种主流算法2007年-2017年的发展进行梳理。",
    "context4": "中文分词是自然语言处理中的基础工作，是一个复杂的研究问题。本文于2019年10月使用中国知网数据库、万方数据知识服务平台，以“中文分词”、“汉语分词\"为主题词检索相关文献,共获得5064篇文献的标题、关键字和摘要。使用计算机科学领域文献库(DBLP）,利用关键字\"ChineseWordSegmentation\"检索，共获得402篇文献的标题、关键字和摘要。人工阅读标题和摘要,筛选以中文分词为主要研究目的\"中文分词文献\"共计1884篇，其中中文1504篇，英文380篇。最终获取的\"中文分词文献”发表年份的数量分布如图1所示。发现从2003年开始，学术界对中文分词一直保持着较高的关注度。本文根据文献研究的具体问题和方法进行人工筛选,保留最具代表性的109篇文献，进行系统总结和分析后，提出需要进一步研究的科学问题。",
    "context5": "![](images/f5549277eb74227e2a5ca203fed06848bc9b8827690078663ed4cc070c388099.jpg)  \n图1\"中文分词文献\"发表年份的数量分布  \nFig.1Distribution of Chinese Word Segmentation",
    "context6": "最初下载的5466篇中英文文献，大部分是以中文分词为基础,研究特定领域的自然语言处理任务。文献标题大多数包含其应用领域，因此本文基于文献标题探索中文分词的应用场景,得到\"中文分词文献\"标题词语共现网络如图2所示。",
    "context7": "![](images/efa6325e7936a106daebd825a05ee70e87fb71e75266406ff37e5064ff0981b4.jpg)  \n图2“中文分词文献”标题名词共现网络",
    "context8": "Fig.2Co-occurrence Network of the Nouns in Chinese Word Segmentation Literature Titles"
  },
  "数据分析与知识发现": {
    "context1": "法被广泛应用，持续保持了较高的关注度;2015年之后深度学习算法相关文献逐渐增多。然而，新算法的出现并没有替代之前的分词算法。深度学习成为近年的研究热点，单独将所有神经网络和深度学习相关的关键字抽取出来,得到词频年份分布如图6所示。可以发现，近三年深度学习、LSTM、双向LSTM和注意力机制是中文分词研究的主流方法。",
    "context2": "调研相关文献发现不同算法之间各有优势，结合不同算法优势解决具体中文分词问题是未来探索的方向。“歧义消解\"和\"未登录词识别”一直作为研究解决的关键问题,相比较而言\"未登录词识别\"被关注更多。这是因为\"未登录词\"数量多、形式多样难于解决。Bakeoff2003[19]和Bakeoff $2 0 0 5 ^ { [ 1 1 ] }$ 语料库统计结果表明未登录词大约是切分歧义的5.6-25.6倍。因此，“未登录词识别\"问题对中文分词的结果影响很大,也是中文分词的重要子任务。",
    "context3": "![](images/1952c4dc87c34d6d9ca199bc00ab1aec11706c76fbbbab1e63c48766cd5ef205.jpg)  \n图5“中文分词文献\"部分关键词分布(篇）",
    "context4": "![](images/fef5b56af48bf507a1357c70d95f5c79793d3d3e0de2ea395f69b46e8f3387e3.jpg)  \nFig.5The Distribution of Key Words in Chinese Word Segmentation Literature   \n图6神经网络和深度学习方法相关关键字分布  \nFig.6Key Words Distribution Related to Neural Networks and Deep Learning",
    "context5": "归纳现有“中文分词文献\"描述的研究问题、研究方法(算法及模型)和应用领域,绘制中文分词的研究现状,如图7所示。中文分词研究现状整体上划分为三部分：中文分词的研究问题、算法及模型、通用工具及应用领域。用不同颜色区分已有研究、近期研究热点和未来研究热点。",
    "context6": "![](images/2c85b18d64210873830435fd7b4e056ead69bca8f3c89f5874e67ae2f8155a12.jpg)  \n图7中文分词研究现状  \nFig.7Research Status of Chinese Word Segmentation",
    "context7": "中文分词研究的问题包括：分词标准、切分歧义和未登录词。分词标准很难被精确定义。以往采用事实标准或自定义标准，现阶段主要基于特定领域标准和特定问题标准。由于视角、研究问题和领域的不同，目前不同的分词标准之间存在差异，未来希望能研究出一套通用标准。",
    "context8": "中文分词算法及模型分为知识驱动的机械分词和数据驱动的统计分词。机械分词的研究问题包括如何存储知识的表示，即词典，以提升检索效率；为减少切分歧义和未登录词问题的词典匹配方法;规则匹配方法。数据驱动下的统计分词模型处理的原子单位主要是词和字。由于字本身存在多义的问题,构成字意的义原[20]、中文的笔画[21]和读音[22]等作为原子单位也有研究。由于中文分词语料库标注成本高,语料库规模普遍偏小,不同语料库之间标准还存在差异。如何同时使用多个存在标准差异的标注语料,甚至未标注的语料是未来的研究热点。相应的模型算法包括半监督学习、弱监督学习、强化学习等。未来进一步提升中文分词的准确率和分词效率,除了单纯的算法和模型改进,还需要考虑将已有的知识库更好地集成到数据模型中。",
    "context9": "传统的管道模型存在错误传播的问题。随着研究范式的不断发展和变化，研究人员不仅需要关注中文分词,也需要关注自然语言处理的相关任务。自然语言处理的多任务联合模型将成为未来研究的重要方向。"
  },
  "2中文分词的发展历程": {
    "context1": "自然语言处理任务日益成为学术界和产业界关注的热点,中文分词作为自然语言处理的基础任务和关键任务，成为研究的核心热点。380篇英文文献大多是会议文献，来源包括ACL、EMNLP、COLING、IJCNLP等。自然语言处理会议悉数关注中文分词文献，收录最多的会议是ACL,自2003年几乎每年都有中文分词相关文献收录。SIGHAN是国际计算语言学协会中文处理特别兴趣组,共举行9次研讨会，共计发表中文分词相关文献达76篇。SIGHAN采用多家机构的评测数据组织多次评测(即BakeOff),评测使用封闭测试和开放测试两种方法。封闭测试只允许使用固定训练语料学习相应的模型，而开放测试可以使用任意资源。测试使用的评价指标包括准确率、召回率和F值。其中，对比的黄金标准是人工标注的数据集。SIGHAN和中国中文信息学会(CIPS)先后三次举办中文处理资源与评测国际会议,称为CIPS-SIGHAN。中文分词的评测和相关会议都极大地推动了中文分词的发展。",
    "context2": "以SIGHAN及CIPS-SIGHAN的评测为主线，展示历届评测的重点内容和相关联的国际会议、时间，如图3所示。图中左侧使用不同颜色矩形框区分各个会议,圆形中的数字表示举办到第几届,评测与会议联合举办则增加了连线。",
    "context3": "SIGHAN $7 2 0 0 5 ^ { [ 1 1 ] }$ 提供的数据集中包括训练集、测试集以及测试集黄金分割标准，除此之外还提供一个用于评分的脚本。比赛数据由4个数据集组成,分别是简体中文的北京大学PKU数据集和微软研究院MSR数据集;繁体中文的CityU数据集和AS数据集。它们至今仍作为学术界评测分词方法准确程度的重要标准。在这些数据集上评测的最佳F值结果如表1所示,包括比赛评测和后续文献。不同方法的最佳F值基本达到甚至超过 $9 5 \\%$ 。单纯设计一种学习算法已很难继续提升分词精度,如何更有效地结合不同算法是未来的研究方向。开放测试除了需要关注算法本身，更好的预训练和后处理对于提升分词结果至关重要。",
    "context4": "对所有“中文分词文献\"中的关键字进行分析。将英文文献中的关键字翻译为中文，部分英文会议文献没有关键字忽略不计。在统计中文文献关键字的过程中，发现部分关键字含义相同但表述不同。例如，条件随机场算法有十余种表达形式,包括条件随机场、条件随机域（CRF）、CRF算法、CRFs模型、CRF标注、CRFs、条件随机场(CRFs）、条件随机域、条件随机场模型、CRF、条件随机场(CRF）、条件随机场算法、Condition RandomField(CRF）、CRF模型等。为保证统计结果更为准确,预先手工构建同义词词典,对同义词进行合并。“中文分词文献\"关键字词云如图4所示，可以印证,文献中解决的问题主要是未登录词识别、歧义识别和歧义消减。算法方面基于分词词典和规则进行匹配的机械分词方法使用最多;统计分词中条件随机场和马尔可夫模型是最具代表性的方法;神经网络和深度学习方法还处于发展期。",
    "context5": "在时间维度上,选择总词频多于20的关键字，去掉\"中文分词算法\"等一般性词汇，得到关键词词",
    "context6": "![](images/fd482dd0846345740d1019d2d5e6653b3d7bc05a6482212dc2d5bde4229e05ed.jpg)  \n图3中文分词相关会议与评测的主题及时间分布",
    "context7": "Fig.3Topics and Time Distribution of the Conferences Related to Chinese Word Segmentation频分布如图5所示。可以看出机械分词算法自1984年至今持续在文献中出现;2004年之后机器学习算",
    "context8": "表1SIGHAN2005数据集上的F值测试结果 $( \\%$ ）  \nTable1F-value Test Resultson SIGHAN2005 Dataset ( $\\%$",
    "context9": "<table><tr><td rowspan=\"2\">年份作者</td><td rowspan=\"2\">研究方法</td><td rowspan=\"2\">来源</td><td colspan=\"4\">封闭测试</td><td colspan=\"5\">开放测试</td></tr><tr><td>PKU</td><td>JMSR CityU</td><td></td><td></td><td>As</td><td>PKU</td><td>MSR CityU</td><td></td><td>as</td><td></td></tr><tr><td>2018 Zhang等[12]</td><td>结合词典的深度学习方法</td><td>AAAI</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>96.5</td><td>97.8</td><td>96.3</td><td></td><td>95.9</td></tr><tr><td>2017 Cai等[13]</td><td>基于字和词的深度学习方法</td><td>ACL</td><td></td><td>95.4</td><td>97.0</td><td></td><td>95.4 95.2 95.8</td><td></td><td>97.1</td><td></td><td>95.695.3</td><td></td></tr><tr><td>2015 Chen等[14]</td><td>基于深度学习的长短期记忆网络</td><td>EMNLP</td><td></td><td>94.3</td><td>95.0</td><td>-</td><td>-</td><td>96.5</td><td>97.4</td><td></td><td>=</td><td>-</td></tr><tr><td>2012 Sun等[15]</td><td>基于丰富特征的现联合学习模型同时学习中文分词和新词发现 ACL</td><td></td><td></td><td>95.4</td><td>97.4</td><td>94.8</td><td>-</td><td></td><td>-</td><td></td><td>-</td><td>-</td></tr><tr><td>2010 Zhao等[16]</td><td>基于字的6位标注方法</td><td>TALIP</td><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td>-</td><td></td><td>98.397.8 96.1</td><td></td></tr><tr><td>2008 Zhao等[17]</td><td>非监督分词辅助基于字的条件随机场方法</td><td>SIGHAN</td><td></td><td>95.4</td><td>97.6</td><td>96.1</td><td>95.7</td><td></td><td>- -</td><td></td><td>-</td><td>-</td></tr><tr><td>2007 Zhang等[18]</td><td>基于词的判别式感知机方法</td><td>ACL</td><td></td><td>94.5</td><td>97.2</td><td>94.6</td><td>96.5</td><td></td><td>=</td><td></td><td>■</td><td>-</td></tr><tr><td>2005 Bakeoff</td><td>评测结果</td><td>评测</td><td></td><td>95.0</td><td>96.4</td><td>94.3</td><td>95.2</td><td>96.9</td><td>97.2</td><td></td><td>96.295.6</td><td></td></tr></table>",
    "context10": "![](images/fdf50d1ac90ccea05783d2026bad0b1168183edb9a11f9533bdeaa33a2e77ad4.jpg)  \n图4关键字词云  \nFig.4Word Cloud of the Key Words"
  },
  "3中文分词的关键问题": {
    "context1": "中文分词首先要面对的问题是需要有清晰的分词标准，然而中文博大精深,分词标准一直以来都无法被统一。目前，只能对具体问题设定特定标准。在特定标准下，实际分词的过程中主要存在切分歧义和未登录词识别两大问题[5]。"
  },
  "3.1分词标准": {
    "context1": "中文分词研究者最初认为要先对\"词\"进行清晰、统一和可计算的定义。然而，目前为止所有关于“词\"的定义都是模糊的、不可直接用于计算的。汉语语法教科书①中对\"词\"的定义为：语言中有意义的能单说或用来造句的最小单位。1993年国家技术监督局发布的《信息处理用现代汉语分词规范》对自然语言处理中的若干问题进行规范和统一,该规范对\"词\"的定义为：最小的能独立运用的语言单位。",
    "context2": "中文词汇本身具有开放性、动态性,与研究问题和研究视角相关,不同人之间也存在认同差异,实验表明人与人之间的认同率只有0.76左右[23]。因此,至今仍无法给出一个通用的可操作标准。已有研究绝大多数都是预先在特定领域或者特定问题前提下设定特定标准，再进行分词研究。事实上，针对不同问题、不同领域的分词标准,甚至是同一问题内部分词标准都存在矛盾。例如：“林丹拿了总冠军。\"这句话在现有公开的均衡分词语料中的标准就存在差异。北大的人民日报语料将姓名拆分,即\"林”“丹”拆开作为两个词;而微软语料规范中视其为一个词,即“林丹”不切分[24]。北大训练语料（SIGHANBakeoff-2005)统计发现,语料内部存在约 $3 \\%$ 的切分标准不一致。因此,通用分词标准一直是中文分词的难题。"
  },
  "3.2切分歧义": {
    "context1": "切分歧义指在切分中文字符序列时存在歧义，有两种常见的分类。第一种分类广泛在中文分词文献中使用。分为交集型切分歧义（OverlappingAmbiguity Segmentation,OAS)[25]和多义组合型切分歧义（Combination Ambiguity Segmentation,CAS)[26]。交集型切分歧义也被称为交叉歧义，例如：“按时下的进展,很难完成任务！\"中\"按时\"和\"时下\"都可以构成词。多义组合型切分歧义也被称为覆盖歧义，例如：“李刚是很有才能的人\"中\"才”、“能\"本身都可以单独构词,也可以合并为\"才能\"构词。另一种是从歧义的真伪角度分类[27]，分为真歧义和伪歧义。真歧义是中文文本本身的语法和语义都没有问题,即便人工进行切分也会产生歧义。例如\"乒乓球拍卖完了”,这句话本身可以有两种不同的理解，而且都是没有问题的,即\"乒乓/球拍/卖完/了\"和\"乒乓球/拍卖/完/了”。反之,被称为伪歧义。解决真歧义的问题非常复杂,需要依赖具体的情境及更多的上下",
    "context2": "文信息。",
    "context3": "基于机械分词算法研究时,切分歧义是中文分词研究的重点问题。随着研究范式的转变，现有的传统机器学习和深度学习算法已经能较好地解决该问题。"
  },
  "3.3未登录词识别": {
    "context1": "未登录词识别包括新涌现的通用词、专业术语和专有名词,如中国人名、外国译名、地名、机构名(泛指机关、团体和其他企事业单位)等。其中，人名、地名和机构名具有多变性,处理难度较大。例如：“康美药业股份有限公司\"可以简称为“康美药业”“康美\"等。因此,在1995年11月的第6届MUC会议(MUC-6)上,提出了一个明确的概念—一命名实体(Named Entity,NE)[28],包括人名、地名、机构名、日期、时间、百分数和货币。事件抽取任务、知识图谱、信息检索、问答系统等都十分依赖命名实体识别。因此,命名实体识别被单独研究。"
  },
  "4中文分词模型算法及联合模型": "",
  "4.1中文分词模型算法": {
    "context1": "中文分词模型算法主要经历了三个阶段，分别是基于匹配的词典分词、基于标注的机器学习算法和基于理解的深度学习算法。其中,基于匹配的词典分词也被称为机械分词。基于标注的机器学习算法和深度学习算法被统称为统计分词方法。此外，近期研究的热点和难点包括单一准则下的多模型集成算法和多准则分词。"
  },
  "（1）机械分词": {
    "context1": "最初的中文分词研究人员认为需要先建立词典,再通过匹配的方法进行分词,这种方法称为机械分词。主要研究问题包括：如何构建一个完备的词典;随着词典规模的不断增加,如何优化词典的存储，更易于查找以提升检索效率[29];匹配算法如何设计;匹配中出现的歧义切分如何消解。",
    "context2": "常见的匹配算法包括：正向最大匹配法或正向最长词优先匹配法（Forward MaximumMatching,FMM)、逆向最大匹配法(Reverse Maximum Matching,RMM)、双向最大匹配法、全切分法等。匹配算法中，存在较多切分歧义问题。切分歧义研究包括歧义发现和歧义消解,歧义消解主要采用规则和统计的方法[27]。由于算法简单,机械分词具有分词速度快的天然优势。然而,分词准确率与词典的好坏正相关，在未登录词较多的情况下，算法的准确率无法保证。"
  },
  "（2）机器学习": {
    "context1": "1990年,Sproat等[30]首次基于统计学习方法实现中文分词。根据处理的粒度,分为基于词和基于字两类标注。在2003年、2005年和2006年三次Bakeoff中文分词测评中，基于字标注的中文分词方法有效提升了分词准确率。因此,基于字标注的中文分词方法迅速成为主流[31]。该方法是将中文分词转化为字序列标注的问题[32-33]。汉字的边界位置标注方法包括2位标记法、4位标记等。被广泛使用的是4位标记法[32],B表示开始位置、M表示中间位置、E表示结束位置及S表示单字构词。机器学习算法需要人工设计特征模板，指定窗口的大小。由于算法的复杂度以及对分词结果准确度要求等原因，窗口大小一般不超过5。",
    "context2": "最初影响力较大的模型是最大熵模型（Maximum Entropy，ME)[34-5]和隐马尔可夫模型(HiddenMarkovModel,HMM)[36]。隐马尔可夫模型存在输出独立性假设的问题,在特征选择时受到限制,故无法选择复杂特征。最大熵马尔可夫模型（Maximum EntropyMarkov Model,MEMM)[37]集成了最大熵模型和马尔可夫模型的优点,将上下文信息引入到模型中，可以选择任意特征,模型学习和识别的准确率都有所提升。但是模型对每个节点进行独立归一化,存在偏置问题。条件随机场(CRF)[38-40]结合了多方面优势,对所有特征进行全局归一化，避免了偏置问题,成为传统机器学习中应用最多、最具代表性的模型算法之一。条件随机场能够获得更高的分词准确率,但模型复杂导致分词效率略低。",
    "context3": "无监督分词策略均需要预设良度标准[41],以往研究中常见的良度标准有子串频率(Frequency ofSubstring with Reduction，FSR）[42]、描述长度增益（DescriptionLength Gain，DL）[43]、邻 接多样性（Accessor Variety，AV）[44]和分支信息熵（BranchingEntropy，BE)[45]。传统的无监督分词算法包括判别式模型和生成式模型两大类。基于判别式模型的文献主要在互信息[46-47]、分支信息熵[47-48]、长度增益[49]等方法上进行改进研究。生成式模型主要基于HMM[50] $\\mathrm { . H D P ^ { [ 5 1 ] } }$ 等进行改进研究。",
    "context4": "半监督研究包括 semi-CRF算法[52]、Zhao等[53]提出强扩展性的半监督分词算法、Zeng等[54]提出的协同正则化字粒度和词粒度模型。半监督能克服熟语料不足的实际困难,分词的准确率较无监督方式有一定提升。",
    "context5": "对于机器学习算法、模型,特征选择好坏仍然是决定最后结果的关键性因素。部分研究人员希望能够通过算法进行自动特征选择,Yang等[55]研究CRF算法下的无监督的特征选择方法。随着深度学习的出现,这一问题才在一定程度上被有效解决[10]。"
  },
  "（3）深度学习": {
    "context1": "2011年,Collobert等[56]首次将深度学习算法引入自然语言任务中。该方法可以通过最终的分词标注训练集，有效学习原始特征和上下文表示。随后$\\mathrm { C N N } ^ { [ 5 7 - 5 8 ] }$ 、GRN[59]、LSTM[14,60-61]、BiLSTM[62]等深度学习模型都被引入到中文分词任务中，并结合中文分词进行多种改进[63-64]。相对于机器学习而言,深度学习算法无需人工进行特征选择，还可以有效地保留长距离句子信息,是对机器学习算法的有效补充。但是深度学习算法更为复杂，需要更多的计算资源。",
    "context2": "在基础深度学习模型的基础上，有效结合预训练和后处理方式已成为深度学习的一种趋势，一般性流程如图8所示。Ma等[65]发现仅使用一个简单的Bi-LSTM模型,基于深度学习的预训练、Dropout及超参调优,可以将分词效果提升到领先水平。实验结果表明,预训练对提升分词准确率有效,平均可以提升 $0 . 7 8 \\%$ 。错误分析发现2/3的错误来自未登录词。更好的预训练结果和更有效的特征表示都是深度学习分词的重要研究方向。预训练既可以根据领域需要和任务特点进行预训练,也可以直接使用现有的预训练结果进行微调。中文分词预训练的基本单位是词(字)的语义、偏旁、拼音和输入法等。语义表示的预训练模型包括与上下文无关的静态词向量训练模型Word2Vec[6]、Glove[7]以及与上下文相关的动态词向量训练模型ELMo[68]、BERT[9]和XLNet[70]等。Wang等[71]在语义表示的基础上,增加了字的拼音、五笔特征,使用Bi-LSTMs-CRF模型训练,在 SIGHAN2005[]和 $\\mathrm { C T B 6 } ^ { [ 7 2 ] }$ 数据集上测试，结果表明多特征融合确实能提升分词准确率。位置信息和外部知识库也被研究人员尝试使用，基于知识库减少训练集的未登录词问题,研究者尝试将词典与深度学习相结合[12.73]。Zhang等[12]在表示中融人词典外部知识，在 SIGHAN2010[74]的Literature、Computer、Medicine、Finance数据集上实验，结果表明融合外部词典分词准确率提升 $2 \\%$ 左右。",
    "context3": "![](images/7821ad59c965fd194f72a2972d13f83007012f77ceb45fc2fa5d101eb4df8f55.jpg)  \n图8基于深度学习的中文分词流程  \nFig.8Chinese Word Segmentation Flow Chart Based on Deep Learning"
  },
  "（4)集成算法": {
    "context1": "在词粒度和字粒度上[75-76],集成机械分词、机器学习和深度学习算法，更好发挥不同分词算法各自的优势是多算法集成的主要探索方向。",
    "context2": "张梅山等[77结合统计与词典提出领域自适应分词算法。近期研究者在机器学习和深度学习算法集成上进行多种尝试。最具代表的是2015年Huang等[78提出的Bi-LSTM-CRF模型实现序列化标注。除此之外,Ma等[79]提出Bi-LSTM-CNN-CRF模型，利用CNN得到字的语义表示，再基于Bi-LSTM-CRF模型实现序列化标注。Yao等[80提出Bi-LSTM-RNN模型。冯国明等[8将词典、统计、深度学习三者有机结合，提出专业领域的自主学习分词算法。集成算法分词不仅在中文分词任务上取得了较好效果，在自然语言多任务联合模型中也被广泛使用。"
  },
  "（5）多准则分词": {
    "context1": "由于研究视角、研究任务等差异，目前仍然无法构建统一的分词标准。不同的标注标准导致不同的中文分词数据集存在标注差异,甚至同一数据集内部也有不同标注粒度的问题。数据集的人工标注成本巨大,已有的单一数据集数据量又十分有限。基于更多的标注数据能辅助训练出更好的模型,因此使用多源语料的多准则、多粒度分词成为新的研究方向。",
    "context2": "根据标题和摘要筛选得到多粒度多准则分词的研究文献7篇。这些研究主要从语料和方法两个方面出发,实现多准则、多粒度分词。",
    "context3": "$\\textcircled{1}$ 设计新的统一的多粒度标注方法将不同语料融合为一个规模更大的语料,再提出新的模型。张文静等 $^ { [ 8 2 ] } 2 0 1 9$ 年提出基于Lattice-LSTM模型,对比Gong等 $^ { [ 8 3 ] } 2 0 1 7$ 年基于字的LSTM模型的效果,F1值从 $9 5 . 3 5 \\%$ 提高到 $9 6 . 2 9 \\%$ 。",
    "context4": "$\\textcircled{2}$ 同时使用多个独立的语料库,通过方法集成多个分词语料。Gong等[24]提出Switch-LSTM模型并在SIGHAN $\\lceil 2 0 0 5 ^ { [ 1 1 ] }$ 和SIGHAN $\\lceil 2 0 0 8 ^ { \\left[ 8 4 \\right] }$ 的8个数据集上测试,平均F值达到 $9 6 . 1 2 \\%$ 。",
    "context5": "两种思路下的研究均在深度学习模型基础上进行创新。关于多粒度中文分词模型方法，从研究思路、研究方法以及实验所使用数据集等方面对比分析各研究,如表2所示。多准则分词问题非常复杂，虽然可以使用更多的语料，但是未登录词仍然不可避免。在此基础上，融合已有领域知识进行模型创新以提高分词准确率,仍然是未来重要的研究方向。",
    "context6": "表2多粒度、多准则分词文献对比分析表  \nTable2Comparative Analysis of Multi-granularity and Multi-criterion",
    "context7": "<table><tr><td>年份</td><td>作者</td><td>来源</td><td>研究思路</td><td>研究方法</td><td>实验使用的数据集</td></tr><tr><td>2019</td><td>Gong等[24]</td><td>AAAI</td><td>方法改进</td><td>模型由多个长短时记忆神经网络(LSTM)和 SIGHAN2005[I](MSR、AS) 一个切换器组成,可以在这些LSTM之间自SIGHAN2008[84](PKU、CTB、SKIP、CityU、 动切换。</td><td>NCC、SXU) CTB6[72]</td></tr><tr><td></td><td>2019 Huang等[85]</td><td>arXiv</td><td>方法改进</td><td>基于 Bidirectional Encoder Representations (BERT),使用模型剪枝、量化和编译器优化。S</td><td>SIGHAN2005[]（CityU、PKU、MSR、AS) SIGHAN2008[84]（SXU) CoNLL2017[86](UD)</td></tr><tr><td>2019（</td><td>Qiu等[87]</td><td>arXiv</td><td>方法改进</td><td>基于Transformer的构架方法采用全连接自注 SIGHAN2005[I]（CityU、PKU、MSR、AS） 意力机制。</td><td>SIGHAN2008[84](CTB、SKIP、NCC、SXU) 每一个句子的开头和结尾增加人工标记,以 SIGHAN2005[1](MSR、AS、PKU)</td></tr><tr><td></td><td>2019He等[88]</td><td>SCI</td><td>语料改进</td><td>区分多粒度语料。再使用LSTM和CRF实现 多粒度分词。</td><td>SIGHAN2008[84]（CTB、SKIP、CityU、NCC、 SXU)</td></tr><tr><td></td><td>2019张文静等[82]中文信息学报</td><td></td><td>语料改进 方法改进</td><td>模型在网格结构的辅助下,对不同粒度的分 词标准都有较强的捕捉能力,且不局限于单 MSR[89]、PPD[90]、CTB[72] 一的分词标准。 借鉴多任务学习的思想,融合多个语料的数</td><td></td></tr><tr><td></td><td>2017Chen等[91]</td><td>ACL</td><td></td><td>据提升共享字向量模块。在此基础上应用对 SIGHAN2005[I](MSR、AS) 方法改进 抗网络,把私有信息从共享模块中剥离到各 SIGHAN2008[84](PKU、CTB、SKIP、CityU、 个私有模块中去,既有大数据量的优势,又避 NCC、SXU) 免了不同语料之间的相互制约。</td><td></td></tr><tr><td></td><td>2017Gong等[83]</td><td>EMNLP</td><td>语料改进构建多粒度语料库。</td><td></td><td>MSR[89]、PPD[90]、CTB[72]</td></tr></table>"
  },
  "4.2与中文分词相关的联合模型": {
    "context1": "传统的自然语言处理任务通过管道模型实现，即中文分词作为一个独立的任务建模,再将分词结果作为后续任务的输入。管道模型存在错误传播的问题,不同任务之间信息的共享非常有限。近年来,与中文分词相关的联合模型大量涌现,联合模型能够避免错误传播，同时提升各子任务的准确率。",
    "context2": "自然语言处理包括多个相关子任务。其中，与中文分词任务最紧密关联的任务是词性标注(Part-of-Speech Tagging，POSTagging）和依存句法分析（DependencyParsing）。已有中文分词相关的多任务处理研究中,被研究最多的是中文分词和词性标注;其次为中文分词、词性标注和依存句法分析。除此之外,多任务还包括中文分词和依存句法分析；所有相关任务的自然语言处理统一框架;中文分词和未登录词识别;中文分词和非正式词检测;中文分词和中文正确拼写;中文分词和命名实体识别。筛选并统计分析重要的多任务联合模型研究，如表3所示。现有研究基本都是基于2004年Ng等[7提出的交叉标记思想,再结合具体任务设计出一种统一的多任务标注方式，最后提出多任务联合的深度学习模型。融入依存句法和已有知识后，CNN和RNN都不能很好地解决这种具有网络结构的模型，图深度学习模型[74]开始被自然语言处理研究人员关注，未来如何结合和改进深度学习中的图深度学习模型[92]实现多任务学习，将成为重点研究方向。",
    "context3": "联合模型能够使模型下的多个子任务间的信息交互更为便利，提升结果的准确率和效率,但会导致模型更加复杂、搜索空间增大等问题。目前的研究尚且粗浅，未来将进一步探究结合不同自然语言处理任务的联合模型。"
  },
  "5结语": {
    "context1": "中文分词作为自然语言处理的基础任务，对自然语言处理有着重要意义。本文从中文分词的发展历程和关键问题出发，梳理对推动中文分词的发展起到重要作用的历届评测和相关国际会议。总结中文分词的研究现状，分类梳理中文分词的算法、模型以及与中文分词相关的联合模型。",
    "context2": "经过多年努力，中文分词研究已经取得了相当"
  }
}