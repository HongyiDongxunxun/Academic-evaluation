{
  "original_filename": "full_1813.md",
  "信息检索中的相关性判断和系统评价述评": "",
  "秦喜艳陆伟姜捷璞": {
    "context1": "(武汉大学信息资源研究中心，武汉,430072)",
    "context2": "[摘要]信息检索系统的效果评价是信息检索研究必不可少的环节，也是检验信息检索理论和进行方法研究的重要途径之一。本文首先从相关性理论出发，讨论了信息检索中检出文档相关性的维度和度量问题，以及等级相关性判断和大规模数据集的相关性评估方法。其次，进一步介绍了信息检索系统效果评价的基本指标查全率和查准率，并对常用评价指标 MAP、Bpref 等进行了比较和分析。最后，笔者认为未来信息检索系统效果评价的研究应注重将用户纳入评价体系中，使评价结果更具有真实性和实用性。",
    "context3": "[关键词]信息检索相关性判断评价 [中图分类号]G203[文献标识码]A [文章编号]1003-2797(2009)04-0089-06 [Abstract]The evaluation on the effectiveness of information retrieval sy stem is indispensable in information retrieval study $,$ and is also one of the important approaches to testify theories and study methods in IR fields·Firstly，this paper discusses the obstacles about dimensions and measures of relevance，and approches for judging relevance of documents on the basis of relevance framework， such as rank relevance judgment，and etc. Secondly， the paper focuses on the two basic measures (Precision and Recall ），and frequently used measures for evaluating and IR system，then make a comparison between them $\\cdot$ Finally，the future study of information retrieval effectiveness evaluation should give more focus on Users，which makes the evaluation results more valuable.",
    "context4": "[Key words] Information retrieval Relevance judgmentEvaluation",
    "context5": "信息检索系统的评价一般涉及两个方面[1]：即有效性(Effectiveness)和效率(Efficiency）。有效性是针对信息检索系统的目的对结果作出的评价，主要用于衡量不同检索模型和算法的优劣。它主要包含客观性评价和比较性评价两种评估类型，这将是本文具体论述的问题范畴。效率则是针对计算机系统的一系列通用评价指标,主要包括响应时间、占用空间等。本文提及的信息检索评价，主要指从信息检索有效性方面衡量的评价方法。",
    "context6": "在衡量系统信息检索有效性时，首先需要确定一个标准测试集，然后根据不同的检索目的和标准确定不同的评价指标。在此，标准测试集包含查询主题集、文档集和表示查询主题与文档之间相关程度的相关性评估集，其中相关评估集的确定显得尤为重要，因为它是检索系统评价的事实依据。因此，检索系统评价与检出文档的相关性判断问题密不可分。但是，由于信息检索不仅具有内在的不确定性，又是一个用户参与的过程,导致相关性问题可操作性差，使得相关性判断具有很大的复杂性和不确定性。"
  },
  "1系统检出文档的相关性判断": "",
  "1.1相关性的维度和度量问题": {
    "context1": "传统的信息检索研究仅仅把主题相关性作为相关性的决定标准，因为传统系统仅支持查询与文档(或文档表达)在算法或主题层次上的匹配[2]。20世纪90 年代以来，人们对其他方面的相关性进行了研究，把相关性看作是一个多维的、动态的概念，而不再简单地把之等同于主题相关。Mizzaro 用四维模型来解释信息检索中的相关性[3],认为相关性包括信息源、用户、时间和构件四个维度。Saracevic的分层模型[4]中由低到高将相关性划分为系统或算法相关、主题相关、认知相关、情境相关和动机相关等多个层次。然而，目前绝大多数信息检索中的相关性只涉及四个维度中的信息源维度及用户维度的部分因素，或者分层模型中的主题相关层次的相关性，这在一定程度上影响了系统有效性评价的效果。",
    "context2": "另外，文档相关性的度量也存在着一定的争议。Goffman曾经指出，相关性并不完全满足数学上作为测量值所必须满足的四个必要条件(测量客体、测量内容、数字或符号、分配数字或符号的法则)[5]尽管上述结论很大程度上与Goffman 所采用的相关性定义有关，但却也引发了人们对相关性可测度问题的思考。此外，大量实证实验也表明：一方面，影响用户相关性判断的因素多种多样[]；另一方面，相关性评价的标准也各不相同[7]。这一系列的原因使得制定相关性判断的标准的问题更加棘手。"
  },
  "1.2人工相关性判断的可行性": {
    "context1": "虽然相关性判断问题很棘手，可测度及可操作性都比较差。但是,Salton 在SMART 系统上关于相关性判断问题的实验[8],得到了如下的结论：虽然不同用户对于同一检出文档的相关性判断结果(得分)会有较大的差异，但是对于多个检出文档而言，用户对结果的相关程度判断的相对次序却是大致稳定的。而且，多个用户之间关于相关性判断的不一致性很大程度上可以通过讨论来消除。",
    "context2": "由此可见，通过人工相关性判断的方法来对检出文档的相关性进行评估是可行的。首先，尽管检出文档的相关性判断涉及多个维度，不可能通过一次评估实验而得出一个完整的相关性结果，但单就系统相关性而言，只涉及查询主题与系统输出文档之间客观存在的相关性，因此对文档和查询主题之间的相关性进行人工判断的做法是可行的。其次，虽然不同用户各自采用的评价标准和所处的环境可能存在较大的差异，但综合大量均衡分布用户的判断结果则应该呈现出一定的稳定性和客观性。"
  },
  "1.3等级相关性判断的方法": {
    "context1": "从早期的Cranfield实验，一直持续到近年来的TREC 会议，对相关性基本都采用等级相关性的评价方法。它首先把相关性按照相关程度分为几个等级，然后组织会议参加者或者与数据集有密切关系的人员对相关性进行人工判断评分，最后综合各种评分结果。例如，由Sormunen 建立的TUTK集合[9],TREC 的多级测试集及INEX 测试集[10]（2002 年开始，首次包含了对XML检索的评价）,都采用了四级等级相关性判断。但是，因为参与评估的人员之间的个体差异并不仅仅体现在各自不同的评估尺度和信息需求上,还体现在其个人的认知能力、工作任务以及所处的组织和社会环境的影响。尤其是当判断所涉及的文献具有非常专业的领域范畴时，由于缺乏相关领域知识,检索用户很难做出正确的相关性判断[11]。因此,这种方法存在一定的缺陷。",
    "context2": "近年来，有部分实测检索会议注意到了上述问题，因此，许多涉及特定领域检索的实验都专门选择了具有特定背景的人员进行相关性评估。例如，TREC会议企业检索的专家检索子项目在2006 年及以前都是由所有参与测试的研究者来进行相关性评估，而在 2007年则改由其选择作为数据集的网页所属组织的少数内部人员直接给出相关性评估结果[12]。",
    "context3": "这种相关性判断的方法一定程度上满足了现有检索系统评价的需要，但也存在一系列的问题。其中最为明显的莫过于现有的评价方法中对相关性的判断都未能超出系统观的范畴。此外，目前的相关性评估只要求得到检出文档集中各个文档的相关性高低程度的排序列表，而对相关性的精确定量问题并未进行深入的讨论。尽管目前检索系统的输出只要求按照相关性排序，但对更进一步的信息分析和挖掘而言，没有较为精确的相关性定量方法及指标是难以支撑的。"
  },
  "1.4大规模数据集的相关性评估": {
    "context1": "早期信息检索研究的数据集规模都非常小，例如,著名的Cranfield 实验，其中第二阶段的数据集(CranfieldII)只含有1400 篇文档,225个查询。然而，为了对数据集中每个查询和文档对的相关性都进行评估，当时耗费了五位相关专业研究生共计1500 个小时的时间[13]。由此可见，相关性评估是一项既耗时又费力的过程。20世纪90 年代以后，陆续出现了一些针对大规模数据集的相关性判断方法，这些方法只需对数据集中的小部分文档进行判断，便能取得与穷尽性判断相近的效果。",
    "context2": "(1)Pooling 方法：从1992 年开始，NIST开始在TREC 会议中使用Pooling 方法，其主要思想为[14]：针对每一查询主题，从各参评系统返回的结果中各抽取出前n篇文档，经过归并去重后形成一个文档子集，然后，将集合中的文献提交给评价人员最终判断每篇文档的相关性。相关性判断只针对子集内的文档进行，而子集外的文档都假设为不相关，不进行判别。",
    "context3": "Pooling方法的有效性主要与两个因素有关：Pooling深度和参评系统的数量。Pooling深度是指从各个系统结果中抽取文档数量的临界值，一般来说,深度越大，则越能真实地模拟穷尽性判断结果,但是人工判断成本和时间耗费也越大，而且达到一定的深度，系统效率虽有变化但却非常微小，因此增加深度作出进一步的判断是没有必要的;参评系统的数量越多，采用的查询公式和检索技术就会相应增多，能够识别出的相关文档数量就越多，则判断效果也越准确。在TREC中，一般选取深度1OO 和尽可能多的参评系统来对系统性能进行评测。",
    "context4": "但是,TREC 测试集引起了一些学者的讨论和质疑。如Burgin[15]，Harter[16]，Voorhees[17]和Buckley[18]和Zobel[19]都对测试集的可信度与健壮性做了全面的分析和测试，其中Zobel通过实验论证,认为TREC 测试集中只占相关结果的 $50 \\% \\sim$ $70 \\%$ 。Blair[20]对TREC 测试集的有效性提出了质疑。",
    "context5": "(2)无需相关性判断的方法：Pooling方法虽然对大规模数据集而言比较有效，但针对类似互联网的动态数据源则显得力不从心。一方面，互联网的数据量远高于一般的大规模数据集;另一方面，互联网数据集时时刻刻都在更新，具有很强的动态性，很难进行重复性和可比性试验。",
    "context6": "2001年,Soboroff 等人[21]提出的无需相关性判断的排序方法则为超大规模动态数据集的相关性评估和检索效果评估提供了一定的参考。这种方法的主要思想是：首先构造一个查询主题集，由参评系统根据相应的查询主题提交结果;然后针对每个查询主题，对参评系统检出的文献进行聚类,模拟相关文献的出现频率并建立反映文献分布状态的模型，最后根据模型随机选择相应数量的文档组成参考集。这种方法与近年来TREC采用的Pooling方法有许多相似之处，最主要的不同表现在相关评估集的构造方法上。它是根据文献的分布状态构造模型，任意挑出了一些文献作为相关文献构成相关评估集，不需要评价人员对文献的相关性做出判断。",
    "context7": "总之,这两种方法得到的系统等级基本一致。Soboroff等人对两种方法进行了比较验证，实验结果如表 $1 ^ { [ 2 2 ] }$ 所示：其中 $\\tau$ 表示KendallTau 相关系数。从中可以看出，两种方法具有很大的相似性。",
    "context8": "表1两种方法排序等级的 $\\boldsymbol { \\tau }$ 关系比较",
    "context9": "<table><tr><td>年份</td><td>平均差(τ)</td><td>标准差(t)</td></tr><tr><td>TREC-3</td><td>0.430</td><td>0.0312</td></tr><tr><td>TREC-5</td><td>0.487</td><td>0.0462</td></tr><tr><td>TREC-6</td><td>0.408</td><td>0.0354</td></tr><tr><td>TREC-7</td><td>0.369</td><td>0.0363</td></tr><tr><td>TREC-8</td><td>0.459</td><td>0.0340</td></tr></table>"
  },
  "2信息检索系统评价指标": "",
  "2.1基本评价指标": {
    "context1": "标准的结果集建立后，就可依据结果集选择不同的评价指标进行系统检索效果的评价。衡量系统检索效果的两个最基本、最经典的指标是查全率( $\\mathrm { R e ^ { - } }$ call)和查准率(Precision),查全率和查准率的概念最早由Perry 和Kent在1955 年提出,查全率是指结果集中的相关文档在系统总相关文档中占的比例，而查准率是指结果集中相关文档在系统所有文档中占的比例[23]。Cleverdon在1962 年首次将它运用于实际信息检索系统的评价实验(CranfieldII)中。",
    "context2": "一直以来，查全率和查准率存在多方面的争议，但是现今大多数系统还是基于它们来评价的，由于过去 Raghavan[24]、张保明[25]等学者已对它们之间的各种关系进行了详细的介绍和探讨，这里不再进行详细论述。需要清楚的是，单纯的依赖一种指标来评价系统的性能是不可靠的，也无法得到普遍认可。因此，应该寻求多方面的指标，综合地评价系统检索效率。"
  },
  "2.2常用评价指标": {
    "context1": "20 世纪90年代以来，针对不同的环境和评价目标,在查全率和查准率的基础上派生出了一系列评价指标。本文选取TREC 特定检索任务(ad hoctracks)使用的trec_eval包中的85 种检索评价指标中的四个常用指标 MAP、Bpref $\\mathbf { \\nabla } \\cdot \\mathbf { P } ^ { \\circledalpha } \\mathbf { N }$ 和R-Preci-sion,并对它们的基本思想给予介绍。",
    "context2": "(1)MAP（Mean Average Precision):MAP 是反映系统在全部相关文档上性能的单值指标。如果与一个查询 $q _ { j } \\in Q$ 相关的文档集是 $\\{ d _ { 1 } \\ldots d _ { m } \\}$ ,而且$R _ { j k }$ 是从第一个结果直到获得文档 $d _ { k }$ 的排序检索结果集，可得出公式(1)[26]",
    "context3": "$$\nM A P ( Q ) = \\frac { 1 } { \\mid Q \\mid } _ { j = 1 } ^ { \\mid Q \\mid } \\frac { 1 } { m } \\sum _ { k = 1 } ^ { m _ { j } } P r e c i s i o n ( R _ { j k } )\n$$",
    "context4": "对于单个查询主题( $[ 0 ^ { = 1 }$ )，MAP是指每篇相关文档检出后的查准率的平均值，是单个查询的一定查全率水平下的查准率的平均化;而对于多个查询主题( $( \\imath ) ^ { > 1 } .$ ),MAP是单个查询主题的平均查准率的再平均化处理，侧重于对系统性能的整体评价。",
    "context5": "(2)Bpref （Binary preference）:在 SIGIRO4中,Buckley和Voorhees提出了Bpref,它主要针对的是相关性判断不全面的情况。它计算的是已判断为不相关文档在已判断为相关文档之前的二元偏好关系，而且它的每个相关文档的得分不依赖于所有其他相关文档的得分，只基于已判断文档的相对排列顺序。",
    "context6": "偏好关系（Preferencerelation）可以通过文档对的比较来描述，通过偏好关系评估者可以明确的判断一个文档对及在一定的相关性等级上判断文档， $\\mathrm { Y a o } ^ { [ 2 7 ] }$ 等人基于文档的偏好关系对系统效率进行了衡量，并给出了偏好关系的明确定义。",
    "context7": "TetsuyaSakai根据Bpref仅仅依赖已判断文档的特性对Buckley 和Voorhees 最初提出的公式进行了修正,在计算Bpref 时从评估列表中移除了所有未判断过的文档，进而获得一个仅包含已判断文档的列表。假如用 $r$ 表示文档在原有结果中的等级， $\\displaystyle { \\mathop { \\prime } _ { r } } ( \\le r )$ 表示文档在新的列表中的等级,Bpref 可表示为公式(2)[28]：",
    "context8": "$$\nB p r e f = \\frac { 1 } { R } \\Sigma _ { i s r e l ( \\mathit { \\acute { r } } ) } ( 1 - \\frac { \\operatorname* { m i n } ( R , \\mathit { \\acute { r } } - c o u n t ( \\mathit { \\acute { r } } ) ) } { \\operatorname* { m i n } ( R , \\mathit { N } ) } )\n$$",
    "context9": "其中：R表示已判断为相关文档的数量;N表示已判断为不相关文档的数量;isrel(r)表示文档在等级r处的相关性，相关为1,反之则为O;count(r)表示在排序列表的前r个等级相关文档的数量，则r一count(r)表示前r个等级不相关的文档数量。这种方法对基于不完整的相关判断集的实验很具有吸引力，在实际中表现的也特别好。但是它不能够解决相关文档数量非常少(1个或2个)时产生的问题，只能够通过限制文档对的数量来解决。",
    "context10": "$( 3 ) \\mathbf { P } ^ { \\mathcal { ( Q } _ { N } } . \\mathbf { P } ^ { \\mathcal { ( Q } _ { N } }$ 是指检出结果集前N个结果的查准率。 $\\mathrm { P } ^ { \\mathrm { \\infty } } \\mathrm { N }$ 值的大小与输出的前N个文档中相关文档的数量有关，与相关文档所处的位置无关。在真实应用环境中，有时候不可能也没有必要对系统输出的所有结果进行相关性判断，如Web搜索，用户往往不关心搜索引擎产生多少相关结果，重要的是有多少准确结果排在第一页或前几页;一般说来，相关程度越高的结果越靠前，用户的满意度就会越高。也就是说，对Web搜索引擎的评估是看它是否能够检出高度相关的页面，而不是所有可能的相关页面，TREC-9的Web任务已经开始了对高度相关文档的评估[29]。",
    "context11": "(4)R-precision:R-precision 是指检出 R个相关文档时的查准率,其中R为给定主题的相关文档数量，这种方法涉及的主要问题在恒定的临界值水平使用查准率评价每个主题，在这里采用Precision和 Recall产生的效果是一样的。例如：针对一个查询有R个相关文档,我们查看系统输出的前R个结果,找到有 $\\mathbf { r }$ 个相关结果，那么根据查全率和查准率的定义，R-precis $\\scriptstyle \\mathrm { i o n } = \\mathbf { r } / \\mathbf { R } ; \\mathbf { R } \\mathrm { \\cdot r e c a l l } = \\mathbf { r } / \\mathbf { R }$ 。",
    "context12": "此外,Kekainen 和 $\\mathrm { J i v e l i n ^ { [ 3 0 ] } }$ 提出了基于传统评价方法(查全率和查准率)的新方法一一—泛化的查全率和查准率;同时提出了全新的累计收益率方法(DCG)方法。其中，泛化的查全率和查准率方法与传统的指标相差并不是很大;而DCG方法却是直接面向检索用户的，它直接反映动态的信息需求和相关性等级。2005年INEX会议也提出了面向用户的 $\\mathbf { n x } ^ { - }$ $\\mathrm { C G ^ { [ 3 1 ] } }$ (属于 XCG)方法,为结构化文档的不同粒度的评价提供了基础。杨立英等人[32]还提出了加权中值法,把用户评价时对系统性能的所持的态度考虑了进去。但笔者认为，这种方法应用的参数较多，显得比较繁杂，不太适合大型数据集的评测。"
  },
  "3四种常用评价方法的比较": {
    "context1": "可以从不同的角度对几个常用指标进行比较，它们之间的关系如表2所示。",
    "context2": "表2四种常用评价方法的比较",
    "context3": "<table><tr><td rowspan=1 colspan=1>评价指标侧重点</td><td rowspan=1 colspan=1>MAP</td><td rowspan=1 colspan=1>Bpref</td><td rowspan=1 colspan=1>P@N</td><td rowspan=1 colspan=1>R-precision</td></tr><tr><td rowspan=1 colspan=1>系统区分能力</td><td rowspan=1 colspan=1>好</td><td rowspan=1 colspan=1>好</td><td rowspan=1 colspan=1>差</td><td rowspan=1 colspan=1>较差</td></tr><tr><td rowspan=1 colspan=1>评价结果稳定性</td><td rowspan=1 colspan=1>较高</td><td rowspan=1 colspan=1>高</td><td rowspan=1 colspan=1>低</td><td rowspan=1 colspan=1>较低</td></tr><tr><td rowspan=1 colspan=1>相关文档影响</td><td rowspan=1 colspan=1>受相关文档总数影响大</td><td rowspan=1 colspan=1>依赖于已判断文档的相对顺序</td><td rowspan=1 colspan=1>依赖前N个结果中的相关文档数</td><td rowspan=1 colspan=1>受相关文档的排序位置影响较大</td></tr><tr><td rowspan=1 colspan=1>查全率/查准率要求</td><td rowspan=1 colspan=1>对查全率/查准率的综合评价</td><td rowspan=1 colspan=1>对查全率/查准率的综合评价</td><td rowspan=1 colspan=1>只关注查准率</td><td rowspan=1 colspan=1>只关注查准率</td></tr><tr><td rowspan=1 colspan=1>适用环境</td><td rowspan=1 colspan=1>相关结果集具有相对的完整性</td><td rowspan=1 colspan=1>结果集不完整或数据动态性强</td><td rowspan=1 colspan=1>Web 搜索引擎</td><td rowspan=1 colspan=1>对用户满意度和忍耐度的评价</td></tr></table>",
    "context4": "首先，一般来说,MAP侧重于对系统整体性能的评价，且具有很好的区分不同检索系统的能力，在许多实验中往往以MAP为基准进行系统排序。如果基于完整的相关结果集，MAP和Bpref两种方法得到的系统等级基本是一样的，对系统性能的的判断结果也是一致的，此时可选用其中任意一种指标来判断。P@N和R一precision 是针对用户对系统高的查准率的要求而产生的方法，它们仅仅描述的是查全一查准曲线上的单个点，而不是试图对整个曲线的有效性进行概括，不适合系统整体性能的分析比较。",
    "context5": "其次，Bpref 对基于不完整的相关信息的系统具有很好的评价效果。Buckley 和Voorhees 不断的减少相关评估集的完整性的实验研究[33]结果表明：对于不完整的相关结果集Bpref 要更稳定。同时,Bpref可基于小的广泛被认可的的不完整数据集(嵌入更大型的数据集中)，对大型数据库运行效果进行研究。",
    "context6": "再次,R-Precision、MAP 和 $\\mathrm { P } ^ { \\mathrm { \\infty } , 1 0 }$ 值是根据结果集中相关文档的排序结果计算所得的，没有考虑到结果集中明确判断为相关的文档和结果集外假定为不相关的文档之间的区别。而Bpref 的相关文档的得分不依赖于其他相关文档的排序等级，只基于已判断文档之间的相对顺序，这一理论使它比MAP更能经得起实践的检验。",
    "context7": "最后，虽然 $\\mathrm { P } \\ @ 1 0$ 和R-precision 稳定性比较差，但由于它们受结果集中相关文档总数的影响不大，显得简单易行,可用于Web 搜索引擎检索效果的一般性比较。"
  },
  "4结语": {
    "context1": "信息检索的核心[34]是在文档集中为用户检出最相关的子文档集，并且按检出文档的相关程度进行排序。而怎样的文档对用户来说是最相关的及相关程度如何却难以把握。于是，本文围绕标准测试集的构造过程中涉及相关性判断的一系列问题展开论述，并对评价检索系统的“客观公正”的尺度一查全率和查准率进行了简单介绍。尽管查全率和查准率在信息检索效果评价中非常通用，但是它们都有一定的局限性。如果仅用查全率和查准率来评价信息检索系统的效果，难免有些片面。因而出现了上文所述的在查全率和查准率的基础上派生出的一系列评价指标。",
    "context2": "尽管如此，当前的这些评价方法和指标大多未能考虑用户之间的个体差异。在当前信息检索个性化发展趋势下,应该将具体用户纳入信息检索系统效果的评价体系之中。因此，信息检索的评价应该同时考虑客观和主观两方面，应该在对检索结果客观性评价的基础上，考虑相关性主观因素。然而采用何种指标把用户的因素考虑进去，使评价结果更具有真实性和实用性,还需要进一步的探讨和研究。"
  },
  "参考文献": {
    "context1": "(唐)李肇·国史补·卷3·四库全书本.  \n5 何远景·鱼尾的起源·文献,1999(4)  \n6 黄永年·古籍版本学·南京：江苏教育出版社,2005.147.  \n7 (清)于敏中等·天禄琳琅书目·卷7·春秋经传集解·扬州：江苏广陵古籍刻印社，1992.511.",
    "context2": "（收稿日期：2009-03-17)"
  },
  "（上接第80页）": {
    "context1": "25 张保明．查全率-查准率互逆相关性的数学解释．情报科学，1982(2)  \n26 Christopher D，Raghavan V，Sch tze H. Introduction toinformation retrieval． Cambridge University Press，159-161，2008.  \n27 Yao Y. Measuring retrieval effectiveness based on userpreference of documents·Journal of the American Socie-ty for Information Science，46(2)：133-145，1995.  \n28 Sakai T．Alternatives to Bpref ·Proceedings of the $3 0 _ { \\mathrm { t h } }$ annual international ACM SIGIR，71-72，2007.  \n29Voorhees E．M．Evaluation by highly relevant docu-ments·Proceedings of the $2 4 \\mathrm { { t h } }$ annual internationalACM SIGIR，2001.  \n30 Ingwerson P,J rvelin K. The Turn; Integration of infor-mation seeking and retrieval in context．1 edition·Springer，2005,9.  \n31 Kazail G，Lalmas M. INEX 2OO5 evaluation measures·4th InternationalWorkshop of the Initiative for the Evalu-ation of XML Retrieval，2O05.  \n32 杨立英,尚克聪,李荣陆·评价情报检索系统性能的新方法-加权中值法·情报学报，2003(1)  \n33 Buckley C，Voorhees E.M.Retrieval evaluation with in-complete information·Proceedings of the $2 7 \\mathrm { t h }$ annual in-ternational conference，2004.  \n34 王家钺·信息检索中\"相关性”概念的研究．现代外语，2001(2)  \n1鲁迅·鲁迅全集·卷3·忽然想到·北京：人民文学出版社，1991.15.  \n2(南朝宋)范晔·后汉书·卷30·襄楷传·北京：中华书局，1986,1075.  \n3(唐)蒋防.霍小玉传·唐人说荟本.",
    "context2": "（收稿日期：2008-06-09)"
  }
}