{
  "original_filename": "full_10140.md",
  "面向检索结果集的结构化综述智能生成研究": "",
  "■孟旭阳　陈阳　白海燕": {
    "context1": "中国科学技术信息研究所北京100038",
    "context2": "摘要：[目的/意义]在学术文献检索与阅读场景下，当前学术信息量已远超用户信息处理能力，造成信息堆积。为应对用户阅读效率与知识吸收难题，面向学术文献检索结果集开展内容的综合挖掘揭示。［方法／过程]一方面从阅读体验出发，针对文献检索场景的特点，进行结构化综述表达设计；另一方面从技术方法与内容质量提升出发，利用基于深度学习的文本自动生成技术，构建科技文献数据集，训练并优化文本摘要模型，在此基础上利用大语言模型技术实现结构化的综述文本生成。[结果/结论]训练优化后的摘要模型在各指标的召回率和F1值上平均增长 $2 . 0 7 \\%$ 。基于大模型的结构化综述生成，在实际测评中能够有效地提炼、总结和归纳内容要点，验证本文技术路线和应用实践的可行性，为进一步提升学术文献的知识化服务水平、智能辅助阅读与语义内容综合挖掘揭示等方面提供应用实践指南。",
    "context3": "关键词：文献检索结构化综述大语言模型文本自动生成  \n分类号：G254  \nDOI: 10.13266/j.issn.0252-3116.2024.06.012",
    "context4": "引用本文：孟旭阳，陈阳，白海燕．面向检索结果集的结构化综述智能生成研究[J].图书情报工作，2024,68(6):129-141. (Citation: Meng Xuyang,Chen Yang,Bai Haiyan. Research on Intelligent Generation ofStructured Review for Retrieval Result Set[J]. Library and Information Service,2024, 68(6): 129-141.)"
  },
  "1引言 /Introduction": {
    "context1": "在大数据和数字化时代背景下，数字资源爆炸式增长，检索系统返回的结果集也越来越多。作为承载科研人员思维火花的高密度知识载体，学术检索系统面临的问题尤为突出。",
    "context2": "对国家科技文献图书中心（简称NSTL）用户的部分检索日志进行统计（2023.04.10—2023.05.09），发现选择快速检索功能的用户占全体用户的 $9 5 \\%$ 以上，检索返回结果数大多为千条甚至万条级别；在选择专业或高级检索功能的用户中，即便是领域较小的学科，返回结果数也集中在几百条左右。那么，面对成百上千甚至成千上万篇的文献，用户的实际吸收情况如何呢？曾有研究对Google用户检索行为进行统计，发现超过 $70 \\%$ 的用户只查看检索结果首页，平均每次查看1.7个结果页面[1]，相似的情况也出现在 Excite搜索引擎中[2]。由此可见，在学术论文检索与阅读这一实际场景中，当前学术信息量已远超用户信息处理能力，用户群体阅读效率偏低，吸收能力也相应弱化。如何帮助用户对论文检索结果集进行高效阅读、筛选和吸收，减轻用户的认知负担，把用户从过载的信息中解放出来，这是一个具有迫切性和现实意义的研究问题。",
    "context3": "自动综述是指针对特定的主题进行多文档自动文摘，最终提供简洁、重要的信息[3]。面向学术检索系统，检索结果集的自动综述指针对系统返回的多篇科技文献，自动分析、归纳与总结，对多篇文献内容进行简洁而全面的概括。结构化综述是指按照一定的结构对多篇文献进行梳理总结，以结构化的表达形式提供层次分明、简洁清晰的综述信息，这种形式可以使用户快速观其大略，提升阅读效率。",
    "context4": "另外，每篇文献作者的侧重点和写作方式不同，因此文献间具有相对的重复冗余性、矛盾性、差异性等特点。建立怎样的综述框架，采用什么技术路线，以何种表现形式进行服务，都是值得探讨的问题。",
    "context5": "因此，笔者面向学术检索系统，在用户输入检索词，系统返回多篇文献的场景下，开展面向检索结果集的结构化综述研究。研究目标是从检索到的成千上万篇文献中提取看点和要点，对文献中的内容进行归纳总结、综合揭示，帮助用户进行高效阅读、筛选和吸收，减轻用户的认知负担。在研究内容上笔者不仅关注综述文本生成方法研究带来的内容质量上的提升，同时也聚焦特定场景下内容组织方式及服务形式（综述表达形式）设计带来的阅读体验上的提升。在两者的共同作用下，以期实现高效阅读服务效果的最大化。"
  },
  "相关研究 /Literature review": {
    "context1": "本文面向检索结果集的自动综述是多文档自动摘要的一种应用形式。自动文本摘要从H.P.Luhn[4首次提出，至今已经有60多年的发展历程。按照对象文档数量进行划分，可分为单文档自动摘要和多文档自动摘要。多文档的自动摘要可看作是单文档的一种扩展，与单文档不同的是需要考虑文档与文档之间的关系，如相似性、差异性、冗余性等特点。无论从涉及的技术，还是实用价值和意义上来看也都更加广泛。",
    "context2": "从研究的文本对象来看，学者们针对不同类型的文本进行了相关探索研究，如新闻、微博、网页、科技文献等。其中，涉及的有结构化文本也有非结构化文本，有短文本也有长文本。从面向的应用场景来看，有面向主题的自动文摘[5、面向问题的自动文摘[、面向对话的自动摘要、面向查询的自动文摘[等研究。",
    "context3": "从生成的综述／摘要表达形式上来看，目前主要有以下3种表达形式： $\\textcircled{1}$ 一段式非结构化的纯文本。如基于抽取式文摘句的排序输出[8]。 $\\textcircled{2}$ 基于固定模板的结构化文本。对于不同的文本对象、不同的主题信息都没有一个清晰的描述标准，多数学者均基于人工综述／文摘的观察、总结归纳定义应用场景下合适的模板表达方式。薛竹君[对人工创作新闻专题综述的写作方法进行分析，总结归纳出综述报告的行文规则，通过挖掘出来的相关语义信息，按照报告行文规则组装成综述报告。也有学者研究自动生成文摘描述模板，李鹏[5在已经聚类并且标注好的句子的依存解析树上利用频繁子树模式挖掘算法来构建面向主题的文摘描述模板。 $\\textcircled{3}$ 文字和图表相结合的呈现方式[9]，常见于新闻专题综述。",
    "context4": "从摘要生成的方法上来看，主要分为抽取式文摘[10] 和生成式文摘[11]。抽取式文摘指利用统计学等原理对原文本中语句的重要度进行计算，挑选出得分较高者形成重要句子集合，并在其中对句子进行排序与重组形成摘要。生成式文摘指将原文本内容抽象表示为中间形式的语义表达，再通过自然语言生成技术对其进行解码，形成概括性摘要，与人工总结凝练过程更加接近。徐晓丹等[12]在研究面向查询的多文档自动摘要时，通过聚类得到子主题，根据查询语句与子主题的相似度进行排序，从每个聚类中选择一个得分最高的句子组成摘要；C.Shen 等[13]基于有监督的机器学习方法，将自动文摘任务转化为二分类问题，通过提取句子位置、长度、与查询句子的相似度等特征，使用SVM机器学习模型进行有监督的训练；王红斌[14]等提出一种结合层级注意力的抽取式新闻文本自动摘要方法，首先对新闻文本进行层级注意力的文本编码表示，然后通过神经网络构建动态打分函数选择分值高的句子作为摘要句，具有较好的泛化性与有效性；J.Chen 等[15]将抽取式摘要看作序列标注问题，并采用神经网络的处理方式解决，取得了较好的效果。与此同时，利用神经网络、深度学习在生成式文本摘要上的相关探索也逐渐发展起来。王勇臻等[16]面向学术文献，将文献综述生成定义为序列文本生成问题，基于 Seq-to-Seq 框架构建基于层次神经网络的序列生成模型，实现文摘的生成；王茂发等[]提出一种生成式自动文摘网络模型，使用堆叠BiLSTM和多头注意力机制（multi-headattention）相结合的方法提升生成摘要的质量。此外，也有一些学者将抽取式和生成式方法结合起来进行摘要的生成[18]，有较好的应用。2022年11月30日，OpenAI公司研发的对话系统ChatGPT[19]发布，随着以ChatGPT为代表的人工智能技术的快速发展和不断更新迭代，展现出更高的文本生成及综述能力[20]。新技术方法的突破为支持和实现更高质量的文献服务提供了新的方法和技术支撑。",
    "context5": "抽取式文摘的多数研究聚焦在句子的相似度和权重计算、句子的选择／筛选以及句子排序方法上，还有的研究关注特征的选择和改进上。生成式文摘的多数研究都聚焦在深度学习算法的优化上。总体来看，抽取式文摘适用性较广，不受文本内容和题材限制、泛化性好，但是容易局限文本表层结构分析，难以深入理解文本内容，存在内容覆盖不全、词汇冗余等问题。生成式文摘更接近人工撰写摘要的方式,能对文本内容进行深层次的语义理解，生成的摘要更具有简约性、概括性。并且，近年来深度学习作为前沿技术在多文档自动摘要领域的热度不断攀升[21],以及以ChatGPT为代表的生成式大语言模型，因其强大的生成能力使生成式预训练模型等深度神经网络方法在自动摘要、综述生成任务中取得了比传统方法更为优异的表现，给该任务的研究带来了更大的潜力。",
    "context6": "目前的研究大多专注于文摘生成方法的研究以提升生成内容的质量。然而面向科研文献文本对象，面向学术搜索引擎下的文献检索结果，在这样特定的实际应用场景下，服务形式及综述表达形式设计也同样至关重要，合理的表达及服务形式再加上知识的高效提取归纳方法，在两者的共同作用下，才能实现高效阅读服务效果的最大化。因此，笔者面向学术文献检索结果集，聚焦该场景下的综述生成内容组织及服务形式的框架设计研究以及综述生成的方法研究，期望以结构化的高质量综述内容帮助用户进行高效阅读、筛选和吸收。"
  },
  "3 研究内容 /Research content": "",
  "3.1结构化综述框架设计": {
    "context1": "经过相关调研梳理和总结，从用户实际需求角度出发，面向检索结果集，用户期望的综述服务主要包括以下3个方面： $\\textcircled{1}$ 基于主题或学科领域的归纳整理和加工。指将海量的主题或学科领域相近的文章进行整理和加工，从而帮助用户了解相关检索结果内容。 $\\textcircled{2}$ 全面且重点突出的简要概括。指生成的综述内容要同时具备全面、简洁、层次分明、重点突出等特性，帮助用户实现高效阅读和吸收。 $\\textcircled{3}$ 提高内容获取效率。指帮助用户判断、筛选和浏览感兴趣的内容，快速找到想要的文献，提高内容获取效率。以上3个方面环环相扣，相辅相成。基于主题或学科的整理和加工有助于形成全面且重点突出的概述，全面且重点突出的特性能够帮助用户实现高效阅读、筛选和吸收，切实提高内容获取效率。",
    "context2": "笔者认为，面向检索结果集的综述内容生成，必须满足以上3个方面的用户需求。为此，设计结构化的综述表达形式，显性揭示检索结果集。所谓结构化，是指将获取的知识内容加以归纳和整理，使之条理化、纲领化。表现形式的结构化可以避免知识内容的杂乱无章，达到条理清晰、层次分明、一目了然的效果，可增强阅读和吸收效率。",
    "context3": "由以上分析，笔者设计了面向检索结果集场景下的结构化综述生成整体框架，如图1所示：",
    "context4": "![](images/f22fd062826004fd121d4c6e3cace1511aa7dcf3df5c568320a5e38db2d0492c.jpg)  \n图1结构化综述生成整体框架  \nFigure 1 Framework of structured review generation",
    "context5": "如图1所示，结构化综述生成框架主要包括3部分内容： $\\textcircled{1}$ 面向检索结果集的内容组织； $\\textcircled{2}$ 结构化表达形式设计； $\\textcircled{3}$ 结构化综述性文本生成。",
    "context6": "检索结果集说明：在用户输入检索词后，系统会返回检索结果集，且多数学术搜索引擎在返回检索结果集时已按照某种排序方式进行排序（多数默认按相关度排序）。本研究基于相关度排序后的检索结果集，从而保证检索结果集与检索词的相关性，且越往后的结果与用户检索的相关度越低。",
    "context7": "内容组织及结构化表达形式。该部分内容面向检索结果集进行合理的内容组织，依据组织方式设计合适的结构化表达形式，实现综述内容文本在整体层面的结构化表达。局部层面，对每个类别下的文献集进行综述文本内容的自动生成。该综述生成方案能够使得综述结果在整体上呈现结构化，局部上满足综述文本内容的针对性，更有实际应用意义。",
    "context8": "结构化综述性文本生成。该部分内容采用如下技术路线：首先构建摘要模型，对于每个类别下的每篇文献进行自动摘要，自动生成每篇文献的简短摘要。具体流程为，选择合适的预训练模型，通过构建学术文献数据集对模型进行重新训练，并通过调参优化得到最佳模型，实现内容的提炼、归纳和总结。其次，基于开源的大语言模型进行本地部署，构建结构化综述生成模型，利用大模型在归纳总结上的优势，将生成的简短摘要集作为输入，通过设置优质的prompt模板，生成最终的综述文本内容。该技术路线首先进行简短摘要的生成，对每篇文献内容进行压缩归纳，再将其作为大语言模型的输入进行综述的生成，这在保证生成内容质量的同时，能够在一定程度上解决大模型输入长度限制的问题。"
  },
  "3.2内容组织及结构化表达设计": {
    "context1": "在基于抽取式的多文本自动文摘研究中，多数学者会先将多文档进行主题聚类，然后从聚类得到的各子主题结果集中进行重要句子抽取、打分排序等方法生成文摘。聚类是一种有效的也是目前用的比较多的一种内容组织策略，可以对结果集的主题进行合理组织和揭示。",
    "context2": "然而，在学术检索系统场景下，检索是实时的，检索结果集的数量是成千上万的，采用聚类方法对检索结果集进行内容组织，必然会极大影响下一步综述生成的效率，显然不能满足实际场景的要求。",
    "context3": "可喜的是，在数字图书馆服务系统中，具备天然的知识组织能力优势。知识组织是图书馆等信息服务机构或系统的一项基础性工作。NSTL 在知识组织方法和使用工具上，实现了从单一的学科分类到科技术语、概念、范畴、本体的多层级知识组织能力[22]。其中，学科分类（采用《中国图书馆分类法》）主要提供期刊或书目为单元的浏览导航和检索服务。在用户检索场景下，同样为检索结果集提供学科分类的分面功能，辅助用户进行学科层面的快速筛选。该服务功能可以看作是从学科或领域层面，对检索结果集的初步组织和整理。",
    "context4": "基于相关研究现状的分析、NSTL用户检索的实际场景特性以及学术文献文本对象的特点，笔者充分利用NSTL在学科分类上的内容组织优势，设计以学科为主线的结构化综述表达形式。具体方案如下：整体层面，以学科为主线进行内容组织，呈现由学科组成的结构化框架；局部层面，即每个学科下，以基于语步功能的结构化模板方式形成结构化文本内容。结构化表达形式设计的示意图如图2所示：",
    "context5": "![](images/7cb0cfc9a56b7f8cfe160735f87d81f4b9237c85c6e1351e2f031779a83e532d.jpg)  \n图2结构化表达形式设计的示意  \nFigure 2 Design of structured expression form",
    "context6": "如图2所示，该方案在整体层面上，呈现以学科为主线的结构化，保证了综述的全面性；局部层面上，呈现以语步功能的结构化，知识粒度细，层次分明，重点突出，能够在一定程度上帮助用户实现高效阅读、筛选和吸收。",
    "context7": "除文本内容上的凝练和综述外，在实际的应用中可增加可视化的图表分析，如学科分布饼状图、热词分布图、研究趋势图、文献类型年代分布图、作者机构统计分析图等，辅助用户对检索结果集有更直观的了解和把控。"
  },
  "3.3摘要模型": {
    "context1": "本文在结构化综述智能生成的技术路线上，首先进行简短摘要的生成，对每篇文献内容进行压缩归纳，再将其作为大语言模型的输入进行综述的生成，这在保证生成内容质量的同时，能够在一定程度上解决大模型输入长度限制的问题。",
    "context2": "在实际的文献检索平台应用中，并不是所有的文献都有全文数据。摘要是对文献内容的简短陈述，能够反映文献的必要信息，因此，从实际工程应用的角度出发，基于文献原始文摘进行压缩归纳进而生成综述，既能保留文本的主要内容信息，且更符合实际数据特点，支撑后续的实际应用。因此，笔者将利用文献原始文摘数据开展后续的研究与实践。",
    "context3": "基于相关研究现状的分析，目前主流的生成式自动文摘主要有两类方法： $\\textcircled{1}$ 基于预训练语言模型微调方法，首先在大数据集上训练得到一个具有强泛化能力预训练模型，然后在下游任务上通过领域数据集进行微调； $\\textcircled{2}$ 基于ChatGPT等生成式大语言模型的方法。因ChatGPT等大语言模型在开源情况、硬件环境要求等方面的限制，在实际应用的部署中具有一定的安全风险和复杂性。再者，因为该简短摘要生成任务在整个综述框架下属于一个子模块，任务内容单一明确，方法一完全能够在实际应用服务中发挥显著作用。因此，笔者在摘要模型中更倾向于基于预训练语言模型微调方法开展相关工作。",
    "context4": "结合相关模型调研、简短摘要生成的任务特点以及实际应用对模型性能稳定的需求等因素，笔者选定Google团队于2020年推出的大规模文本语料库预训练生成式摘要模型PEGASUS 模型[23]作为实验模型。",
    "context5": "PEGASUS 模型基于 Transformer 框架，面向生成式摘要任务与训练目标，通过设置间隔句生成（gapsentencesgeneration，GSG）预训练目标，鼓励模型依据已有语句生成被掩盖的语句，促进模型达到类似理解文本与生成摘要的功能。模型的结构和文本训练的具体流程如图3所示：",
    "context6": "![](images/38eb7e3e2d7c6482b66b3d83d8f2fdf69ee2268c9e8de692dd8861cf6da53568.jpg)  \n图3PEGASUS 模型基本结构与输入输出形式Figure 3 Basic structure and input-output forms of the PEGASUS model",
    "context7": "为了提升模型在科技文献数据上文本生成的质量，笔者参考数据驱动思想，补充构建科技文献数据集，进一步训练PEGASUS 模型并进行参数调整，优化模型，使模型在科技文献数据的应用下获得更好的表现。因此，简短摘要自动生成任务的技术路线如图4所示：",
    "context8": "![](images/08cda99b1233c28cbb518f6fa55688e6ed1b3bdf601cd943ba2f6e30502e86a4.jpg)  \n图4简短摘要自动生成技术路线  \nFigure 4 Roadmap of short abstract automatic generation",
    "context9": "在文献数据采集上，为保证数据集的质量以增强模型的性能，笔者遵循领域多样性、内容高质量的原则，手工筛选理、工、农、医4个领域共60 本高影响力英文期刊（每个领域15本期刊），从网络上获取其刊载文献的元数据，共采集有效英文文献41 440篇。",
    "context10": "在支撑模型训练与微调的文献语料构建上，需要设计合理的输入文本与输出文本之间的对应关系，以供模型学习。考虑到数据标注难度、实际场景限制以及生成文本归纳创新程度，笔者以文献的原标题、原摘要、关键词作为输入文本，借助ChatGPT大语言模型生成的简短摘要作为参考摘要，即输出文本，从而完成数据集的对应关系设计，最终形成的文献语料数据集字段及描述如表1所示：",
    "context11": "表1文献语料数据集字段及描述  \nTable 1 Field and description of literature corpus dataset",
    "context12": "<table><tr><td>关键字段</td><td>描述</td></tr><tr><td>paper_id</td><td>文献 id</td></tr><tr><td>original_abstract</td><td>原始文献摘要</td></tr><tr><td>short_abstract</td><td>简短摘要</td></tr><tr><td> subject</td><td>文献所属学科领域</td></tr></table>",
    "context13": "为保证语料的质量，笔者对ChatGPT生成的简短摘要效果进行人工评估。为提升评价的客观性并确保评价过程的可信性，对生成简短摘要的文本阅读性、语法规范性以及语义性3个维度进行综合打分评价，具体评价指标及评价问题设计详见表2，共定义7个人工评价指标，设置13个评价问题供评价人员打分（满分10分制）。",
    "context14": "表2人工评价指标及评价问题设计  \nTable 2 Design of manual evaluation indicator and evaluation problem",
    "context15": "<table><tr><td>评价维度评价指标</td><td></td><td>评价问题</td></tr><tr><td>文本 阅读性</td><td>表述 自然度</td><td>生成文本是否存在用词生硬、割裂的问题 生成文本表述是否贴合正常习惯、轻松易读</td></tr><tr><td></td><td>段落 连贯性</td><td>生成文本上下文承接是否顺畅 生成文本段落间是否存在逻辑跳跃的问题</td></tr><tr><td>语法 规范性</td><td>词句 通顺度</td><td>生成文本的词句、短语搭配是否符合语法规则 生成文本是否存在用语重复的问题</td></tr><tr><td></td><td>结构 完整性</td><td>生成文本各独立句子的语法结构是否完整 生成文本单词时态、单复数形式是否存在错误</td></tr><tr><td>语义性</td><td>内容 信息量</td><td>生成文本是否包含丰富的信息点 生成文本是否含有新增内容</td></tr><tr><td></td><td>原文 忠实度</td><td>生成文本和原文信息点的吻合程度</td></tr><tr><td></td><td>事实 准确性</td><td>生成文本是否存在违背常识定理的内容</td></tr><tr><td></td><td></td><td>生成文本是否存在对原文信息的错误应用</td></tr></table>",
    "context16": "考虑到人力成本，笔者采用随机抽样方式，对理、工、农、医4个学科领域分别随机抽样10篇英文论文，共 40 篇英文论文为评价对象。人工对论文内容及生成文本进行研读，并对上述13个评价问题评价打分，评价结果见表3。",
    "context17": "表3简短摘要人工评价结果  \nTable 3Manual evaluation of the short abstract",
    "context18": "<table><tr><td>学科领域</td><td>综合得分均值</td></tr><tr><td>理</td><td>9.2</td></tr><tr><td>工</td><td>9.7</td></tr><tr><td>农</td><td>9.6</td></tr><tr><td>医</td><td>9.7</td></tr></table>",
    "context19": "如表3所示，通过人工判读，4个学科领域生成的简短摘要综合得分均值较高，这反映出生成的简短摘要在文本阅读性、语法规范性以及语义性等方面表现良好，能够较为准确地传达文本的意义和信息，可以作为参考摘要进行摘要模型的训练。",
    "context20": "在构建的科技文献语料数据集基础上，对PEG-",
    "context21": "ASUS 模型进行训练、测试、调参优化，保存最佳效果模型作为最终的摘要模型。实验相关情况及结果分析见第四章节。"
  },
  "3.4结构化综述智能生成模型": {
    "context1": "随着ChatGPT的火热，针对大语言模型的相关研究越来越多。由于大语言模型目前在文本内容理解、信息提取、归纳总结等能力上具有出色的表现,这对综述生成的任务来说具有非常强的适用性。因此，笔者利用大语言模型技术，探索面向辅助阅读场景的科技文献检索结果集的结构化综述实际应用实践。",
    "context2": "各大公司相继推出了自己的大语言模型，然而这些模型通常需要极大的硬件和算力支持，并且许多模型并没有开源发布，这对进一步研究和应用落地带来了阻碍。为了探究大语言模型在英文科技文献集的综述生成中的实践探索，笔者针对该研究在实际应用的需求，面向英文语种文献、开源发布、硬件显存要求低等方面对目前满足条件的主要大语言模型进行梳理和总结，如表4所示：",
    "context3": "表4主要的开源大模型梳理  \nTable 4Main open source big models",
    "context4": "<table><tr><td>模型名称</td><td>研究单位</td><td>参数规模</td><td>训练数据量</td><td>性能表现</td></tr><tr><td>LLaMA[24]</td><td>Meta AI</td><td>7B、13B、30B和65B4种参1.4万亿 token 数规模</td><td></td><td>LLaMA-13B在大多数基准测试中优于GPT-3</td></tr><tr><td>Alpaca[25]</td><td>斯坦福大学</td><td>13 B、65B两种参数规模</td><td>52K条问答指令数据</td><td>性能约等于GPT-3.5</td></tr><tr><td>Vicuna[26]</td><td>加州大学伯克利分校、卡内基7B、13B两种参数规模 梅隆大学、斯坦福大学等机构</td><td></td><td>70k 条问答指令数据</td><td>在LLaMA的子孙模型中表现最佳，能达到 ChatGPT90% 的效果</td></tr><tr><td>Koala[27]</td><td>加州大学伯克利分校</td><td>13B</td><td>500k 条问答功能数据表现与Vicuna 类似</td><td></td></tr></table>",
    "context5": "如表4所示，LLaMA是MetaAI公司发布的开放且高效的大型基础语言模型，Alpaca、Vicuna 和Koala都是在LLaMA的基础上进行微调构建的。这4 种模型主要使用英文语料进行训练，且都开源，在参数规模上都有较小的版本，能够基本满足本文面向实际应用的实践探索需要。结合实际的硬件资源情况及网络上对模型相关性能表现的评价，笔者选择Vicuna-7B作为实践对象。",
    "context6": "Vicuna[2开源大模型是来自加州大学伯克利分校、卡内基梅隆大学和斯坦福大学等机构的学者，受MetaLLaMA和StanfordAlpaca项目的启发共同推出，目前已经发布两个版本：包含Vicuna-7B和Vi-cuna-13B，即参数规模分别为70亿、130亿。Vicuna模型在不同的任务上都表现出优异的水平，在质量、多样性、一致性等指标上都达到接近ChatGPT/Bard的质量，而且明显优于其他基准模型。Vicuna 模型的整体架构如图5所示：",
    "context7": "![](images/275b6dd8a983c4c24d2d473194c0c1d964d348479a72a49294b0a454227f5bb4.jpg)  \n图5Vicuna 模型的整体架构  \nFigure 5Overall architecture diagram of the Vicuna model",
    "context8": "如图5所示，Vicuna研究者们从ShareGPT收集了大约7万个用户分享的对话数据（ShareGPT是一个用户可以分享他们的ChatGPT对话的网站），然后，进一步优化Alpaca 提供的训练脚本，以更好地处理多轮对话和长序列。相较于Alpaca，Vicuna 在训练中将序列长度由512扩展到了2048，并且通过梯度检测和flash attention来解决内存问题；调整训练损失考虑多轮对话，并仅根据模型的输出进行微调。在评估方面，Vicuna 的研究人员提出一个基于",
    "context9": "GPT-4的评估框架以自动评估聊天机器人的性能，初步评估表明Vicuna达到ChatGPT/Bard的 $90 \\%$ 的能力。Vicuna 实现了与ChatGPT/Bard相比具有竞争力的性能，且7B的版本对硬件配置要求较低，综合考虑性能表现以及硬件配置要求，笔者采用Vicuna-7B作为实验模型，进行本地化部署并开展结构化自动综述的实践探索。",
    "context10": "笔者提出基于Vicuna开源大模型的结构化综述生成框架，如图6所示：",
    "context11": "![](images/e2387f997230d8669ebc7b6122749645d464aaa8ffc4a63f34a7f5466c67aae1.jpg)  \n图6基于开源大模型的结构化综述生成框架 Figure 6Structured review generation framework on open source big model",
    "context12": "如图6所示，本部分内容主要进行本地化的部署，并基于Vicuna开源大模型对科技文献领域数据进行模型的微调，再通过设计结构化综述prompt，以激发大模型最大的生成能力，并对生成的综述文本从文本的结构化清晰性、语义性可读性、综述的概括性、语法规范性等方面进行全方位的评估。"
  },
  "4 实验分析 /Experimental analysis": "",
  "4.1评价方法及指标": {
    "context1": "实现综述文本生成后，对于生成效果的评价必不可少。本文包括两方面的评价： $\\textcircled{1}$ 摘要模型生成效果的评价。采用自动化评价方法，以量化形式直观反映模型的生成效果，同时为后续模型效果的不断改进提供衡量标准。 $\\textcircled{2}$ 实际检索实例下综述生成效果的评价。采用人工评价的方法，主要包括表现形式效果以及实例生成文本的全方位评价，相较于自动评价，人工评价方法能以人类思维角度和阅读习惯出发，深入语义内容对文本进行感知和解读，从而更切实地判",
    "context2": "断综述生成文本的质量。",
    "context3": "在自动化评价方法上，本研究以文本摘要方向广泛使用的ROUGE[28]工具作为自动评价方法。ROUGE评价的原理基于召回率，对标准参考文本中被生成文本覆盖的n元词组数量进行统计，生成相应数值以评估生成文本的质量。ROUGE指标具有多种变体，更适合判别长文本。此处使用广泛的ROUGE-1、ROUGE-2 和ROUGE-L3 类指标。",
    "context4": "（1）ROUGE-1。该指标的分母为参考摘要中的一元词组，分子为自动摘要中与参考摘要匹配的一元词组的个数，即自动摘要和参考摘要共有的一元词组个数，适用于对文本的召回程度进行统计。具体计算如公式（1）所示：$R O U G E - 1 = \\frac { \\sum _ { S \\in \\{ R e f e r e n c e S u m m a r i e s \\} } \\sum _ { g r a m _ { 1 } \\in S } C o u n t _ { m a t c h } \\left( g r a m _ { 1 } \\right) } { \\sum _ { S \\in \\{ R e f e r e n c e S u m m a r i e s \\} } \\sum _ { g r a m _ { 1 } \\in S } C o u n t \\left( g r a m _ { 1 } \\right) }$",
    "context5": "公式（1）",
    "context6": "（2）ROUGE-2。该指标是对参考摘要和自动摘要共有的二元词组的个数占参考摘要二元词组总数比例的统计。具体计算如公式（2）所示：$R O U G E - 2 = \\frac { \\sum _ { S \\in \\{ R e f e r e n c e S u m m a r i e s \\} } \\sum _ { g r a m _ { 2 } \\in S } C o u n t _ { m a t c h } \\left( g r a m _ { 2 } \\right) } { \\sum _ { S \\in \\{ R e f e r e n c e S u m m a r i e s \\} } \\sum _ { g r a m _ { 2 } \\in S } C o u n t \\left( g r a m _ { 2 } \\right) }$",
    "context7": "公式（2）",
    "context8": "（3）ROUGE-L。该指标中的L是指最长公共子序列（longest common subsequence，LCS），在实际计算过程中仅对最长公共子序列进行识别与统计。具体计算如公式（3）、公式（4）及公式（5）所示。",
    "context9": "$$\nR _ { \\mathit { L C S } } = { \\frac { \\mathit { L C S } ( X , Y ) } { m } }\n$$",
    "context10": "$$\nP _ { \\mathit { L C S } } = { \\frac { \\mathit { L C S } ( X , Y ) } { n } }\n$$",
    "context11": "$$\nF _ { L C S } = { \\frac { ( 1 + \\beta ^ { 2 } ) R _ { L C S } P _ { L C S } } { R _ { L C S } + \\beta ^ { 2 } P _ { L C S } } }\n$$",
    "context12": "公式（5）中， $\\mathrm { R } _ { \\mathrm { L C S } }$ 、 $\\mathrm { P _ { L C S } }$ 分别为召回率和准确率，$\\mathrm { F _ { L C S } }$ 为 $\\mathrm { R } _ { \\mathrm { L C S } }$ 、 $\\mathrm { P _ { L C S } }$ 的调和平均值，用来评估参考摘要和自动摘要的相似性，一般被认为代表ROUGE-L值。LCS（X，Y）是指参考摘要和自动摘要中最长公共子序列的长度， $\\mathbf { m }$ 是参考摘要的长度， $\\mathbf { n }$ 是自动摘要的长度， $\\beta$ 为常数。"
  },
  "4.2摘要模型训练实验与分析": {
    "context1": "完成数据集的构建后，对PEGASUS模型进行测试和进一步的训练优化。为了更大程度地发挥模型效果，本次PEGASUS模型实验选择效果更优的PEGA ${ \\mathrm { S U S } } _ { \\mathrm { L A R G E } }$ 为基础实验模型。实验环境参数详细",
    "context2": "情况见表5。"
  },
  "表5实验环境参数": {
    "context1": "Table5 Experimental environmental parameters   \n表6模型在测试集上的测试结果对比",
    "context2": "<table><tr><td>框架</td><td>版本</td></tr><tr><td>OS</td><td>Ubuntu 20.04</td></tr><tr><td>Python</td><td>3.7.10</td></tr><tr><td>PyTorch</td><td>1.10.0</td></tr><tr><td>TensorFlow</td><td>2.7.0</td></tr><tr><td>GPU</td><td>GeForce RTX 3090</td></tr><tr><td>CUDA</td><td>11.2</td></tr><tr><td>CPU</td><td>Intel(R) Xeon(R) Gold 6142 @ 2.60GHz</td></tr></table>",
    "context3": "首先，从实践层面对 $\\mathrm { P E G A S U S _ { L A R G E } }$ 模型的基础性能进行验证。实验采用构建的科技文献数据集的测试集，测试比例Test_Ratio分别为0.1、1.0，以ROUGE为自动评价方法对测试结果进行评估打分并记录相应结果，并将其与在BiLSTM模型上的测试结果进行对比，实验结果见表6。",
    "context4": "如表6所示，评估指标共涉及3类，其中召回率（R）计算的是机器生成摘要与标准参考摘要中基本单元的重叠度，可用于衡量机器生成文本内容的全面性。精确率（P）用于识别机器生成摘要中有多少内容是实际上相关或被需要的，可用于判别机器生成文本的冗余度。F值是综合两者的评估指标，可用于反映机器生成文本的整体表现。从测试结果来看，PEG-$\\mathrm { A S U S } _ { \\mathrm { L A R G E } }$ 模型在ROUGE-1和ROUGE-L中的R召回率和F值上的表现有明显提升，代表其相对更能捕捉到原始语段中的重要内容，生成文本更全面，但在精确率上则稍弱，反映生成文段的冗余度偏高，文本内容不够简洁。而 $\\mathrm { P E G A S U S _ { L A R G E } }$ 模型的 ROUGE-2评分则整体处于下风。考虑到 $\\mathrm { P E G A S U S _ { L A R G E } }$ 模型的抽象能力更好，ROUGE-2分值的总体偏低可能在于原文复制比例低，因而二元词组的平均出现比例也相应降低。从F值来看， $\\mathrm { P E G A S U S _ { L A R G E } }$ 模型的综合表现则有优势，模型本身具有更优的性能。",
    "context5": "Table 6 Comparison of the model on the test set",
    "context6": "<table><tr><td rowspan=\"2\">模型</td><td colspan=\"9\">ROUGE分值/%</td></tr><tr><td>R1-R</td><td>R1-P</td><td>R1-F</td><td>R2-R</td><td>R2-P</td><td>R2-F</td><td>RL-R</td><td>RL-P</td><td>RL-F</td></tr><tr><td>BiLSTM</td><td>13.9</td><td>60.4</td><td>23.4</td><td>8.9</td><td>25.8</td><td>12.7</td><td>14.7</td><td>48.8</td><td>20.8</td></tr><tr><td>PEGASUSLARGE（0.1）</td><td>21.5</td><td>53.2</td><td>29.8</td><td>7.7</td><td>22.7</td><td>11.1</td><td>19.1</td><td>47.3</td><td>26.5</td></tr><tr><td>PEGASUSLARGE （1.0）</td><td>22.4</td><td>54.5</td><td>30.8</td><td>8.3</td><td>24.1</td><td>11.9</td><td>20.0</td><td>48.9</td><td>27.6</td></tr></table>",
    "context7": "接下来，将自建科技文献数据集作为语料，对PEGASUSLARGE模型进一步训练、测试以及调参优化。训练集、测试集、验证集的比例设置为：10:1:1。训练初步设置模型参数如表7所示：",
    "context8": "表7PEGASUSLARGE 模型的训练参数Table 7Training parameters of the PEGASUSLARGE model",
    "context9": "<table><tr><td>模型参数</td><td>参数设置</td></tr><tr><td>Train_Batch_Size</td><td>4</td></tr><tr><td>Eval_Batch_Size</td><td>4</td></tr><tr><td>Learning_Rate</td><td>1e-3（0.001)</td></tr><tr><td>Epoch</td><td>1000</td></tr><tr><td>Warmup_Steps</td><td>10</td></tr><tr><td>Weight_Decay</td><td>0.001</td></tr></table>",
    "context10": "在训练后的模型PEGASUSLARGE_PT上开展测试比例分别为0.1和1.0的生成效果测试，并将其与未经过自建科技文献数据集训练的 $\\mathrm { P E G A S U S _ { L A R G E } }$ 模型的生成效果进行对比，以判断和验证自建科技文献数据集对模型进行补充训练的学习效果以及对模型效果的改进情况。具体测试结果见表8，与PEGA-${ \\mathrm { S U S } } _ { \\mathrm { L A R G E } }$ 模型的对比情况见图7。",
    "context11": "表8PEGASUSLARGE_PT模型的测试结果Table 8Test of PEGASUSLARGE_PT model",
    "context12": "<table><tr><td rowspan=\"2\"></td><td colspan=\"8\">ROUGE分值/%</td></tr><tr><td>R1-R</td><td>R1-P</td><td>R1-F</td><td>R2-R</td><td>R2-P</td><td>R2-F</td><td>RL-R</td><td>RL-P RL-F</td></tr><tr><td>PEGASUSLARGE_PT（0.1）</td><td>16.97</td><td>16.75</td><td>16.67</td><td>1.29</td><td>01.45</td><td>01.34</td><td>16.16</td><td>15.95 15.87</td></tr><tr><td>PEGASUSLARGE_PT（1.0)</td><td>17.06</td><td>16.61</td><td>16.63</td><td>1.43</td><td>01.54 01.46</td><td>16.18</td><td>15.75</td><td>15.77</td></tr></table>",
    "context13": "通过对比发现，经过自建的科技文献数据集训练后的模型PEGASUSLARGE_PT相较于原模型在各个指标上均偏低，经过分析生成内容发现重复生成内容明显，整体生成效果不好，初步分析原因，主要是参数设置导致模型学习不到位。",
    "context14": "接下来，对训练轮次、学习率、Warmup_steps等参数进行调整并记录训练结果，实验过程中的主要几次参数设置调整情况见表9，其中FT1、FT2、FT3分别指3次微调参数设置情况。",
    "context15": "![](images/2c50543348033d343b792065284fc6a3bff63b784b28e2481a0d6b478c0ab606.jpg)  \n图7PEGASUSLARGE 和 PEGASUSLARGE_PT 对比Figure 7 Comparison between PEGA $0 . 5 \\times A B G$ and PEGASUSLARGE PT",
    "context16": "表9训练参数设置调整情况  \nTable 9 Adjustment of training parameter settings",
    "context17": "<table><tr><td>模型参数</td><td>初步微调</td><td>FT1</td><td>FT2</td><td>FT3</td></tr><tr><td>Train_Batch_Size</td><td>4</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Eval_Batch_Size</td><td>4</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Learning_Rate</td><td>1e-3</td><td>：</td><td>-</td><td>：</td></tr><tr><td>Train_Epoch</td><td>1000</td><td>2500</td><td>1250</td><td>1250</td></tr><tr><td> Warmup_Steps</td><td>10</td><td>500</td><td>1200</td><td>2000</td></tr><tr><td> Weight_Decay</td><td>0.001</td><td>0.01</td><td>0.01</td><td>0.01</td></tr></table>",
    "context18": "在调参优化后的模型上，开展测试比例分别为0.1和1.0的效果测试，并与上一轮训练的",
    "context19": "PEGASUSLARGE_PT模型对比，测试结果如表10所示：",
    "context20": "表10　调参优化后模型PEGASUS $\\scriptstyle 1 \\land 1 \\lor 1 \\lor 1 \\lor 1$ 的测试结果Table 10 Test of PEGASUSLARGE_PF model after parameter tuning optimization",
    "context21": "<table><tr><td rowspan=\"2\"></td><td colspan=\"9\">ROUGE分值/%</td></tr><tr><td>R1-R</td><td>R1-P</td><td>R1-F</td><td>R2-R</td><td>R2-P</td><td>R2-F</td><td>RL-R</td><td>RL-P</td><td>RL-F</td></tr><tr><td>PEGASUSLARGE_PT (0.1)</td><td>16.97</td><td>16.75</td><td>16.67</td><td>1.29</td><td>01.45</td><td>01.34</td><td>16.16</td><td>15.95</td><td>15.87</td></tr><tr><td>PEGASUSLARGE_PT (1.0)</td><td>17.06</td><td>16.61</td><td>16.63</td><td>1.43</td><td>01.54</td><td>01.46</td><td>16.18</td><td>15.75</td><td>15.77</td></tr><tr><td>PEGASUSLARGE _PT1(0.1)</td><td>26.18</td><td>41.02</td><td>31.26</td><td>6.71</td><td>10.48</td><td>7.93</td><td>23.34</td><td>36.55</td><td>27.84</td></tr><tr><td>PEGASUSLARGE_PT1(1.0)</td><td>27.07</td><td>41.82</td><td>32.12</td><td>6.70</td><td>10.54</td><td>7.91</td><td>24.36</td><td>37.63</td><td>28.90</td></tr><tr><td>PEGASUSLARGE _PT2(0.1)</td><td>27.44</td><td>42.31</td><td>32.48</td><td>7.24</td><td>11.11</td><td>8.45</td><td>24.43</td><td>37.55</td><td>28.89</td></tr><tr><td>PEGASUSLARGE_PT2(1.0)</td><td>28.06</td><td>42.24</td><td>32.80</td><td>7.48</td><td>11.46</td><td>8.67</td><td>25.28</td><td>38.07</td><td>29.55</td></tr><tr><td>PEGASUSLARGE _PT3(0.1)</td><td>27.62</td><td>40.03</td><td>32.05</td><td>7.20</td><td>10.20</td><td>8.17</td><td>24.73</td><td>35.88</td><td>28.71</td></tr><tr><td>PEGASUSLARGE _PT3(1.0)</td><td>28.70</td><td>41.30</td><td>33.03</td><td>7.52</td><td>10.69</td><td>8.50</td><td>25.90</td><td>37.27</td><td>29.81</td></tr></table>",
    "context22": "由表10可看出，在对参数进行调整优化后模型生成内容较好，在各指标上都有明显的提升。初步微调时模型效果欠佳的原因可能是学习率引起的模型不稳定，因此在优化时调整了Warmup_steps，使模型微调时进行学习率预热，在模型开始训练时使用一个较小的学习率，待模型稳定时再进行正常训练，以此得到更佳效果。但经过测试Warmup_steps 并不是越高越好，其设置为1200 和2000 时效果差别已经不大，之后又进行设置为3500时的测试，测试结果已经开始下降，因此该参数在设置为1200—2000",
    "context23": "时已达到最优。",
    "context24": "此时，将微调优化后的PEGASUSLARGE $\\_ { \\mathrm { p T 3 } }$ 再次与 $\\mathrm { P E G A S U S _ { L A R G E } }$ 原模型的效果进行对比，结果见图8。",
    "context25": "由图8分析可知，经过自建科技文献数据集的训练、调参优化后，模型生成文本内容的全面性和综合表现得到提升。PEGASUSLARGE_PT3在各指标中的召回率和F1值上都得到了明显提升，ROUGE1、ROUGE2和ROUGEL的平均增长比例分别为 $2 . 0 7 \\%$ ，但准确率低于原模型生成内容，经过对生成文本内容的分析发现，采用原始Pegasus 模型生成的摘要，更为倾向于从原文中直接抽取内容并非进行总结，所以在各准确率上的指标相对较高。然而，微调后的模型生成内容的长度明显略长于原模型，生成摘要内容更为全面，因此在各指标中的召回率、F1值上都得到",
    "context26": "![](images/f5520fb7038a106be143c1077d4290ebfa21cb3e1d1dcdd6ea07850a4e159cd2.jpg)  \n图8PEGASUSLARGE 和 PEGASUS $S _ { \\mathrm { L A R G R } } \\rho _ { 1 3 }$ 对比 Figure 8 Comparison between PEGA $S _ { \\Delta } U S _ { \\Delta , A B C } =$ and PEGASUSLARGE_PT3",
    "context27": "了明显提升。",
    "context28": "最后，结合生成的文本内容实例，对生成内容的可读性、正确性、简洁性等进行分析，由于篇幅原因，此处分析2个具体实例的生成效果，如表11所示：",
    "context29": "表11I PEGASUSLARGE _PF 模型的生成文段（部分）Table 11 Generation paragraph of PEGASUSLARGE_PF model(partial)",
    "context30": "<table><tr><td>原文摘要</td><td>PEGASUSLARGE 生成摘要</td><td>PEGASUSLARGE_PT3 生成摘要</td></tr><tr><td>Sphingolipids,such as ceramide,sphingosine and sphingosine 1-phosphate (S1P) are bioactive molecules that have important functions in a variety of cellular pro- cesses,which include proliferation,survival,differentiation and cellular responses to stress.Sphingolipids have a major impact on the determination of cell fate by contributing to either cell survival or death.Although ceramide and sphingosine are usually considered to induce celldeath,S1P promotes survival of cels.Sphingosine suggesting that SPHK2 kinases (SPHKs) are the enzymes that catalyze the conversion of sphingosine to S1P.There are two isoforms,SPHK1 and SPHK2,which are encoded by different genes. SPHK1.....</td><td>contributing in cancer cell transformation, angiogenesis as well as multi-drug therapy, has a role in cancer progression.</td><td>SPHK1 has recently been This review evaluates the role of the Sphin- golipid Kinase SPHK1 in tumor angiogenesis and metastasis. SPHK1 has a dual transmis- sion strategy and has been associated with cell death, tumor growth,and metastasis. SPHK1 has a therapeutic role in both tumor angiogenesis and metastasis.</td></tr><tr><td>PyC is ubiquitous in the environment where it can be transported by wind and Water before being deposited in aquatic sediments.We compare results from four different methods used to trace PyC that were applied to a high-temporal resolution sedimentary record in order to constrain changes in PyC concentrations and fluxes over the past similar to 250 years.We find markedlydiscordant records for different ing different origins of PyC tracers,particularly during the preindustrial age,implying different origins andPyC. modes of supply of sedimentary PyC.....</td><td>We find discordant records for PyC tracers, particularly during the pre-industrial age,imply-</td><td>In a study ofPyC production and distribution across a 250 year period,we found that PyC has a markedly different composition than originally thought and that its distribution varied considerably according to age, sug- gesting further investigation is warranted.</td></tr></table>",
    "context31": "由表11可看出，模型 PEGASUSLARGE_PF3生成文本的实际内容中语法结构相对完整，关键词信息出现频率较高，语义连贯性较好，内容具有较强的可读性、简洁性和概括性，分析结果表明自建科技文献数据集对模型进行补充训练的学习成效显著。"
  },
  "4.3 结构化综述智能生成效果与分析": {
    "context1": "根据章节3.2的结构化表达形式设计的指导，通过引入角色、强调生成目标、明确输出要求、字数限制等方式，经过多次的prompt设计、实际示例的效果测评分析、调优等，最终得到效果较佳的prompt设计如下：",
    "context2": "“role\":“You are a researcher who is good at summarizing papers using concise statements\"",
    "context3": "“instruction\":Brief summaries of several papers have been provided below in “input_data\".I would like you to summarize these abstracts to generate the literature review. Summarize according to the following four points.",
    "context4": "(1) What is the research background of these pa  \npers? (2) What are the problems studied in these papers? (3) What research methods have been proposed in   \nthese papers? (4) What research has been achieved in these pa"
  }
}