{
  "original_filename": "full_4616.md",
  "基于深度学习的术语识别研究综述": "",
  "阮光册钟静涵张袆笛": {
    "context1": "（华东师范大学信息管理系上海 200062)",
    "context2": "摘要：【目的】梳理深度学习模型在术语识别中的研究现状与面临挑战。【文献范围】在中国知网和WebofScience中,分别以主题 $\\dot { } = $ “术语识别” $^ +$ “术语抽取”、主题 $\\underline { { \\underline { { \\mathbf { \\Pi } } } } }$ “(extract terms OR term recognition OR technologydetection OR relation classification）AND deep learming AND ner\"作为检索式进行检索,共筛选73篇文献进行述评。【方法】对基于深度学习的术语识别一般框架、模型的选择及各模型的优缺点、未来发展趋势进行综述。【结果】基于深度学习的术语识别方法可划分为使用单一神经网络模型、复合神经网络模型和结合深度学习模型的术语识别三大类。从方法使用来看,以BiLSTM-CRF为核心及延伸的模型是术语识别的主流方法;BERT及BERT的优化模型是近年来的研究热点;在特定领域倾向于使用多任务模型代替神经网络模型;迁移学习以及主动学习的应用成为新的研究方向。【局限】仅对已有研究的不同模型及训练结果进行结构化分析,缺少对不同模型在同一数据集上的训练效果对比,待未来进一步研究【结论】基于深度学习的术语识别未来可在术语标注模式、融合术语的多维特征、小数据集或零数据集的术语识别技术、跨领域模型泛化、结果可解释性和完善评价方法等方面深入研究。",
    "context3": "关键词：术语识别深度学习文本挖掘分类号：TP18G35DOI: 10.11925/infotech.2096-3467.2023.0158",
    "context4": "引用本文：阮光册，钟静涵，张祎笛.基于深度学习的术语识别研究综述[J].数据分析与知识发现,2024,8 (4): 64-75.(Ruan Guangce, Zhong Jinghan, Zhang Yidi. Review of Term Recognition Studies Based on Deep Learning[J]. Data Analysis and Knowledge Discovery, 2024, 8(4): 64-75.)"
  },
  "1引言": {
    "context1": "术语识别是命名实体识别的一个分支,指从目标语料库中采用人工或自动技术抽取与领域高度相关的词或短语的过程。随着命名实体研究的深人,其识别内容也不仅仅局限于人名、地名和机构名等一般意义的实体,而是开始对研究领域的重要概念、粒度更细的领域知识实体进行抽取[],术语识别成为重要的研究方向[2-3]。近些年来，从大规模文本集合中抽取特定领域(如电子生物医学文献、专利文献、金融文献等领域)有价值的内容(如术语、实体、关系、语义图等)成为自然语言处理的重要研究领域,其研究成果已经应用于领域新技术预测、知识图谱构建、本体构建、领域知识库建设和补充等工作中。",
    "context2": "早期的术语识别主要依靠专家人工识别,建立具有领域特点的术语库，这种识别方式虽然准确性较高,但严重依赖于专家的领域知识,工作繁重,时间成本高。",
    "context3": "随着计算机技术的发展,大量自动术语识别方法开始出现,从技术实现上来看，大致可分为基于浅层语言特征的经典方法和基于深层文本结构特征的方法。经典方法包括基于语言学和基于词频统计法,其基本思路是制定可涵盖语料库语言特征的规则集合[4],利用领域术语的构词特征[5]、信息熵[和词频统计特征[7-8实现术语的识别。然而,经典方法没有考虑深层语义信息,其精确度受语料库规模和质量的影响较大,领域间的可移植性较差。随着自然语言处理技术的发展，基于语义关系[9、图结构[10]、主题信息[1-12]、机器学习[13-14]和深度学习[15]等技术开始涌现，利用目标语料库词语之间的概率统计分布特征、语义关联等深层次的文本结构特征实现术语识别。其中,基于深度学习的术语识别方法能够应用于超大文档集合，无须人工筛选术语特征,减少了对外部知识的依赖，能够结合候选术语的上下文信息,以词嵌入向量实现更丰富的术语特征;同时,通过参数共享和特征迁移等的方式,实现跨领域术语识别,使得该方法成为术语识别领域的研究热点[16]。",
    "context4": "目前已有一些术语识别研究文献，围绕关键技术[17-18]、机器学习方法[19]和存在的问题[20]进行综述，也有围绕深度学习应用于命名实体抽取的研究综述[21-22],但仍缺乏围绕深度学习方法应用于术语识别研究的综述性文献。考虑到当下一些研究[23]开始采用深度学习进行术语识别,本文以基于深度学习的术语识别方法为视角，梳理其一般框架、研究现状及未来的发展趋势。"
  },
  "2基于深度学习的术语识别方法概述": {
    "context1": "深度学习是一种数据表示的特殊机器学习方法，自2015年,Li等[24]采用循环神经网络(RecurrentNeuralNetwork,RNN)进行术语识别研究后，众多学者开始探索基于深度学习的术语识别应用。早期的学者将基于深度学习的术语识别视为二分类问题,通过分类器识别术语或非术语。然而，该种方法训练时间长,容易出现错误候选术语;同时,由于缺乏文本序列信息,实体分类的准确率会降低[25]。",
    "context2": "目前，大多基于深度学习的研究将术语识别转换为序列标注问题[26],对字符向量、词向量、词性特征、实体特征等使用标签方案,该方法的一般步骤如图1所示。",
    "context3": "![](images/439c2578372ad033fe0df28b8ae01a7e316db7e72ce6fb6298349d10c62465d4.jpg)  \n图1基于深度学习的术语识别一般框架   \nFig.1A General Framework for Term Recognition Based on Deep Learning",
    "context4": "从实现的过程来看，首先，对公开来源获取的语料库进行数据标注。常用的标注方法包括三位序列标注的BIO方法、4位序列标注的BMES和BIOES方法[26]。也可通过领域术语表优化数据标注。随后对标注好的数据采用词嵌入处理,为了提升识别的效果，可加入除字符级之外的词级、句级、词性、部首、拼音、词长、构词特征等属性捕获更多的语料特征[27]。然后输入给特定的深度学习模型,模型的训练可以通过单一神经网络和混合神经网络获取特征，以及可以采用多任务学习对模型间的参数进行共享,或通过迁移学习的方式将源领域数据的特征迁移到目标领域数据，设置好损失函数、优化器等，最后由多个处理层组成的深度学习模型学习并抽取术语。最后,对结果进行评估,并对模型不断调整直至达到最优效果。",
    "context5": "目前，基于深度学习的术语识别实践应用效果已经达到或接近了专家识别的准确率。为系统梳理国内外该领域研究的现状，本文以中国知网和Webof Science核心集作为主要文献来源，分别以主题 $\\dot { } = $ “术语识别” $^ +$ “术语抽取”、主题 $\\vdots = ^ { \\ast }$ (extract terms ORterm recognition OR technology detection OR relationclassification'）AND deep learningAND ner\"作为检索式进行文献检索，对文献进行梳理。文献检索的起止时间为2015年1月1日 ${ \\sim } 2 0 2 2$ 年6月28日。检索得到中文文献84篇，英文文献182篇，对检索结果进行人工筛选，限制文献的研究对象是专业领域的术语,术语识别方法为深度学习，最后保留73篇。",
    "context6": "通过文献整理,本文将按照深度学习模型进行术语识别时使用神经网络的数量、模型训练时的参数设置、模型训练时的特征学习，将术语识别研究方法划分为基于单一神经网络的术语识别、基于混合神经网络的术语识别以及结合深度学习模型的术语识别。"
  },
  "2.1基于单一神经网络的术语识别": {
    "context1": "单一神经网络的术语识别解决方案是由字符编码、特征提取和序列标注(或标签分类)三层构成，大多数采用卷积神经网络（ConvolutionalNeuralNetwork，CNN）、循环神经网络(RNN）、长短期记忆网络（Long Short-Term Memory Network，LSTM）、双向特征编码模型（Bidirectional EncoderRepresentations from Transformers,BERT)等其中之一,并通常被应用于特征提取层。该种方法只针对一个任务，实现起来比较简单,因此是近年来使用最多的方法。采用单一神经网络模型完成术语识别的原理和特点如表1所示。",
    "context2": "表1单一神经网络模型完成术语识别的原理及特点  \nTable1Principles and Characteristics of Single Deep Learning Model Completing Term Recognition Task",
    "context3": "<table><tr><td>模型</td><td>原理</td><td>特点</td></tr><tr><td>CNN</td><td>通过卷积操作从局部信息中提取更高级别的特征,分类器负责 输出字向量的位置信息[15]</td><td>加强学习语义的局部特征和潜在信息,难以捕捉长信息</td></tr><tr><td>RNN</td><td>引人&quot;循环&quot;思想处理序列文本,可以利用上下文信息预测当前 捕获过去短期的信息,使语义学习更加准确,但是容易出现梯度 输出[24]</td><td>消失或爆炸的问题</td></tr><tr><td>BiLSTM</td><td>引入细胞状态,增加输入门、遗忘门、有选择地控制信息[28]</td><td>既能解决短期依赖问题，又能处理长期依赖问题</td></tr><tr><td>Attention</td><td>保留LSTM编码器的输出结果,再通过计算权重的方法选择性 打破模型的编码器和编码器都依赖于一个固定长度的向量,专 学习序列信息输出[29]</td><td>注最相关的信息</td></tr><tr><td>BERT</td><td>进行深度的双向特征编码,通过随机掩码的方式,结合上下文信 适合处理大数据集,训练时间大大缩短,识别多义词实体以及表 息进行模型训练[30]</td><td>征句子结构存在优势</td></tr></table>",
    "context4": "2015年，Li等[24]最早将RNN模型应用于生物医学的术语识别,研究结果发现其序列标注效果比条件随机场模型(ConditionalRandomFields,CRF）和深度神经网络(Deep Neural Network,DNN)更优。随后,Gao等[31]提出基于端到端的深度学习模型,采用RNN模型作为分类器,将文本向量中包含的跨度作为分类标准，在未分割的文本上获得更多句子上的广义特征，实现了短文本的术语抽取。2016年，Wang等[15]使用LSTM和CNN作为分类器,学习候选术语的表示形式,研究发现,即使在少量标注数据的情况下，两个模型仍可以获得与有监督机器学习相当的实验结果。",
    "context5": "受限于模型本身的缺陷，浅层CNN难以捕捉长期信息，RNN只能从过去的序列中获得信息，无法解决长程依赖问题,LSTM模型忽略了未来序列的上下文信息等问题,使得采用单一模型的术语识别效果并不理想。",
    "context6": "为解决单一模型存在的缺陷,学者们开始尝试将单一模型与CRF序列层结合使用完成术语识别。Huang等[32]、Lample等[33]最先将双向长短期记忆网络（Bi-directional LongShort-TermMemory,BiLSTM)和CRF算法相结合，应用于实体抽取任务，模型由嵌入层、BiLSTM文本特征提取层和CRF序列标注层构成。CRF用于解码能够充分考虑独立标签前后的依赖关系，因此较好地解决BiLSTM无法处理有强依赖关系的问题[28]。实验表明,通过融入句法、语序、词性、词长等多样化文本特征[34-36],可进一步提升模型的识别效果。目前,BiLSTM-CRF是术语识别的主流方法，在食品安全[37、理论术语[27]、电子病例[38]、国防军事[39]等方面的术语识别都有很好的表现。",
    "context7": "近年来,随着基于Transformers的BERT预训练模型在识别多义词实体以及句子表征方面所展现出来的强大能力，学者们开始围绕BERT模型开展术语识别[40-41]研究。在模型训练阶段,将BERT与领域知识结构特征(如知识图谱)融合[42],能够解决BERT模型难以利用先前知识和缺失解释的问题，获得了较好的术语识别效果。",
    "context8": "单一模型在不同应用领域完成术语识别的性能指标及部分采用的数据集如表2所示。",
    "context9": "表2单一神经网络的术语识别的性能指标  \nTable 2Representative Literatures of a Single Neural Network for Term Recognition",
    "context10": "<table><tr><td>模型</td><td>时间</td><td>应用领域</td><td>数据集</td><td>性能指标</td><td>贡献</td></tr><tr><td>RNN</td><td>2015[24]</td><td>生物</td><td>GENIA3.02</td><td></td><td>F1:0.82（比CRF提升0.07）在未分割的文本上获取句子中更广义的信息</td></tr><tr><td>BiLSTM</td><td>2018[43]</td><td>医疗事件</td><td>笔记、病例</td><td>0.15)</td><td>Mayo Clinic的医疗 准确率:0.88（比CRF提升融合单词向量、词性信息,可以拓展到其他领域实 验</td></tr><tr><td>SpanBERT</td><td>2020[44]</td><td>自然语言处理</td><td>GLUE</td><td>F1:0.95(比BERT提升0.2）</td><td>通过Masking连续的随机跨度,在问题和共同参考 解决的任务中表现良好</td></tr><tr><td>BiLSTM-CRF 2018[45]</td><td></td><td>中医临床</td><td>类方》</td><td>《全国名医验案F1:0.75（比LSTM提升 0.07)</td><td>症状的组成要素融入了额外的字符级别特征</td></tr></table>"
  },
  "2.2基于混合神经网络的术语识别": {
    "context1": "术语识别任务训练除了采用单一神经网络，往往会使用混合神经网络的方法,即一个神经网络的输出会作为下一个神经网络的输入，以获取更多的文本特征、序列特征、上下文特征等。",
    "context2": "基于混合神经网络的术语识别的研究主要是在BiLSTM模型基础上的融合，包括引入CNN、",
    "context3": "Attention[46]、BERT等算法,其目的是解决实际应用中面临的术语种类繁多、存在多种表述方式、难以划分边界以及术语包含缩写、嵌套、大小写英文混合、中英文混合等情况[47]。总体而言,混合神经网络的深度学习模型效果基本优于单一神经网络的深度学习模型。",
    "context4": "常见的混合神经网络模型的原理与特点如表3所示。",
    "context5": "表3常见的混合深度学习模型融合的原理及特点  \nTable 3Principles and Characteristics of Multiple Deep Learning Models Fusion",
    "context6": "<table><tr><td>模型</td><td>原理</td><td>特点</td></tr><tr><td>BiLSTM-Attention</td><td>BiLSTM获取序列上下文特征;Attention 注意力机制对 既考虑了文本上下文信息,又有效地突出重点 BiLSTM提取出序列信息进行加权变换;Softmax输出结果信息,能够获得更加丰富的语义特征</td><td></td></tr><tr><td>CNN-BiLSTM</td><td>CNN层对字向量进行卷积和池化,抽取字符级向量; BiLSTM层提取文本特征,获取上下文信息</td><td>相较于基础模型,CNN算法能够很好地描述数 据的局部特征,对于特殊字符的识别效果较好， 可全面地捕获文本信息</td></tr><tr><td>BERT-BiLSTM</td><td>BERT进行文本表示;BiLSTM进行特征提取</td><td>相较于基础模型,能够对序列及字符的语义特 征进行建模、捕获序列及字符的特征,在识别多 义词实体以及表征句子结构方面表现突出</td></tr><tr><td>CNN-BiLSTM-Attention-LSTM</td><td>CNN获得字符特征;BiLSTM获取序列特征;Atention注意 考虑到了文本权重分布的差异,解码器的训练 力机制捕获依赖关系;LSTM进行解码输出</td><td>速度更快</td></tr><tr><td>BERT-Attention-MCCNN[48]</td><td>BERT产生动态词向量;Atention注意力机制学习词汇权 缓解静态词向量无法区分一词多义、未考虑单 重;残差单元构成MCCNN,学习表达关系的语义;Softmax 词权重从而长句子提取效果差、模型复杂等的 输出结果</td><td>问题</td></tr></table>",
    "context7": "CNN-BiLSTM-CRF组合模型的原理是,首先使用CNN抽取字符级向量，然后利用BiLSTM获取上下文信息，最后通过CRF获取标签序列。由于CNN算法能够很好地描述数据的局部特征,对于特殊字符的识别效果好，使得组合模型能够更加全面地捕获文本信息。模型在计算机领域[49]、石化行业[50]的实践应用中获得了较好的效果。",
    "context8": "将注意力机制(AttentionMechanism）作为组合模型的一部分，有助于模型聚焦于关键信息,防止有效信息的丢失。其基本过程是：首先训练字符嵌入，利用BiLSTM输出序列上下文特征表示，随后引入注意力机制对不同字符的隐藏层状态分配不同的权重，最后利用CRF层获得全局标签序列。文献[51]采用BLSTM-Attention-CRF模型,解决对嵌套和复合结构术语识别的问题,准确率可达到 $8 6 \\%$ 以上。",
    "context9": "BiLSTM-CRF与BERT模型结合，主要思路是：运用BERT完成初始化词嵌入层的任务，使得后续的BiLSTM-CRF方法能够很好地基于上下文及字符的语义特征进行建模和捕获[52]。该模型应用到临床医学[53]、算法[54]、统计信息[55]、方志知识图谱[56]、中国民族药学[57]等术语识别研究中，解决静态词向量无法区分一词多义、未考虑单词权重从而长句子提取",
    "context10": "效果差的问题。",
    "context11": "除上述上述模型的融合以外，一些学者将更多的深度学习模型进行组合,完成术语识别研究。例如,文献［58]使用Attention和RNN对舆情文本进行编码,得到具有权重信息的文本表示,利用卷积神经网络对其进行局部信息编码,再利用多头注意力机制获取具有敏感权重的局部特征,在司法领域提高了敏感信息识别的准确率。",
    "context12": "常见混合神经网络术语识别的性能评价如表4所示。",
    "context13": "表4常见混合神经网络的术语识别性能指标  \nTable 4Representative Literatures of Fusion Neural Network for Term Recognition",
    "context14": "<table><tr><td>文献模型</td><td>时间</td><td>应用领域</td><td>数据集</td><td>性能指标</td><td>贡献</td></tr><tr><td>CNN-BiLSTM-CRF</td><td>2018[49]</td><td>计算机文献</td><td>中国知网</td><td>CRF提升0.03）</td><td>F1:0.78（比BiLSTM-对GloVe和Word2Vec两种词嵌入模型 进行比较,GloVe表现更好</td></tr><tr><td>DCNN-BiLSTM- CRF</td><td>2021[50]</td><td>煤炭装置</td><td>HAZOP</td><td>CRF提升0.18）</td><td>F1:0.886（比BiLSTM-解决了专业领域词语复杂、一词多义、 嵌套难以识别问题 CNN模型对局部信息进行提取可降低</td></tr><tr><td>CNN-Attention- BiLSTM</td><td>2022[59]</td><td>案件</td><td>司法领域负面 新浪微博、Github 等网站 F1:0.89(比BiLSTM 的舆情信息</td><td>Attention提升0.08）</td><td>识别冗余信息影响,由于BERT预训练 模型分词结构固定,不适用于本任务</td></tr><tr><td>Dynamic-att-BiL- STM-LSTM</td><td>2021[60]</td><td>社区文本</td><td>Semeval-2018,Alien- Vault, WeLive,Amazon -Related Blogs</td><td>CRF提升0.47）</td><td>F1:0.87（比 BiLSTM-提出了一个对抗性主动学习框架,增量地 为选择有信息的样本,准确率提升明显</td></tr><tr><td>BERT-BiLSTM-CRF 2019[53]</td><td></td><td>临床病例</td><td>中文临床乳腺癌笔记</td><td>F1:0.967</td><td>在英文的融合的数据文本上BERT提升 了模型的语义理解</td></tr><tr><td>BERT-BiLSTM-CRF 2022[54]</td><td></td><td>学术文献</td><td>Semantic Scholar英文文 献 1998年人民日报数据集</td><td>F1:0.32</td><td>算法识别模型可以有效推动算法进化网 络构建和算法检索与追踪方面的工作 使用迁移学习,解决语料标注冷启动问</td></tr><tr><td>TFT-BERT-BiLSTM- CRF</td><td></td><td>2021[56]方志知识图谱</td><td>和微软亚洲研究院(MS- RA)数据集</td><td>F1:0.86（比 BERT-BiL- STM-Softmax 提升0.50）</td><td>题,BERT相比其他方法预测结果F1值 提升了约30%</td></tr><tr><td>BERT-BiLSTM-GCN -CRF</td><td></td><td>2021[48]非遗传统戏剧</td><td>遗产官网</td><td>升0.590)</td><td>百度百科和非物质文化F1:0.904（比Baseline提 使用图卷积网络获取长距离句子特征， 构建非遗传统戏剧术语库</td></tr></table>"
  },
  "2.3结合深度学习模型的术语识别": {
    "context1": "（1）多任务学习模型",
    "context2": "多任务学习（Multi-taskLearning,MTL)是一种归纳学习的方法[58],该方法同时在多个任务上训练模型,模型间共享参数,进而提高单个任务性能。如图2所示,多任务学习中一般有两种方式组合：一是对所有任务共享隐藏层,保留一些特定于任务的输出层;二是每个任务都具有自己的参数和模型,但共享软参数（SoftParameters）。研究发现,不同的数据集和识别任务，两种方式的结果会存在一定",
    "context3": "差异[61]。",
    "context4": "与单个神经网络模型相比，多任务学习在不同任务中获取更多的数据信息，进而大大提高了术语识别的效果[2]。目前,一些领域(如生物医学领域)的术语识别研究更倾向于使用MTL代替神经网络模型[60]。",
    "context5": "然而,多任务学习模型也存在一些缺点,如由于多个数据集的标签完全不同而造成模型学习效果降低。为解决这一问题,文献[63]构建仅学习观察到的标签(ObservedLabels)的联合学习模型，不考虑缺失标签的影响而克服模型在不同数据集上的输出冲突。随着BERT模型的广泛应用,将BERT与多任务学习相结合有效提升了术语识别的准确性[64]。",
    "context6": "![](images/999eff2693ef51705497d2e34f1b4e22840b81ef97ffdfaf09c430b460b99232.jpg)  \n图2多任务学习的一般原理  \nFig.2General Principles of Multi-task Learning"
  },
  "（2）深度迁移学习": {
    "context1": "基于深度学习的术语识别高度依赖于专业知识,为解决模型跨行业、跨领域的泛化应用能力不强的问题,一些研究将迁移学习[65]引人术语识别中，迁移学习是将某个领域或任务上学习到的知识(特征、参数)或模式应用到其他相关的领域或问题中的方法,原理如图3所示。与多任务学习相比,其在源领域和目标领域学习的过程是依次进行的，当源数据发生改变时，目标数据也能做出适当调整。",
    "context2": "![](images/a7d34aff5d918ea797e2872de023b1c2b4a73ff19ad6d9e784ed1ccab8423e3e.jpg)  \n图3迁移学习的一般原理  \nFig.3General Principles of Transfer Learning",
    "context3": "深度迁移学习的基本过程是：首先提取源领域数据标签，采用深度神经网络转移模型参数权重，最后输出目标领域的标签序列。目前借助迁移学习模型,学者利用对少量目标领域标注数据,对已有相关但不同标注数据的辅助数据集进行迁移学习，结合深度学习模型识别目标领域术语实体[66],或者利用从源领域标注数据中学习的知识,对标注较少的目标领域进行术语识别[56.67-68]并取得了较好的效果。然而,采用深度迁移学习的术语识别需注意由于负迁移等带来识别效果下降的问题。"
  },
  "（3）主动学习": {
    "context1": "主动学习算法能够在缺乏标注样本的情况下构造有效训练集，原理是通过迭代抽样，从未标注的样本中寻找有利于提升分类效果的样本进行人工标注,随后重新训练更新标注样本,尽可能地减小标注成本[69],提高模型学习效率。",
    "context2": "采用该方法进行术语识别时，在获得标注序列后,还需要筛选人工标注样本,重新训练更新标注样本的模型,以此迭代循环。研究发现,采用主动学习进行术语识别,可以大幅度降低人工标注成本,结合BERT-BiLSTM-CRF的方法，使用少量标注样本达到了与监督学习算法基本相同的序列标注效果[47]。但是,如何判断主动学习模型在迭代过程中是否优化、训练数据如何降噪等,仍需要更进一步研究。"
  },
  "2.4基于深度学习术语识别的总结": {
    "context1": "基于深度学习的术语识别主要利用深度学习模型实现术语抽取,无须费时费力地设计特征工程,也能获得与机器学习相当的术语识别准确率,特别适合对超大文本集合的数据处理。",
    "context2": "从现有研究来看，BiLSTM-CRF是最常见的模型架构,无论是单独使用,还是结合其他神经网络模型,均可取得较好的术语识别效果。然而,基于深度学习的术语识别的准确性高度依赖序列标注数据的质量，因此，目前通过集成或微调预训练语言嵌入模型成为一种新的范式。",
    "context3": "从术语识别的流程(图1)来看,在词嵌入层，对其是否引入外部知识并将其整合到术语识别架构中仍没有形成共识。一方面,引入外部知识将增加额外的标注成本;另一方面，也会影响模型的泛化能力。此外,在特征提取层,Transformer通常适合较大数据集的训练，当数据集较小时，LSTM的性能有时会更优;在序列标注层,CRF是最常用的标签解码器，但当采用BERT等获取上下文语义关系的语言嵌入模型时，往往采用Softmax会带来更好的分类效果,因此,需要结合具体的任务选择合适的解码器。",
    "context4": "从应用角度来看，采用何种方法和架构取决于数据集和任务所处领域。一般来说,如果拥有较多高质量数据集，可选择从零开始训练模型并微调模型;对于特定应用领域,如医疗等,可使用相关领域的数据通过Fune-Tune构建预训练模型。",
    "context5": "然而，基于深度学习的术语识别的缺点也很明显,包括需要大量的标注数据、依赖于复杂模型、模型训练耗时长、模型跨领域泛化能力较弱等。导致这些缺点的原因，首先在专业术语方面，种类繁多的术语表述方式、内容混杂中英文、长短词嵌套等因素,都会提升实体边界划分的难度,造成术语识别错误、遗漏低频重要术语等情况的发生;其次在数据集方面，现存的领域语料库规模较小、缺乏标注的高质量语料，同样会导致模型效果不佳。"
  },
  "3部分开源模型介绍": {
    "context1": "为便于展开学术研究，一些机构发布预训练模型,这些开源模型均有开放的代码和数据集,方便学者和机构结合不同任务进行选择和学习。部分开源术语识别模型如表5所示。",
    "context2": "表5部分开源术语识别模型  \nTable5Part of the Open Source Term Identification Models",
    "context3": "<table><tr><td>发布时间</td><td>名称</td><td>发布机构</td><td>模型</td><td>术语抽取算法</td><td>资源下载</td></tr><tr><td>2021</td><td>CBLUE</td><td>中文信息学会医疗健 康与生物信息处理专 业委员</td><td>以BERT为基础的11白 种预训练模型</td><td>采用现实世界噪声数据,以BERT为代表 的11种中文预训练模型,完成包括医学 文本信息抽取、术语标准化、文本分类和 问答4大类经典任务</td><td>https://github.com/ CBLUEbenchmark/ CBLUE</td></tr><tr><td>2019</td><td>ERNIE</td><td>清华大学、华为</td><td>BERT</td><td>将词汇、句法和知识信息与BERT模型相 结合;可针对具体任务进行微调</td><td>https://github.com/ thunlp/ERNIE</td></tr><tr><td>2019</td><td>K-BERT</td><td>北京大学、北京师范 大学、腾讯</td><td>BERT</td><td>将知识图谱三元组转换为句子树，采用 BERT实现临床医学病例和药物识别</td><td>https://github.com/ autoliuweijie/K-BERT</td></tr><tr><td>2019</td><td>BioBERT</td><td>韩国高丽大学、Naver</td><td>BERT</td><td>在英文维基百科和BooksCorpus 进行 BERT预训练并初始化BioBERT;在生物 医学领域语料库(PubMed 和PMC)上进 行预训练并在下游任务微调</td><td>https://github.com/ dmis-lab/BioBERT</td></tr><tr><td>2019</td><td>LSTM Voter</td><td>法兰克福大学</td><td>ATT)-BiLSTM-CRF</td><td>结合5个NER工具训练结果,利用Atten- (Char-BiLSTM/Char-tion构建的字符级特征;BiLSTM学习上 下文信息,输出标签预测概率;CRF获取 标注序列</td><td>https://github.com/ texttechnologylab/ LSTMVoter</td></tr><tr><td>2018</td><td>Multi-BioNER</td><td>伊利诺伊大学、南加 州大学、斯坦福大学</td><td>BiLSTM-CRF</td><td>BiLSTM构建字符和词嵌,BiLSTM层学 习上下文信息;CRF获取标注序列</td><td>https://github.com/ yuzhimanhua/Multi- BioNER</td></tr><tr><td>2018</td><td>Collabo Net</td><td>韩国高丽大学</td><td>CNN-BiLSTM-CRF</td><td>使用CNN构建字符级单词嵌入;BiL- STM学习上下文信息,输出标签预测概 率;CRF获取标注序列</td><td>https://github.com/ wonjininfo/Collabo- Net</td></tr></table>"
  },
  "4结语": {
    "context1": "本文以深度学习模型在术语识别中的应用为视角，介绍实现术语识别的一般框架,并梳理现有研究中常用的模型以及结合多任务学习、迁移学习的应用。",
    "context2": "目前，基于深度学习的术语识别方法表现出了较好的术语抽取效果,但也面临诸多挑战,通过对现有研究工作的总结，本文认为未来可以从以下方面展开相关研究。",
    "context3": "(1)探索新的术语标注模式,解决目标语料标注不足的问题。目前术语识别的术语集具有很强的领域性，专业术语难以移植，未来可以考虑借助远程对齐外部知识库(如维基百科、百度百科、同义词典等）,使用企业开开源文本标注工具（如Prodigy①、Doccano等)对数据进行自动标注,结合当下网络语义环境对数据补充,节约人力成本。",
    "context4": "(2)融合术语多维特征，提升术语识别效果。目前的术语识别方法主要基于字符和词级以及其延伸的特征进行嵌入，还缺少对于领域特征、上下文特征、主题特征和分布特征等多维度信息的探索，因此将术语的多维特质(上下文特征、情感特征等)以及多种方法(如C-Value等)与深度学习模型相融合是未来研究的新方向。",
    "context5": "(3)提升术语识别模型的泛化能力。模型可以更准确地识别\"未知\"术语仍是需解决的难题。因此,在模型学习前,可以从词嵌入角度[49]和平衡数据集中实体分配比例[70]等进行考虑。模型训练时，可增加Dropout层、正则化解决过拟合问题,实现更优的性能。另外,可在神经网络模型基础上[7]采用生成式对抗网络（Generative Adversarial Networks,GAN)[2],通过增加噪声,提升模型的泛化能力。",
    "context6": "(4)探索小样本的术语识别技术。基于深度学习方法的术语识别通常需要大量带注释的训练数据,但是目前不同领域的语料库建设差异显著。例如,医学领域的语料库建设工作成熟,已有语料库有BC4CHEMD、BC5CDR和BC2GM等[60],但在文化传播、新能源、航空航天等领域语料库相对匮乏。未来需探索将领域内无标注和少量标注的数据结合进行术语识别的方法,探索小样本术语识别技术。",
    "context7": "(5)术语识别过程的可解释性。由于深度学习的黑箱属性，术语识别模型的学习结果具有抽象性且并不包含抽取的术语排序列表。因此,对深度学习术语识别的行为进行解释将成为新的研究方向。目前，已有研究对医学领域的深度学习可解释性应用进行梳理[73]，并探讨结合文本和图片信息提高可解释性的问题。此外,可视化工具,如乔治亚理工大学的DODRIO③,可以呈现进行句法分析时词之间依赖关系重要性排序的过程。",
    "context8": "(6)术语结果的评价问题。目前基于深度学习的术语结果评价主要分为两种：其一是人工评价，其二是借助准确率(Precision）、召回率(Recall)和F1值等常用指标,但目前缺失对不同模型之间领域适用性和相对优势的比较。"
  },
  "参考文献：": {
    "context1": "[1]Zhang Z Q，Gao J,Ciravegna F. SemRe-Rank: Improving AutomaticTerm Extraction ByIncorporatingSemantic Relatedness With Personalised PageRank[J].ACM Transactions on Knowledge Discovery from Data,2018,12(5): 57.   \n[2] Zadeh B Q,Handschuh S.Evaluation of Technology Term Recognition with Random Indexing[C]//Proceedings of the 9th International Conference on Language Resources and Evaluation. 2014:4027-4032.   \n[3] 刘建华,张智雄,徐健,等.自动术语识别—对科技文献进行 文本挖掘的重要技术方法[J].现代图书情报技术,2008,24(8): 12-17.(Liu Jianhua, Zhang Zhixiong,Xu Jian,et al. Automatic Term Recognition—An Important Method for Text Mining on Scientific Literature[J]. New Technology of Libraryand Information Service,2008,24(8): 12-17.)   \n[4] 丁君军,郑彦宁,化柏林.基于规则的学术概念属性抽取[J].情 报理论与实践,2011,34(12):10-14,33.(Ding Junjun,Zheng Yanning,Hua Bolin. Extraction of Academic Concept Attribute Based on Rules [J]. Information Studies: Theory & Application, 2011,34(12): 10-14,33.)   \n[5] 刘胜奇,朱东华.TValue 术语抽取法[J].情报学报,2013,32 (11):1164-1173.(Liu Shengqi, Zhu Donghua.Automatic Term Extraction Based on TValue[J].Journal of the China Society for Scientific and Technical Information,2013,32(11): 1164-1173.)   \n[6] 李丽双,王意文,黄德根.基于信息熵和词频分布变化的术语 抽取研究[J].中文信息学报,2015,29(1):82-87.(Li Lishuang, Wang Yiwen，Huang Degen．Term Extraction Based on Information Entropy and Word Frequency Distribution Variety[J]. Journal of Chinese Information Processing,2015,29(1): 82-87.)   \n[7] Lossio-Ventura JA,Jonquet C,Roche M,et al.Biomedical Terminology Extraction: A New Combination of Statistical and Web Mining Approaches[C]/Proceedings of JADT 2014:12es Journées Internationalesd'Analyse statistique des Données Textuelles.2014: 421-432.   \n[8]Frantzi K,Ananiadou S,Mima H. Automatic Recognition of Multi-wordTerms: TheC-Value/Nc-ValueMethod[J]. International Journal on Digital Libraries,20oo,3(2):115-130.   \n[9] Astrakhantsev N. Automatic Term Acquisition from DomainSpecific Text Collection by Using Wikipedia[J]. Proceedings of the Institute for System Programming of RAS,2014,26(4): 7-20.   \n[10]Lossio-Ventura JA,Jonquet C,Roche M,et al. Yet Another Ranking Function for Automatic Multiword Term Extraction[C]// Proceedings of the 9th International Conference on Natural Language Processing. 2014: 52-64.   \n[11] 庞贝贝,苟娟琼,张雷,等.基于主题分析和语用情境融合的多 义术语识别研究[J].情报理论与实践,2022,45(1):177-186. (Pang Beibei, Gou Juanqiong,Zhang Lei,et al.Research on Polysemy Recognition Based on Topic Analysis and Usage Context Fusion[J]. Information Studies:Theory & Application, 2022, 45(1): 177-186.)   \n[12] Nugumanova A,Akhmed-Zaki D,Mansurova M,et al. NMFBased Approach to Automatic Term Extraction[J].Expert Systems with Applications,2022,199: 117179.   \n[13] 张华,叶娜,周俏丽,等.基于分类策略的术语识别系统融合 [J].小型微型计算机系统,2015,36(2):385-390.(Zhang Hua, Ye Na,Zhou Qiaoli，et al. Classification Strategy Based Term Recognition SystemsCombination[J].Journal of Chinese Computer Systems,2015,36(2): 385-390.)   \n[14]屈鹏,王惠临.专利信息服务中的术语抽取[J].情报科学, 2015,33(9): 66-71.(Qu Peng,Wang Huilin. Term Extraction in Patent Information Services[J]. Information Science,2015,3(9): 66-71.)   \n[15] Wang R, Liu W, McDonald C.Featureless Domain-Specific Term Extraction with Minimal Labelled Data[C]//Proceedings of the Australasian Language Technology Association Workshop 2016. 2016: 103-112.   \n[16] Terryn A R,Hoste V,Lefever E.Tagging Terms in Text: A Supervised Sequential Labelling Approach to Automatic Term Extraction[J]. Terminology,2022,28(1): 157-189.   \n[17] 张雪,孙宏宇,辛东兴,等.自动术语抽取研究综述[J].软件学 报，2020,31(7):2062-2094.(Zhang Xue,Sun Hongyu，Xin Dongxing,et al. Survey on Automatic Term Extraction Research [J]. Journal of Software,2020,31(7): 2062-2094.)   \n[18]袁劲松,张小明,李舟军.术语自动抽取方法研究综述[J].计算 机科学,2015,42(8): 7-12.(Yuan Jinsong,Zhang Xiaoming,Li Zhoujun.SurveyofAutomaticTerminologyExtraction Methodologies[J]. Computer Science,2015,42(8): 7-12.)   \n[19] 胡雅敏,吴晓燕,陈方.基于机器学习的技术术语识别研究综 述[J].数据分析与知识发现,2022,6(2/3):7-17.(Hu Yamin,Wu Xiaoyan, Chen Fang.Review of Technology Term Recognition Studies Based on Machine Learning[J].Data Analysisand Knowledge Discovery,2022,6(2/3): 7-17.)   \n[20]祝清松,冷伏海.自动术语识别存在的问题及发展趋势综述 [J].图书情报工作,2012,56(18):104-109.(Zhu Qingsong,Leng Fuhai. Existing Problems and Developing Trends of Automatic Term Recognition[J].Library and Information Service,2012,56 (18): 104-109.)   \n[21]Li J, Sun A,Han J, et al. A Survey on Deep Learning for Named Entity Recognition[J]. IEEE Transactions on Knowledge and Data Engineering,2020,34(1): 50-70.   \n[22] Song B S,LiF,Liu Y S,et al. Deep Learning Methods for Biomedical Named Entity Recognition: A Survey and Qualitative Comparison[J].BriefingsinBioinformatics，2021，22(6): bbab282.   \n[23] 张卫,王昊,邓三鸿,等.面向数字人文的古诗文本情感术语抽 取与应用研究[J].中国图书馆学报，2021,47(4):113-131. (Zhang Wei,Wang Hao,Deng Sanhong,et al. Sentiment Term Extraction and Application of Chinese Ancient Poetry Text for Digital Humanities[J]. Journal of Library Science in China,2021, 47(4): 113-131.)   \n[24]Li L S, Jin L K,Huang D G.Exploring Recurrent Neural Networks to Detect Named Entities from Biomedical Text[C]// Proceedings of Chinese National Conference on Chinese Computational Linguistics, International Symposium on Natural Language Processing Based on Naturally Annotated Big Data. 2015: 279-290.   \n[25] Khosla K,Jones R，Bowman N.Featureless Deep Learning Methods for Automated Key-Term Extraction [EB/OL]. [2023-02- 08].https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/ reports/custom/15848334.pdf.   \n[26]Kucza M,Niehues J, Zenkel T,et al. Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks[C]//Proceedings of the Interspeech 2018.2018:2072-2076.   \n[27] 赵洪,王芳.理论术语抽取的深度学习模型及自训练算法研究 [J].情报学报,2018,37(9): 923-938.(Zhao Hong,Wang Fang.A DeepLearning Model andSelf-Training Algorithmfor Theoretical Terms Extraction[J].Journal of the China Society for Scientific and Technical Information,2018,37(9): 923-938.)   \n[28] 肖连杰,孟涛,王伟,等.基于深度学习的情报分析方法识别研 究——以安全情报领域为例[J].数据分析与知识发现,2019,3 (10):20-28.(Xiao Lianjie,Meng Tao,Wang Wei,et al. Entity Recognition of Intelligence Method Based on Deep Learning: Taking Area of Security Intelligence for Example[J].Data Analysis and Knowledge Discovery,2019,3(10): 20-28.)   \n[29]Vaswani A,Shazeer N,Parmar N,et al. Attention isAll You Need [C]/Proceedings of the 31st International Conference on Neural Information Processing Systems.2017: 6000-6010.   \n[30]Venugopalan M,Gupta D.An Enhanced Guided LDA Model Augmented with BERT Based Semantic Strength for Aspect Term Extraction in Sentiment Analysis[J].Knowledge-Based Systems,2022,246:108668.   \n[31]Gao Y Z,Yuan Y. Feature-Less End-to-End Nested Term Extraction[C]/Proceedings of CCF International Conference on Natural Language Processing and Chinese Computing. 2019: 607-616.   \n[32]Huang Z H, Xu W, Yu K. Bidirectional LSTM-CRF Models for Sequence Tagging[OL]. arXiv Preprint, arXiv: 1508.01991.   \n[33]Lample G,BallesterosM,Subramanian S,et al. Neural Architectures for Named Entity Recognition[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016: 260-270.   \n[34]Luo H S,Li TR,Liu B,et al. Improving Aspect Term Extraction with Bidirectional Dependency Tree Representation[J]. IEEE/ ACM Transactions on Audio, Speech, and Language Processing, 2019,27(7): 1201-1212.   \n[35]Lyu X R,Lyu XQ,Sun F S,et al. Patent Domain Terminology Extraction Based on Multi-feature Fusion and BiLSTM-CRF Model[C]//Proceedings of FSDM 2018.2018: 495-500.   \n[36] 王昊,邓三鸿,苏新宁,等.基于深度学习的情报学理论及方法 术语识别研究[J].情报学报,2020,39(8):817-828.(Wang Hao, Deng Sanhong，Su Xinning，et al．A Study on Chinese Terminology Recognition of TheoryandMethodfrom Information Science: Based on Deep Learning[J]. Journal of the China Society for Scientific and Technical Information,2020,39 (8): 817-828.)   \n[37]徐飞,叶文豪,宋英华.基于BiLSTM-CRF模型的食品安全事 件词性自动标注研究[J].情报学报,2018,37(12):1204-1211. (Xu Fei,Ye Wenhao, Song Yinghua.Part-of-Speech Automated Annotation of Food Safety Events Based on BiLSTM-CRF[J]. Journal of the China Society for Scientific and Technical Information,2018,37(12): 1204-1211.)   \n[38]LiPL, Yuan Z M,Tu WB,et al. Medical Knowledge Extraction and Analysis from Electronic Medical Records Using Deep Learning[J]. Chinese Medical Sciences Journal,2019,34 (2): 133-139.   \n[39] 冯鸾鸾,李军辉,李培峰,等.面向国防科技领域的技术和术语 识别方法研究[J].计算机科学,2019,46(12):231-236.(Feng Luanluan，Li Junhui，Li Peifeng，etal.Technology and Terminology Detection Oriented National Defense Science[J]. Computer Science,2019,46(12): 231-236.)   \n[40]Phongwattana T,Chan JH. Development of Biomedical Corpus Enlargement Platform Using BERT for Bio-entity Recognition [C]/ProceedingsofInternational ConferenceonNeural Information Processing.2019: 454-463.   \n[41]刘浏,秦天允,王东波.非物质文化遗产传统音乐术语自动抽 取[J].数据分析与知识发现,2020,4(12):68-75.(Liu Liu,Qin Tianyun,Wang Dongbo.Automatic Extraction of Traditional Music Terms of Intangible Cultural Heritage[J].Data Analysis and Knowledge Discovery,2020,4(12): 68-75.)   \n[42]Liu W J, Zhou P, Zhao Z,et al. K-BERT: Enabling Language Representation with Knowledge Graph[C]//Proceedings of the AAAI Conference on Artificial Intelligence.2020: 2901-2908.   \n[43]侯伟涛,姬东鸿.基于Bi-LSTM的医疗事件识别研究[J].计算 机应用研究,2018,35(7):1974-1977.(Hou Weitao,Ji Donghong. Research on Clinic Event Recognition Based Bi-LSTM[J]. Application Research of Computers,2018,35(7): 1974-1977.)   \n[44]Joshi M, Chen D Q,Liu Y H, et al. SpanBERT: Improving Pretraining by Representing and Predicting Spans[J]. Transactions of the Association for Computational Linguistics,2020,8: 64-77.   \n[45] 李明浩,刘忠,姚远哲.基于LSTM-CRF的中医医案症状术语 识别[J].计算机应用,2018,38(S2):42-46.(LiMinghao,Liu Zhong，Yao Yaozhe.LSTM-CRF Based Symptom Term Recognition on Traditional Chinese Medical Case[J]. Journal of Computer Applications,2018,38(S2): 42-46.)   \n[46]Desimone R.Neural Mechanisms of Selective Visual Attention [J].Annual Review of Neuroscience,1995,18:193-222.   \n[47]吴俊,程,郝瀚,等.基于BERT嵌入BiLSTM-CRF模型的中 文专业术语抽取研究[J].情报学报,2020,39(4):409-418.(Wu Jun, Cheng Yao,Hao Han, et al. Automatic Extraction of Chinese Terminology Based on BERT Embedding and BiLSTM-CRF Model[J].Journal of the China Society for Scientific and Technical Information,2020,39(4): 409-418.)   \n[48] 任秋彤,王昊,熊欣,等.融合GCN远距离约束的非遗戏剧术语 抽取模型构建及其应用研究[J].数据分析与知识发现,2021,5 (12):123-136.(Ren Qiutong,Wang Hao,Xiong Xin，et al. Extracting Drama Terms with GCN Long-Distance Constrain[J]. Data Analysis and Knowledge Discovery,2021,5(12): 123-136.)   \n[49]Han X W, Xu L Z, Qiao F. CNN-BiLSTM-CRF Model for Term Extraction in Chinese Corpus[C]//Proceedings of International Conference on Web Information Systems and Applications.2018: 267-274.   \n[50]Peng L F,Gao D,Bai Y J.A Study on Standardization of Security Evaluation Information for Chemical Processes Based on Deep Learning[J]. Processes,2021,9(5): 832.   \n[51]马建红,张亚梅,姚爽,等.基于BLSTM_attention_CRF模型的 新能源汽车领域术语抽取[J].计算机应用研究,2019,36(5): 1385-1389,1395.(Ma Jianhong, Zhang Yamei, Yao Shuang,et al. Terminology Extraction for New Energy Vehicle Based on BiLSTM_Attention_CRF Model[J].Application Researchof Computers,2019,36(5):1385-1389,1395.)   \n[52]李焕.基于深度学习与主动学习的中医术语识别研究[D].北 京：北京工业大学，2019.(Li Huan.Research of Chinese Medicine Terminology Recognition Based on Deep Learning and Active Learning[D]. Beijing: Beijing Universityof Technology, 2019.)"
  }
}