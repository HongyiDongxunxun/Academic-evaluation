{
  "original_filename": "full_9748.md",
  "近70年文本自动摘要研究综述": {
    "context1": "刘家益12,邹益民",
    "context2": "（1.中国科学院文献情报中心,北京100190；2.中国科学院大学,北京100049;",
    "context3": "3.浙江师范大学 经济与管理学院，浙江金华321004)",
    "context4": "摘要：【目的/意义】文本自动摘要能快速获取文本主要内容,极大提高信息使用效率,帮助人们从信息海洋中解放出来。随着互联网大数据日益深入发展,文本信息的数量已经远远超出人工处理极限，文本自动摘要研究显得越发迫切和重要【方法/内容】本文通过对过去70年国内外文本自动摘要经典文献重要文献进行收集、整理和分析,总结归纳出六类主要文本自动摘要方法及其理念和具体做法,对比评析其优势不足，并对未来研究方向进行展望,绘制出该研究领域的一个发展全景图。【结果/结论】自动摘要方法所使用的特征经历了由简单到复杂、由个体到联系、由表层到深层的发展路径;如何深入利用深度学习方法，以及如何将传统方法与深度学习方法更好结合起来将是下一步研究的热点和重点。",
    "context5": "关键词：自动摘要;机器学习;语言网络;深度学习；综述",
    "context6": "中图分类号：G254.37 文献标识码：A 文章编号:1007-7634(2017)07-154-08  \nDOI:10.13833/j.cnki.is.2017.07.027"
  },
  "A Review of Automatic Text Summarization Research in Recent 70 Years": {
    "context1": "LIU Jia-yi1², ZOU Yi-min3",
    "context2": "(1.Library of National Science,Chinese Academy of Sciences,Beijing 100190,China;",
    "context3": "2.University of Chinese Academy of Sciences,Beijing 100049,China;",
    "context4": "3.College of Economics and Management, Zhejiang Normal University, Jinhua 321004,China)",
    "context5": "Abstract:【Purpose/significance】The automatic text summarization methodcan quickly get the main content of text informationand improvetheeficiencyof informationuse,andthenefectivelyhelppeople tobeliberatedfrom information ocean .With therapiddevelopmentof Internetandbigdata,theamountof textinformationhas gonefarbeyondthe limitof manual processing,which makes the automatic summarization research increasingly urgent and important.【Method/ process】Inthis paper,through collecting,reorganizing andanalyzing theliteratureaboutautomatic text summarization in the past 7O years at homeand aboard,the author summedup six main types of automatic text summary methods and their ideasand practices,analyzedtheiradvantageandshortcomings,andlooked forwardtothefutureresearch direction,which is helpful to drawupadevelopment panorama oftheresearch field to referforlater.【Result/conclusion】Theconclusion shows that the features of using the automatictext summarization methods have changed from simple to complex,from individual torelation,fromthesurfaceto thedeep development,sohowtouse deep learning methods,andhowto combine thetraditional methods with the depth learning methods betteristhe hotspot and focus of the nextstep research.",
    "context6": "Keywords: automatic summarization,； machine learning; language network； deep learning；review",
    "context7": "成文本摘要的理论方法和技术。"
  },
  "1引言": {
    "context1": "文本摘要通常是指从单个或多个文档中产生一段文本，该文本传达了原始文本中的主要信息,但是仅有不到原始文本一半甚至更少的篇幅[-3]。文本自动摘要是利用计算机生",
    "context2": "文本自动摘要是一个既传统又前沿的研究领域。自20世纪50年代自动摘要技术出现以来，每出现一波新技术浪潮,都会涌现出一大批相应的自动摘要新方法,然而效果一直与人工摘要有差距。在互联网大数据时代,文本信息的数量已经远远超出了人工处理的极限，自动摘要研究显得越发迫切和重要。深度学习方法在相关领域的出色表现,让研究人员看到了一丝曙光。但传统经典方法对深度学习方法仍有帮助作用[4]。实际上自动摘要方法发展历程也印证了这一点,即,某些经典的文档特征和理念,无论是对于传统旧方法,还是对新兴方法,都极为有用,最出色的方法通常是结合了新旧理念的方法。因此极有必要对自动摘要方法进行全面梳理和总结。",
    "context3": "本文以时间为主线，对过去70年国内外文本自动摘要相关文献进行收集、整理和分析，梳理出六类主要的文本自动摘要方法，归纳每一类方法的主要研究思想和做法，同时评析其不足,并对未来的发展方向进行展望,绘制出该研究领域的一个发展全景图,以期为后来者提供参考。"
  },
  "2 基于简单统计的方法": {
    "context1": "自动摘要研究是从抽取文档中的重要句子开始的[]。如何判断句子重要性是影响抽取式摘要质量的关键因素。在研究初期，人们通过统计简单直观的文本特征,如词频、词位置、特定的线索词、标题等，从文档中识别重要句子组成摘要。",
    "context2": "词频是最简单有效的方法。直觉告诉我们,除去停用词外,在文中出现次数越多的词,越有可能更重要。根据词频确定词权重，以组成句子的词权重之和作为句子权重来判断句子重要性,是基于词频的自动摘要方法的主要思想。Luhn在其经典论文中指出[5],句子的重要性可通过分析句子中所包含的词来得到。他对所有词的出现频率进行统计，根据经验确定一个频率区间,区间内的词作为重要词,区间外的高频词和低频词则视为噪声。句子重要性的计算如图1所示。",
    "context3": "![](images/89569fd77d9b9763272c5a21060dacfea574b502eb3dec5b9bf080af2f5355e0.jpg)  \n图1Luhn提出的句子重要性计算方法示意图[5]",
    "context4": "括号中部分是句子的包含重要词和不超过四个非重要词的部分,如果某个句子包含这样的部分,则该句将被选中作为候选摘要句。句子的重要性得分按公式(1)计算：",
    "context5": "句子位置对句子的重要性影响很大,Baxendale对句子位置与文本主题进行研究发现[, $8 5 \\%$ 的主题句处于段首，$7 \\%$ 的主题句处于段尾。Edmundson[在研究中指出,特定的词可对句子重要性起到提示作用，例如\"几乎不(hardly)”、“不可能(impossible)”、“显著地(significant)\"等,他将这类词称为\"线索词(CueWord)”。Edmundson指出,句子是否是题目或标题也是衡量句子重要性的有用特征，他将词频、句子位置、线索词、标题词四个特征进行线性组合，作为衡量句子重要性的指标，取得了一定效果。",
    "context6": "词频、句子位置、线索词、句子是否为标题这四个特征是早期抽取式自动摘要所使用的主要特征。但这类方法只是对句子或词本身的表层特征进行简单统计,既未充分利用词的相关词(relatedwords）、词间关系等，也未充分利用有用的背景语料及其他外部资源。针对上述问题,一些改进方法被提出来。"
  },
  "3利用外部资源的方法": {
    "context1": "随着自动摘要研究的深入，研究人员不再满足于简单统计文本本身的特征,开始借助外部资源来辅助确定文档中的词权重、获取词间语义关系等,从而识别重要句子。常用的外部资源有背景语料、同义词典、知识库等,较为著名方法的有TFIDF、词汇链等方法。",
    "context2": "TFIDF方法最早由Salon提出[],它较好地利用了背景语料,从语料全局角度确定词权重。它的基本思想是,在一个语料库中，一个词的重要性与词频正相关，与包含它的文档数负相关。TFIDF算法在词频基础上引入了逆文档因子，能有效地甄别类似于停用词的高频无效词，较好地改进了词频法。TFIDF方法被广泛用于确定词的重要性等问题[2-15]。利用TFIDF方法进行自动摘要的基本思路是，首先通过背景语料集统计各个词的TFIDF值，作为词的重要性得分，然后计算文章各句的词的TFIDF和，作为句子重要性得分,抽取最重要句子作为摘要。一些其他类型的自动摘要方法中，通常也会融合考虑TFIDF特征。",
    "context3": "Barzilay等[提出了基于词汇链(lexical chains)的自动摘要方法，该方法不再以单个词作为分析单元，而是利用WordNet[]、词性标注工具、维基百科等对词义进行分析，把原文中与某个主题相关的词集合起来,构成词汇链。词汇链的重要程度由公式(2)决定：",
    "context4": "$$\nS c o r e ( C h a i n ) = L e n g \\ t h \\ast H I\n$$",
    "context5": "公式(2)中,Length是词汇链中所有词的词频之和。HI全称为Homogeneity Index,是均一度指数,由公式(3)决定：",
    "context6": "$$\nS = { \\frac { \\mathrm { s i g n } ^ { 2 } } { n } }\n$$",
    "context7": "其中, $\\mathrm { s i g n }$ 代表括号内的重要词数,n代表括号中的总词数。依据此算法，图一中句子得分为 $1 6 / 7 { = } 2 . 3$ 。根据候选句重要性得分高低顺序，选出最高得分的若干句子，作为摘要。这一方法被广泛采用,并取得较好效果。一些学者对句子评分的计算方法进行改良,但计分的基础仍是词频[-8]。",
    "context8": "$$\nH I = 1 - \\frac { D } { L e n g \\operatorname { t h } }\n$$",
    "context9": "其中，D是对词汇链中的词汇去重后词的个数，如果每个词的词频均为1,则D=Length,均一度指数 $\\scriptstyle \\mathrm { H I = 0 }$ 。计算出词汇链的得分后，再根据公式(4),选出强词汇链。",
    "context10": "$$\n\\begin{array} { r l } & { S c o r e ( C h a i n ) } \\\\ & { > A v e r a g e ( S c o r e s ) + 2 ^ { * } S \\tan d a r d D e v i a t i o n ( S c o r e s ) } \\end{array}\n$$",
    "context11": "确定强词汇链以后，Barzilay制定了三条启发式规则，为每个强词汇链抽取一个句子,构成摘要。词汇链方法由于利用背景知识资源将相关词聚集起来表示文档主题,对自动摘要结果有一定改进,引起了一些关注。一些学者对词汇链方法进行了改进[18-20]。",
    "context12": "与词汇链方法类似,一些学者[21-22提出,用概念(con-cept)取代词作为句子重要性的基本分析单元。在这里,概念是指描述某一抽象与具体实体及其行为的同义词或短语，这些词或短语可通过词性标注和文本分块等技术来自动获得。概念的重要性与概念出现频率正相关,与包含概念的文档数负相关。",
    "context13": "通过引入外部资源,研究人员能够确定并利用词的全局重要性、词义关系等判断句子重要性，较为有效地补足了简单统计方法的短板。但无论是此类方法,还是简单统计方法，受限于当时的技术条件，对原文本的分析都还停留在比较浅的层面。"
  },
  "4 基于修辞结构的方法": {
    "context1": "修辞结构理论(Rhetorical Structure Theory,RST)由 Mann和Thompson[23]于1987年提出。该理论认为,命题单元是文本的基本组成单位,所有命题单元(propositionalunit)都通过话语关系连接,从而构成连贯文本。话语关系连接的命题单元分为核心(Nucleus)和外围(satellite)两类,核心是中心命题,而外围则是对核心的补充和支持。修辞结构通常表示为树结构,如图2,示例文本[24中存在四个命题单元,它们的修辞结构如图3所示。其中N、S分别代表话语关系中的核心和外围。",
    "context2": "![](images/93851ed3d09806a421ac32ac132049a3993987da330eb268a42f60bce2d21d47.jpg)  \n图2修辞结构示例文本 $1 ^ { [ 2 4 ] }$",
    "context3": "![](images/fef868a50c9e70e7bf73c725b9d42be9e21ef38daa6bf7a56a88a93e3d23a2ec.jpg)  \n图3示例文本1的修辞结构[24]",
    "context4": "![](images/35e57dcf7cb99362517422bf445c39b66dc5b068653bc2fe2554cdcd20b322af.jpg)  \n图4Ono自动摘要算法示意图[25]",
    "context5": "RST对核心和外围命题的区分被应用于自动摘要研究中。基于RST生成摘要就是利用文档的修辞结构特征挑选核心命题的过程。Ono等[2提出了第一个也是最简单的基于RST的自动摘要算法。该方法假定根节点的权重等于树的层数，从根开始，以深度优先算法向下遍历树，每经过一个外围弧,则权重减一,经过核心弧时权重不变。以该算法遍历图3后得到的各结点的权重值如图4,图中黑色加分数字即为对应结点得分。由此方法得到的命题单元是部分有序的,即,会有若干结点的得分相同。",
    "context6": "ODonnell[2在Ono方法的基础上改进,认为不同的话语关系，其权重也是不同的。根节点的权重仍为树的深度，从根节点开始，深度优先遍历,遇到外围弧，则下一个结点的权重即为当前权重值乘以对应的话语关系权重。例如，随机地假定Non-Vol-Result的权重为0.8,Concession的权重为0.6,List的权重为0.4,则根据此算法，图2中的RST树结点的得分如图5所示。该方法得到的也是部分有序的结果。",
    "context7": "![](images/cb55c13880f7995ea71710d2ab42e93d53d289da836fc7edc653c59ceeef93e4.jpg)  \n图5O'Donnell自动摘要算法示意图[26]",
    "context8": "Marcu提出[2]使用促进集(Promotion set)来决定最重要结点的方法。RST树中的每个内部结点都有促进集。一个结点的促进集自底向上生成,由该结点的核心子结点的促进集的并集组成。叶子结点的促进集是它本身。根节点的权重是树的深度。使用深度优先策略从根节点出发，访问指定结点，途中，每经过一个不包含该结点的促进集，则权重减一，直至到达该结点。根据该算法，图2中RST树的3号结点的得分如图6,图中大括号中的数字即为对应结点的促进集。",
    "context9": "![](images/de3f94d595200a245613046b3bd7db4682f6926a75dc56d4aa814377d2d7781d.jpg)  \n图6 Marcu自动摘要算法示意图[27]",
    "context10": "修辞结构独辟蹊径，以命题为文本分析单元，能较好保留文本的语义完整性。但修辞结构理论主要用于同一修辞结构树内重要命题的识别,难以用于不同树间,存在局限性。此外,修辞结构的解析生成方法目前尚不成熟,难以大规模应用。因此基于修辞结构的方法近30年来并未得到广泛关注。"
  },
  "5基于统计机器学习的方法": {
    "context1": "20世纪90年代,机器学习方法，尤其是监督机器学习方法在自然语言处理领域得到广泛使用。该方法通过对人工标注语料进行训练,可以获得句子的文本特征(features)与句子重要性的关系模型,利用此模型,即可对未标注句子的重要性进行自动预测,生成摘要。",
    "context2": "![](images/baaf10b04e2f7fb6375ac6d16c273dfe2233e4dfcec60599afacd870e10da421.jpg)  \n图7机器学习的一般过程",
    "context3": "在这类方法中，抽取式自动摘要问题被转化为一个二元分类问题。一个句子要么是摘要句,要么是非摘要句[3]。首先人工将重要句子标注出来,然后提取这些句子的特征，学习算法通过统计分析学习，得到特征与句子重要性的关系，进而得到合适的分类器。向分类器输入句子，可得到句子的重要性得分。依据得分高低，即可抽取重要句子，生成摘要。",
    "context4": "首次将机器学习方法应用于自动摘要的是Kupec 等[28],他们共选取了句子长度特征、线索短语特征、段落特征、主题词特征、大写词特征等五类特征，并假设这些特征之间相互独立。该方法从21种科技期刊中随机选取了188篇文章，由专业摘要员参照原文编写摘要,作为训练语料,使用Bayes分类模型(见公式(5))进行训练得到一个分类器,用于给句子的重要性进行打分。基于该分类器生成的占原文大小$2 5 \\%$ 的摘要,准确率达 $8 4 \\%$ 。",
    "context5": "$$\nP ( s \\in S | F _ { 1 } , F _ { 2 } , . . . F _ { k } ) { = } \\frac { \\prod _ { j = 1 } ^ { k } P ( F _ { j } | s \\in S ) P ( s \\in S ) } { \\prod _ { j = 1 } ^ { k } P ( F _ { j } ) }\n$$",
    "context6": "机器学习方法极大地释放了计算机的计算能力。借助统计理论和算法,计算机不再停留于简单统计,而是进一步可对海量文本信息进行高效深入地分析建模，揭示人工难以直观发现的,隐藏在海量文本中的隐性模式。较为广泛使用的算法包括朴素贝叶斯算法[28-29]、决策树算法[30]、最大熵算法[31]、隐马尔科夫算法等[32]。",
    "context7": "机器学习方法还极大地拓展了特征选择范围。因为可用于训练分类器的特征是开放的,从实践上看,任何可能指示句子重要性的特征均可用于训练分类器。最初的方法假设特征之间独立，后续的研究则尝试选择更合适的特征[33],以及假设特征之间不相互独立[30.32]。这其中包括前文提到的词频、句子位置、线索词、TFIDF值、词汇链等传统特征,此外,研究者还提出了多种开放性的句子特征,例如：包含标题词数、平均词汇连接度、首字母大写词数、是否包含数字数据、是否包含代词、是否包含形容词、是否包含日期、是否包含命名实体等等。"
  },
  "6 基于语言网络的方法": {
    "context1": "1998年，Nature和Science上分别发表了有关小世界网络和随机网络的论文[34-35]，两篇文章在全球科学领域产生了巨大影响,被认为是复杂网络研究的里程碑。语言是复杂网络的一个极好例证[3：它在句法、语法、语义、语音等几乎所有层面都展示了复杂的网络结构，并且随着时间推移而动态演化。Ricard等指出，语言是一个复杂网络，构成网络的结点可以是词、概念、句子等文本单元,结点之间以句法、语义、语音、拓扑等产生关系。",
    "context2": "网络模型的一个重要应用是对结点的重要性进行评估，因此可通过构建语言网络模型,利用网络结构特征从原文档中识别重要词和重要句子,进而提取重要内容。基本思路是,以词、句子等文本单元及其相互关系构建语言网络模型，利用网络结构特征选出重要结点,进而选出重要句子。Pag-eRank是一个经典的网络结点重要性算法，被广泛应用于语言网络研究，基本思想是：如果某顶点有一条边指向V顶点，就表示该顶点投给V顶点一票。如果投向V结点的票数越多,就表明B顶点越重要。V顶点的重要性还与投向它的顶点自身的权重有关,投向V顶点的顶点自身权重越高，则B顶点的权重也越高。该算法的核心思想如公式(6)所示[38]：",
    "context3": "$$\nS ( V _ { i } ) = ( 1 - d ) + d ^ { * } \\sum _ { j \\in I n ( V _ { i } ) } \\frac { 1 } { \\left| O u t ( V _ { j } ) \\right| } S ( V _ { j } )\n$$",
    "context4": "式中， $I n ( V )$ 表示所有指向V的结点， $O u t ( V _ { j } )$ 表示所有从V指出的结点， $S ( V )$ 表示结点V的权重,d是一个概率值，表示随机点击一个新链接的概率,通常取值 $0 . 8 5$ 。",
    "context5": "最先将语言网络方法用于自动摘要的是Mihalcea等[39],他们以句子为顶点，以句子间的相似度为边，构建了无向文本网络。通过计算边的权重,以及顶点的权重,来确定重要句子。边的权重由边所连接的两个句子的相似度给出,该相似度通过公式(7)计算：",
    "context6": "$$\nW ( V _ { \\mathrm { 1 } } , V _ { \\mathrm { 2 } } ) = \\frac { C } { \\log l _ { \\mathrm { 1 } } + \\log l _ { \\mathrm { 2 } } }\n$$",
    "context7": "其中, $V _ { \\scriptscriptstyle 1 }$ 和 $V _ { \\scriptscriptstyle 2 }$ 是两个句子，c是两个句子共同具有的词数，L1和L2分别是v1和 $\\mathbf { v } 2$ 的长度。相似度也可通过余弦相似度、最大相同子句等方法求得。句子(即顶点)的权重值通过公式(8)定义：",
    "context8": "$$\n\\mathit { W S } ( V _ { i } ) = ( 1 - d ) + d ^ { * } \\sum _ { V _ { j } \\in \\mathit { I n } ( V _ { i } ) } \\frac { w _ { j i } } { \\sum _ { V _ { k } \\in \\mathit { O u t } ( V _ { j } ) } w _ { j k } } \\mathit { W S } ( V _ { j } )\n$$",
    "context9": "其中，d是从一个顶点指向另一个顶点的概率值，通常取 $0 . 8 5 { : } \\mathrm { I n } ( \\mathrm { V i } )$ 表示指向Vi的顶点， $\\mathrm { { O u t } ( V i ) }$ 表示从Vi指出的顶点,对于无向图， $\\mathrm { { I n } ( V i ) { = } O u t ( V i ) ; }$ Wij则表示Vi与Vj之间的边的权重。该方法使用纯粹的语言网络模型,除句子相似性外，未利用句子的任何其它统计特征，但在15个对比系统中取得了第三名的好成绩，优于大多数之前的算法，确实令人振奋，也反映出语言网络的有效性。",
    "context10": "之后的研究者们进一步深化了基于语言网络的自动摘要方法[39-44],尝试以词[42]、句子[41]、语义概念[43-44]（见图8)等作为文本单元,以句子相似性[4I]、词共现关系、语义相似性[41.43]、句法关系[42]（见图9）、共指关系（Coreference Rela-tion)[41]、话语关系(Discourse Relation)[41]等作为文本关系,构建语言网络，提取文档摘要。图10揭示了以句子为结点，以句子相似性、语义相似性、共指关系、话语关系等四种关系构建语言网络进行自动摘要的结果差异。",
    "context11": "![](images/50902b53a6227476274de0ecaa0272bf8c66f4e0294779aedb865ea56ac6d9ed.jpg)  \n图8一个简化的文档语义图[43]",
    "context12": "![](images/5d6adcb4547e502b2a9cebfc4f9a40f911e7d4a5056ada1865c71b36a0512864.jpg)  \n图9根据句法关系生成的语言网络[42]",
    "context13": "![](images/2b8d7b5cc27aa20ee9b7e4b0ab6ff94d770767668c297d1c93f7188b88fc4005.jpg)  \n图10以不同关系构建语言网络进行自动摘要的结果对比[41]"
  },
  "7基于深度学习的方法": {
    "context1": "深度学习是一类广泛的机器学习技术和架构，其特点是采用多层的非线性信息处理方法[45]。2006年,Hinton等[4]提出了深度置信网络(DBN)和相应的高效学习算法，该方法不仅具有很强的无监督学习能力，降低算法对人工的依赖，还能较为高效地进行训练,成为其后至今深度学习算法的主要框架[47-48],标志着深度结构学习(也称深度学习)作为机器学习的一个新领域出现了。通常把深度结构分为三类[45.47]：无监督或生成式深度网络、判别式深度网络、混合式深度网络。",
    "context2": "目前深度学习方法已逐渐在包括自然语言处理的众多领域中广泛应用起来[48-49]。其最吸引人的地方在于,可通过全局上下文的学习,将词级、短语级或句子级符号信息表示为连续实值向量(也称嵌入,word embedding),使文本语义特征可计算,免去对外部资源和人为设计特征的依赖[45]。深度学习在自动摘要研究上的应用起步较晚,近年来也取得了一定进展。目前的方法主要有两类。",
    "context3": "一类是利用深度结构强大的学习能力，进行抽取式自动摘要。Yan等人[s0-51设计了一种深度架构用于多文档自动摘要,首次将深度学习用于自动摘要。在该架构中，输入层是文档的词频向量f $\\mathsf { \\Phi } ^ { \\mathrm { d } } = \\{ \\mathrm { f } _ { 1 } ^ { \\mathrm { ~ d ~ } } , \\mathrm { f } _ { 2 } ^ { \\mathrm { ~ d ~ } } , . . . . . . . \\mathrm { f } _ { \\mathrm { v } } ^ { \\mathrm { ~ d } } \\}$ ,根据词汇集和文档d计算，其中V是词汇集长度;输出层是摘要 $\\mathrm { S } { = } \\{ \\mathrm { s } _ { 1 } , \\mathrm { s } _ { 2 } , \\ldots \\ldots , \\mathrm { s } _ { t } \\}$ ;隐层则由若干个受限玻尔兹曼机(RBMs)构成,多个隐层使得底层特征可用于推断出更优的高层特征,而高层特征的有效性又可通过底层来验证。深层架构的初始参数由查询词决定，其他层的参数则是随机的。深度学习过程被划分为三个阶段，对应自动摘要的三个阶段，即概念抽取（Concept Extrac-tion）、重建验证(Construction Validation）和摘要生成（Sum-mary Generation）。在概念抽取阶段，三个隐层使用贪婪分层提取算法(greedy layer-wise extraction algorithm)对文档进行概念提取;重建验证阶段通过对深层结构进行全局优化来重构数据分布，最后，第三阶段,通过动态规划方法选出最重要句子作为摘要。该深层架构详见图11。实验表明，该架构比现有的大多数方法更出色，能够到达当前的最佳水平。Padmapriya等[52-53]也提出了类似方法。",
    "context4": "![](images/ce7bde1a7593c9885cbe498e55306756f6928290075212686374a72221bc31de.jpg)  \n图11面向查询的深层提取架构[51)",
    "context5": "另一类方法是利用深度学习方法自动生成句子作为摘要。“序列到序列（sequence-to-sequence）”方法在机器翻译[54]、语言识别[s]、视频字幕[56]等研究上取得了很好效果。自动摘要问题同样可视为从原文本到摘要文本的映射，因此可使用序列到序列建模方法来解决。不同之处在于，首先,摘要的长度并不依赖于原长度;其次摘要相对于原文而言通常有信息损失[4]。Facebook公司的Rush等[57]首次将深度学习用于概括式自动摘要研究,他们用卷积模型对原文档进行编码，用上下文相关的注意力前馈神经网络生成摘要，取得了当前最好结果。Chopra等[5使用同样的编码器对原文档进行编码,但以递归神经网络(RNN)作为解码器,大大提高了摘要效果。IBM公司的Nallapati等4将目前在机器翻译研究中效果最好的注意递归神经网络(RNN)编解码器模型[54应用到自动摘要研究中,同时设计编解码器对词特征、停用词、文档结构等有用信息进行利用,取得了优于单纯利用RNN模型的效果，也远远优于现有自动摘要方法。谷歌公司在2016年开源了其深度学习框架TensorFlow[59中的自动摘要模块Textsum，该模块同样使用\"序列到序列\"思想，基于深度学习自动生成新闻标题,效果近似于人工。",
    "context6": "![](images/363e991ee9652c05c2672f7177496ddd9f79102f82238a87259b9e2655d0c7e5.jpg)  \n图12文献【4]中利用词特征的编解码器",
    "context7": "基于深度学习的自动摘要方法是目前效果最好的方法，但研究数量和研究深度还不够，有待进一步深化。另外，深度学习方法与传统方法的结合也是未来研究的重点。"
  },
  "8总结与展望": {
    "context1": "算,再结合神经网络语言模型的强大计算能力,在生成式摘要上取得了一定突破。",
    "context2": "在自动摘要的初期研究中，由于计算能力有限，人们尝试以原文的词、句为单元，以简单统计规律如词频、句子位置、线索词等为线索,找出包含重要内容的句子。随后，人们开始借助文档外部的知识资源，如WordNet的语义资源、维基百科的知识资源，以及背景语料、人工标注等，以帮助挑选重要词、重要主题,进而判断句子重要性,拓展了自动摘要方法的广度。基于修辞结构的方法对文本的修辞结构特征进行分析,也取得了可以预期的效果。随着计算资源的日益丰富,机器学习方法逐渐兴起,释放了机器的计算能力,使复杂统计理论得以应用,同时也极大地拓宽了特征选取的范围，有力推动了自动摘要研究方法的发展。语言网络方法的引入，则将自动摘要方法从单独分析文本单元引入到分析文本单元的相互关系的新的层面。研究者利用图论,对文本单元间的各种关系,如句子词共现、句子相似度、语义关系、话语关系等,进行了探索,找出其与句子重要性的关联以辅助判断句子重要性。研究者还将文本的句法、语义等表示为图，通过图模型直接对文本的句法、语义进行分析，均取得了显著效果。深度学习方法尝试对文档的文本单元进行编码,将文本单元的句法语义等特征表示为多维向量，使特征可计",
    "context3": "梳理近70年的自动摘要研究可以发现，自动摘要方法所使用的特征经历了一个由简单到复杂、由个体到联系、由表层到深层的发展路径，自动摘要正在从浅层方法向深层方法深入发展。简单统计方法、利用外部资源方法、机器学习方法等,都是典型的浅层方法,而机器学习方法,由于理论上可以对各种浅层文本特征进行学习，因此将这类方法发挥到了极致;语言网络方法的引入,不仅为分析句子间的统计关系提供了有力工具,还为表现和分析语言句法、语义、话语关系等提供了有力工具，连接了浅层方法和深层方法使得对文本的分析能够进一步深入。深度学习方法则将局部或全文上下文的多维特征进行编码,使得文本的上下文特征、句法特征、语义特征都变为可计算,进一步打开了深层方法的大门。可以预见，未来的自动摘要研究将主要集中在基于深度学习的深层方法上。如何深入利用深度学习方法，以及如何将传统方法与深度学习方法更好地结合起来是下一步研究的热点和重点。"
  },
  "参考文献": {
    "context1": "1 Das D,Martins A F T.A survey on automatic text summarization[EB/OL].https://www.cs.cmu.edu/\\~af m/Home_files/Das_Martins_survey_summarization.p df,2016-03-24.   \n2 Radev D R,Hovy E,McKeown K.Introduction to the Special Issue on Summarization[J]. Computational Linguistics,2002,28(4):399-408.   \n3 Nenkova A,McKeown K.A survey of text summarization techniques[C]//Charu C.,Aggarwal C Z.Mining Text Data.New York: Springer US,2012:43-76.   \n4 Nallapati R,Zhou B,Santos C N D,et al. Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond[EM/OL].https://arxiv. org/pdf/1602.06023,2017-01-05.   \n5 Luhn H P.The automatic creation of literature abstracts[J]. IBM Journal of research and development, 1958,2(2):159-165.   \n6Radev D R.Text Summarization Tutorial of ACM SIGIR2001[EB /OL].http://www.summarization.co m/sigirtutorial2001.ppt,2016-11-23.   \n7 Haghighi A,Vanderwende L.Exploring content models for multi-document summarization[EB/OL]. http: //dx.doi.0rg/10.3115/1620754.1620807,2016-11- 25.   \n8Vanderwende L, Suzuki H,Brockett C,et al. Beyond SumBasic:Task-focused summarization with sentence simplification and lexical expansion[J]. Information Processing& Management,2007,43(6):1606-1618.   \n9 Baxendale P B.Machine-Made Index for Technical Literature—An Experiment[J]. IBM Journal of Research & Development,1958,2(4):354-361.   \n10 Edmundson H P. New Methods in Automatic Extracting []. Journal of the ACM,1969,16(2):264-285.   \n11 Salton G, Yu C T.On the construction of effective vocabularies for information retrieval[J].ACM Sigplan Notices, 1975, 10(1):48-60.   \n12 施聪莺,徐朝军,杨晓江.TFIDF算法研究综述[].计算机 应用,2009,29(S1):167-170.   \n13 徐文海,温有奎.一种基于TFIDF方法的中文关键词抽取 算法].情报理论与实践,2008,31(2):298-302.   \n14 Zhang B.AN IMPROVED TEXT FEATURE WEIGHTING ALGORITHM BASED ON TFIDFJ]. Computer Applications & Software,2011,28(2):17-20.   \n15 李静月,李培峰，朱巧明.一种改进的TFIDF网页关键词 提取方法[].计算机应用与软件,2011,28(5):25-27.   \n16 Barzilay R,Elhadad M. Using Lexical Chains for Text Summarization[EB/OL].https://academiccommons.columbia.ed u/catalog/ac:160050,2016-08-28.   \n17Miller G A.WordNet: a lexical database for English[J]. Communications of the ACM,1995,38(11):39-41.   \n18 Silber H G, Mccoy K F. Efcient text summarization using lexical chains[C]// Proceedings of IUI 2OoO International Conference on Intelligent User Interfaces. New York: ACM Press,2000:252-255.   \n19 Kolla, Maheedhar.Automatic text summarization using lexical chains: algorithms and experiments[D]. Lethbridge: University of Lethbridge,2004.   \n20 Pourvali M,Abadeh M S. Automated Text Summarization Base on Lexicales Chain and graph Using of WordNet and Wikipedia Knowledge Base[J]. International Journal of Computer Science Issues, 2012, 9(1):144-158.   \n21Schiffman B,Nenkova A, Mckeown K.Experiments in multidocument summarization[C]// HLT $^ { \\prime } 0 2$ Proceedings of the second international conference on Human LanguageTechnology Research. San Francisco: Morgan Kaufmann Publishers Inc., 2003:52-58.   \n22 Ye S, Chua T S,Kan MY,et al. Document concept latice for text understanding and summarization[J]. Information Processing & Management, 2007,43(6):1643-1662.   \n23 Mann W C, Thompson S A. Rhetorical Structure Theory: A Theory of Text Organization[J]. Text - Interdisciplinary Journal for the Study of Discourse,1987,8(3):243-281.   \n24 Da V,Rodrigues C,Pardo T A S,et al. A comprehensive comparative evaluation of RST-based summarization methods[J].Acm Transactions on Speech & Language Processing,2010,6(4):4:1-4:20.   \n25 Ono K, Sumita K, Miike S. Abstract Generation Based on Rhetorical Structure Extraction[EB/OL]. https://arxiv.org/ abs/cmp-lg/9411023,2016-10-06.   \n26O'Donnell M. Variable Length On-Line Document Generation[J]. Journal of Cancer Education,1997,16(3):1-2.   \n27Marcu D C. The rhetorical parsing,summarization, and generation of natural language texts[D]. Toronto: University of Toronto,1997.   \n28 Kupiec J.,Pedersen J.,Chen F.A trainable document summarizer[C]//Mani I.,Maybury M. T.Advances in Automatic Text Summarization. Cambridge:MIT Press,1999: 55-60.   \n29Aone C, Okurowski ME,Gorlinsky J,et al.A Trainable Summarizer with Knowledge Acquired from Robust NLP Techniques[C]// MANI I, MAYBURY M T.Advances in Automatic Text Summarization. Cambridge: MIT Press, 1999:71-80.   \n30 Lin C Y. Training a selection function for extraction[C]// CIKM ‘99 Proceedings of the eighth international conference on Information and knowledge management. New York: ACM Press,1999:55-62.   \n31 Osborne M. Using maximum entropy for sentence extraction[C]// AS 02 Proceedings of the ACL-02 Workshop on Automatic Summarization. Stroudsburg: Association for Computational Linguistics,2002: 1-8.   \n32 Conroy JM, O'Leary D P. Text summarization via hidden Markov models[C]// SIGIR ${ \\bf \\tilde { \\Delta } } _ { 0 1 }$ Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. New York: ACM Press,2001:406-407.   \n33Eduard C YL.Identifying Topics by Position[C]//ANLC ‘ 97 Proceedings of the fifth conference on Applied natural language processing. Stroudsburg: Association for Computational Linguistics,1997:283-290.   \n34 Watts D J, Strogatz S H. Collective dynamics of ‘mall-wor ld' networks[J].Nature,1998,393(6684):440-442.   \n35Barabasi A L,Albert R. Emergence of Scaling in Random Networks[J]. Science,1999,286(5439):509-512.   \n36 Steels L. Language as a Complex Adaptive System[C]// Parallel Problem Solving from Nature PPSN VI (PPSN 2000. Lecture Notes in Computer Science).Berlin: Springer, 2000:17-26.   \n37 Solé R V, Bernat C M, Sergi V, et al. Language networks: Their structure, function，and evolution[J]. Complexity, 2010,15(6):20-26.   \n38Page L. The PageRank citation ranking: bringing order to the Web[J]. Stanford Infolab,1998,9(1):1-14.   \n39 Mihalcea R,Tarau P. TextRank: Bringing Order into Texts []. Unt Scholarly Works,2004,(9):404-411.   \n40 Erkan G. Radev D R. LexRank: grabh-based lexical centrality as salience in text summarization[J]. Journal of Artificial Intelligence Research,2004,22(1): 457-479.   \n41Ferreira R,Freitas F,Cabral L D S,et al.A Four Dimension Graph Model for Automatic Text Summarization[C]// WI-IAT ‘13 Proceedings of the 2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT).New York: ACM Press,2013:389-396.   \n42Leskovec J，Grobelnik M,Milic-Frayling N.Learning Sub-structures of Document Semantic Graphs for DocumentSummarization[EB/OL].http://www-cs.stanford. edu/people/jure/pubs/nlpspo-linkkdd04.pdf,2016-10-06.   \n43 Plaza L, Diaz A. Using Semantic Graphs and Word Sense Disambiguation Techniques to Improve Text Summarization[J]. Procesamiento Del Lenguaje Natural, 2011,47(0):   \n97-105.   \n44Miranda-Jiménez S, Gelbukh A, Sidorov G. Summarizing Conceptual Graphs for Automatic Summarization Task [C]//Conceptual Structures for STEM Research and Education (ICCS-ConceptStruct 2O13 Lecture Notes in Computer Science). Berlin: Springer,2013:245-253.   \n45 邓力,俞栋,谢磊.深度学习:方法及应用[M].北 京：机械工业出版社,2016:2-15.   \n46 Hinton G E, Osindero S,Teh Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18 (7):1527-1554.   \n47 孙志远,鲁成祥,史忠植,等.深度学习研究与进展[.计 算机科学,2016,(2):1-8.   \n48 Mo D.A survey on deep learning: one small step toward AI [EB/OL].http://www.cs.unm.edu/\\~pdevineni/papers/Mo. pdf,2016-12-03.   \n49 郭丽丽,丁世飞.深度学习研究进展.计算机科学,   \n2015,(5):28-33.   \n50 Liu Y, Zhong S H, Li W. Query-oriented multi-document summarization via unsupervised deep learning[C]// AAAI'12 Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence.Palo Alto: AAAI Press,2012: 1699-1705.   \n51 Zhong S H, Liu Y, Li B,et al. Query-oriented unsupervised multi-document summarization via deep learning model[J]. Expert Systems with Applications,2015,42(21): 8146-8155.   \n52Padmapriya G,Duraiswamy K.AN APPROACH FOR TEXT SUMMARIZATION USING DEEP LEARNING ALGORITHM[J]. Journal of Computer Science,2014,10 (1):1-9.   \n53 Padmapriya G, Duraiswamy K. Association of deep learning algorithm with fuzzy logic for multidocument text summarization[]. Journal of Theoretical & Applied Information Technology,2014,62(1):166-173.   \n54Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate[EB/OL]. http://arxiv.0rg/pdf/1409.0473v4,2017-01-21.   \n55Bahdanau D,Chorowski J, Serdyuk D,et al.End-to-End Attention-based Large Vocabulary Speech Recognition [EB/OL]. https://arxiv.0rg/pdf/1508.04395,2017-01-21.   \n56Venugopalan S,Rohrbach M, Donahue J,et al. Sequence to Sequence -- Video to Text[EB/OL]. https://arxiv.org/ pdf/1505.00487,2017-01-21.   \n57Rush A M, Chopra S,Weston J,et al. A Neural Attention Model for Abstractive Sentence Summarization[EB/OL]. https://arxiv.0rg/pdf/1509.00685,2017-01-21.   \n58 Chopra S,Auli M,Rush A M.Abstractive Sentence Summarization with Attentive Recurrent Neural Networks[C]// Proceedings of NAACL-HLT 2016.San Diego: Association for Computational Linguistics,2O16:93-98.   \n59 Abadi M, Barham P,Chen J,et al. TensorFlow: A system for large-scale machine learning[EB/OL].https://arxiv.org/ pdf/1605.08695v2,2017-01-21.",
    "context2": "(责任编辑：毛秀梅)"
  }
}