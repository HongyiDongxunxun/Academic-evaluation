{
  "original_filename": "full_429.md",
  "KNN 分类算法改进研究进展": "",
  "奉国和吴敬学": "",
  "华南师范大学经济与管理学院广州510006": {
    "context1": "(摘要）指出传统 KNN(k-nearest neighbor)算法的两大不足：一是计算开销大,分类效率低;二是在进行相似性度量和类别判断时,等同对待各特征项以及近邻样本，影响分类准确程度。针对第一点不足,提出三种改进策略,分别为：基于特征降维的改进、基于训练集的改进和基于近邻搜索方法的改进;针对第二点不足,提出两种改进策略,分别为：基于特征加权的改进和基于类别判断策略的改进。对每种改进策略中的代表方法进行介绍并加以评述。",
    "context2": "(关键词）KNN 分类特征降维 特征加权 训练集优化快速算法（分类号）G354"
  },
  "A Literature Review on the Improvement of KNN Algorithm": {
    "context1": "Feng GuoheWu Jingxue",
    "context2": "School of Economic & Management，South China Normal University，Guangzhou 510006 (Abstract）Thepaperpointsout thatthetradionalknearestneighbor(KNN）algorithmhastwoshortcomings，oneisitshighcomputationalomplexityandanotheritatitgivesequalimportancetoachfeatureitemsndneighborsamplesduringtheproesofsii laritymeasureandcategoryjudgment.According tothefirstshortcoming，threekindsof improvementstrategyareputforard，which arefeaturereduction，optimizationof rainingsetandimprovementofneighborsearchingmethod.Acordingtothesecondshortcoming, twokindsofimprovementstrategyareputforward，whicharefeatureweightingandsampleweighting.Representativemethodofeach strategy is also introduced and commented objectively.",
    "context3": "(Keywords）KNN categorization dimension reduction feature weightingtraining set optimizing fast KNN"
  },
  "1引言": "",
  "2　提高KNN 算法效率的改进方法": {
    "context1": "可用于分类的方法有：决策树、遗传算法、神经网络、朴素贝叶斯、支持向量机、基于投票的方法、Rocchio分类、KNN分类、最大熵等。KNN算法作为向量空间模型（VSM）下最好的分类算法之一，由Cover和Hart于1967年提出,基本思想是:在VSM表示下，分别计算待分类样本与训练样本的相似性,找出与待分类样本最相似的 $\\mathbf { k }$ 个近邻,根据这 $\\mathrm { k }$ 个近邻的类别确定待分类样本归属。但是,作为一种惰性学习方法，KNN算法直到有样本需要分类时才建立分类器,逐个计算与训练样本的相似程度，计算量大，分类效率低，另外存在易受样本空间密度影响、相似性度量以及类别判断不考虑权重等不足。针对这些不足，已有大量改进方法提出,对这些改进方法加以整理及评价可以为后续研究提供参考。",
    "context2": "传统KNN算法进行分类时需要逐一计算待分类样本与训练样本之间的相似度,造成巨大计算开销,针对这一问题可以从特征降维、优化训练集和改善近邻搜索方法三个方面进行改进，从而提高算法效率。"
  },
  "2.1基于特征降维的改进": {
    "context1": "为了降低计算开销,清理“噪声”数据,需要对样本进行特征降维，特征降维方法分为特征选择和特征抽取。特征选择是将原始特征集中区分能力较强的特征项提取出来作为新的特征集合；而特征抽取则是根据某一原则构造由原始特征集到新特征集的转换关系，将分散在大量原始特征中的分类信息尽量集中到新的少量特征中。二者的区别在于经特征选择生成的新特征集是原始特征集的子集，而经特征抽取生成的新特征集则不是原始特征集的子集。由于特征降维属",
    "context2": "KNN算法改进的外围思想,在其他分类算法中都会有所涉及，本文只做简单阐述。",
    "context3": "2.1.1特征选择特征选择过程主要是特征评估函数的选用，目前比较常用的特征评估函数包括：文档频率(DF）、互信息（MI）、信息增益(IG）、期望交叉熵（ECE）、几率比(OR）和 $\\mathrm { X } ^ { 2 }$ 统计（CHI)等。上述方法各有优缺点，但没有哪种方法在特征选择上占绝对优势：DF方法简单但一般不单独采用,而是与其他方法相结合;IG效果较好但计算复杂;MI则没有考虑每个类别中文本量对词条比率的影响;CHI比其他方法减少约 $50 \\%$ 的词汇,但统计开销大,对低频特征项区分效果不好;OR则在二元分类中表现出色，需根据具体情况选择合适的方法。Yang等分析和比较了DF、IG、MI和CHI等方法，发现IG和CHI方法比其他方法要好。代六玲等通过实验证明,在英文文本分类中表现良好的IG、MI和CHI方法在不加修正的情况下并不适合中文文本分类，并提出修正方法。",
    "context4": "智能优化算法为特征选择提供了良好的方法与工具,Raymer等利用遗传算法、李凯齐等利用改进的蚁群算法、Zhang等利用禁忌搜索算法进行特征选择并取得良好效果。利用智能优化算法进行特征选择能够基于整个类寻找最优特征项，具有较高精度，但这类方法通常需要反复迭代,训练集规模增加会造成计算开销急剧上升。此外,还有Bayes准则、类别特征域、低损降维、区分类别能力的高性能算法等新特征选择算法。",
    "context5": "2.1.2特征抽取常用的特征抽取方法包括:特征聚类、主成份分析(PCA）、潜在语义标引（LSI)和非负矩阵分解(NMF)等,其中特征聚类最为常用,PCA发展较为成熟,LSI可视为特殊的PCA,但对数据变化敏感且需要较大存储空间，不适用大规模文本分类，NMF对语义解释更为直观。",
    "context6": "改进算法主要有基于核的主成份分析（KernelPCA）[。Lee等提出其他解决非线性问题的方法;季铎等[提出消除潜在语义结构中不存在的共现特征的算法;郝占刚等[使用潜在语义索引初次降维，再利用遗传算法实现二次降维。",
    "context7": "特征抽取涉及到语义上的分析，而目前自然语言语义处理技术尚不发达,用特征抽取方法进行特征降维的效果并不显著。相比之下,特征选择选出的特征集合是原始特征集的子集，所以更易实现,方法也更加多样。"
  },
  "2.2基于训练集的改进": "",
  "KNN方法基于实例学习,分类知识存储于训练样": {
    "context1": "本中,对训练集进行优化不仅能减小计算开销，还可提高分类准确率。优化方法分为两类：一是对训练样本进行剪裁以减小训练集规模;二是在原训练集中选取或生成一些代表,形成新的小规模样本集代替原训练集。二者的区别在于通过前一类方法会使训练集损失部分样本,而后一类方法不减少训练集样本数量，只是对各类样本按一定规则进行组织。",
    "context2": "2.2.1训练集剪裁对训练集进行剪裁通常有两种思想：一种思想认为分类的错误率主要来自于类别边界的样本，如能将不同类别边界样本以适当方式进行筛选,则既可以减小样本数量又能提高识别准确率，其实质是使各类别边界更加清晰，代表方法有Editing方法[、MultiEdit方法[等。这种方法的主要作用在于降低分类错误率，虽然缩减了训练集，但程度较低，而随机划分和迭代运算则增大了工作量。",
    "context3": "另一种思想则认为训练集中靠近各个类别中心的大部分样本对分类决策用处不大,去掉这部分样本将大大减小计算开销并压缩存储量，其实质是仅保留各个类别边界样本并据此构造分类器，代表方法为压缩近邻法[,这种方法能够以准确率略微下降为代价,去掉训练集中大部分样本,但是将类中心区域样本全部去掉也会影响分类精度，同时该方法无法解决类偏斜问题。李荣陆[针对类偏斜问题提出基于密度的剪裁方法。",
    "context4": "2.2.2在训练集中生成代表在训练集中生成代表的通常做法是首先将各个类别样本聚类，然后用各种不同的方法对聚类形成的簇进行处理，以提高计算效率。文献 20]将各簇样本进行文本串接形成的向量作为该簇代表进行分类，此方法较简单,但分类准确率易受影响,对簇的取值依赖性高。文献[1]结合Roc-chio思想进行排类，能够在保证分类精度的基础上提高分类速度。文献 2]基于 CURE（Clustering UsingRepresentatives)算法对聚类方法进行改进,解决了聚类算法非球形和相似大小的问题，并能较好地处理样本中的孤立点，提高抗噪能力。",
    "context5": "与裁剪训练集相比,在训练集中生成代表的方法不删除训练样本,在保留绝大部分分类知识的同时，还能降低计算复杂度，对于分类准确率与样本规模正相关的KNN算法来说是一种有效改进。但这种方法的不足主要在于，查找近邻时在局部范围进行，可能忽略实际有价值的训练样本,同时要求对于训练集的处理具有较高精度，否则会大大提高错误率。"
  },
  "2.3基于近邻搜索方法的改进": {
    "context1": "KNN算法是一种惰性学习方法，它不事先建立分类模型,而是单纯保存所有训练样本直到有新样本需要进行分类时计算。由于对每个新样本都要计算它与所有训练样本之间的距离，导致KNN算法分类效率低下,并占用大量存储空间。因此,对KNN 算法的另一改进方向是改善近邻搜索方法以提高效率。",
    "context2": "2.3.1 KNN 模型法 KNN 模型算法(KNN Model)是Guo 等人提出的一种KNN改进算法,它在训练数据集上构建多个类似索引结构的KNN模型簇，弥补了分类新数据耗费大量时间的缺陷，提高了算法效率，同时由于每个模型簇的大小由KNNModel算法根据训练集各类数据在多维数据空间的分布情况自动形成，从而减少了对k值的依赖,也在一定程度上提高了分类的精度。但KNN模型属于非增量学习算法，从而限制了它在一些领域的应用,文献 [4]提出一个基于KNN 模型的增量学习算法，文献[25]则为增量KNN模型算法提出了一种模型簇修剪策略。",
    "context3": "2.3.2超球体搜索方法Zhang等提出将搜索空间限定于一个超球体的思想,缩短了寻找 $\\mathrm { k }$ 个近邻的时间,提高了算法效率,但是他们对超球的增长没有给出深入的研究，当使用于稀疏的大数据集合时，如果该半径增长没有一定的指导，则会导致试探次数显著增大。文献[7]通过对测试样本点生长步长进行采样,建立了BP神经网络模型，然后用该模型指导超球体的生长,缩小了算法搜索范围,有效地减少了超球体的试探生长次数，对处理稀疏数据集有明显的优越性。",
    "context4": "2.3.3结合智能优化算法的方法智能优化算法中的一些方法也可用于提高KNN算法查找近邻的效率。邓箴等利用模拟退火法改进KNN算法中寻找 $\\mathrm { k }$ 个近邻的过程并取得良好效果;李欢等利用粒子群优化方化法快速寻找 $\\mathrm { k }$ 个近邻，提高算法效率，但容易受到初始粒子选择的影响。"
  },
  "3 提高KNN算法准确率的改进方法": {
    "context1": "传统KNN算法进行相似性度量和类别判断时，等同对待各特征项以及近邻样本，影响分类准确率，可通过特征加权和优化类别判断策略进行改进,以提高分类精度。"
  },
  "3.1基于特征加权的改进": {
    "context1": "传统KNN算法在进行相似性度量时等同对待各维特征而影响分类精度，因此有必要计算各维特征对分类的贡献程度，即特征加权。",
    "context2": "3.1.1经典加权算法及其改进TF-DF（term fre-quency-inverse document frequency）是目前最常用的加权方法，但该方法并没有考虑特征项在类内和类间分布情况而影响的分类效果。由于文本分类中训练样本的类别标号提前可知，这部分先验知识可以被充分利用，因此对传统TF-IDF方法的改进主要集中在利用这些先验知识将无指导的特征加权方法转变为有指导的特征加权方法以提高分类精度，主要包括：",
    "context3": "·用特征选择中的评估函数代替IDF部分。在进行特征选择过程中通常会充分利用训练集中已分类样本的类别信息来构建评估函数，因此可以考虑将这些评估函数用于特征加权。陆玉昌等分析了文档频度等8种特征选择算法，并用其替换IDF部分进行分类,发现在特征选择中表现最差的互信息方法却在特征加权中表现最好，并对此进行了分析。",
    "context4": "·根据TF-DF思想构造新的加权函数。由于传统 TF-DF方法的不足主要集中在IDF部分,因而在构造新的加权函数时通常保留TF部分,仅对IDF部分进行修改。如刘海峰等将特征项在文档中的位置信息引入加权公式;尚文倩等基于基尼指数改进加权函数;胡清华等基于可变精度粗糙集理论改进加权函数，并在各自的仿真实验中证实其优于传统方法。",
    "context5": "3.1.2结合分类器的加权方法这种方法将特征加权与分类器相结合，根据分类器对样本的评分或分类精度变化为特征项赋予权值。如Mladeni等结合线性SVM和感知机等线性分类器进行加权,将分类器在区分正例和反例样本时为不同文档所打的分数作为对每个特征进行加权的依据。王晓晔等利用BP神经网络分类器，通过删除特征节点前后分类器精度的差值确定该特征项权值。结合分类器的加权方法能够有效提高特征加权准确率与分类精度，但这种方法通常计算量较大，处理大样本集耗时过长。"
  },
  "3.2基于类别判断策略的改进": {
    "context1": "传统KNN算法的另一不足是在根据 $\\mathbf { k }$ 个近邻判断待分类样本所属类别时，对 $\\mathbf { k }$ 个近邻进行简单的算术平均,即各个近邻被同等对待。改进的KNN算法在进行类别判断时，往往会对 $\\mathbf { k }$ 个近邻赋予不同的权值以体现其贡献度高低。通常做法是将距离或相似度转换为权值，较近的近邻被赋予较大权重,随着距离的增加,权重逐渐减小,但衰减不能过快,且保证权重不为0。3.2.1基于密度改进类别判断策略当训练集样本分布存在类偏斜问题时，由于高密度类别的样本比低密度类别的样本更容易获得，可能导致本应属于低密度类别的待分类样本由于 $\\mathbf { k }$ 个近邻中高密度类别样本数量占优而被误分。这种情况下，如果未对训练集进行基于密度的剪裁或剪裁之后仍存在密度不均衡现象，则需要在类别决策函数中引入基于密度的权重系数,使高密度类别样本具有低权重而低密度类别样本具有高权重，文献B5]介绍了这样一个基于密度的权重系数。",
    "context2": "3.2.2基于模糊集改进类别判断策略Keller等将模糊集理论引入KNN类别判断中提出模糊KNN方法，该方法并不单纯指定未知样本属于哪一类别，而是通过隶属度函数给出未知样本对于某一类别的隶属程度，这更符合实际情况。利用模糊集理论进行类别判断能够充分发挥近邻样本对分类的作用，削弱训练样本分布不均匀对分类性能的影响，提高分类精度，并且在一定程度上降低对k值的敏感性。但隶属度函数的选择与设置至关重要,如何得到更加适合KNN分类的隶属度函数还有待研究。"
  },
  "4结语": {
    "context1": "特征降维和特征加权是针对向量空间模型(VSM)的改进与优化的，普遍适用于多种文本分类算法，同时也是文本分类及其他模式识别应用的基础技术,受到人们的广泛关注,发展也比较成熟。优化训练集通常可以实现提高算法效率和准确率的双重目的,改进效果明显,也是一个人们比较热衷的研究方向,且成果不断丰富。使用快速搜索算法与优化类别判断策略也已有了一些成功经验。至于如何确定合适的 $\\mathbf { k }$ 值,目前通常的做法是通过实验绘制“k值-准确率\"曲线选择峰值处的 $\\mathrm { k }$ 值。通过对KNN算法改进研究进展的总结可以看出,与相关领域的算法相结合来弥补自身不足已成为KNN算法改进的一个重要方向，其中与智能优化算法的结合最为突出。如遗传算法与神经网络已经广泛应用于特征选择、特征加权以及训练集优化等方面,蚁群算法、粒子群优化等较新的算法也正逐渐被引入KNN 算法的改进之中。如何进一步完善这些算法并将其更好地应用于KNN算法之中,将是后续研究的重点。"
  },
  "参考文献:": {
    "context1": "[1]奉国和.自动文本分类技术研究[].情报杂志,2007,26(12)：108 -111.  \n[2] Cover T M,Hart P E.Nearest neighbor pattern clasification [].IEEE Transactions on Information Theory,1967,13(1):21 -27.  \n[3] Yang Yiming,Pedersen JO.A comparative study on feature selec-tion in text categorization [C //Proceedings of the Fourteenth Inter-national Conference on Machine Learning（ICML'97)．San Fran-cisco: Morgan Kaufman Publishers,1997:412 -420.  \n[4] 代六玲,黄河燕,陈肇雄.中文文本分类中特征抽取方法的比较研究[.中文信息学报,2004,18(1):26-32.  \n[5] Raymer M L,Punch W E,Goodman E D,et al.Dimensionalityreduction using genetic algorithms [l．IEEE Transactions on Evo-lutionary Computation,2000,4(2) :164 -171.  \n[6]李凯齐，习兴春，曹建军,等.基于改进蚁群算法的高精度文本特征选择方法.解放军理工大学学报（自然科学版）,2010，11(6) :634 -639.  \n[7] Zhang Hongbin,Sun Guangyu.Feature selection using tabu searchmethod [．Patterm Recognition,2002,35(3):701-711.  \n[8]闫鹏,郑雪峰,李明祥,等.二值文本分类中基于 Bayes 推理的特征选择方法.计算机科学 $, 2 0 0 8 , 3 5 ( 7 ) : 1 7 3 - 1 7 6 .$   \n[9]赵世奇,张宇,刘挺.基于类别特征域的文本分类特征选择方法[.中文信息学报,2005,19(6):21-27.  \n[0]宋枫溪.统计模式识别中的维数削减与低损降维[.计算机学报,2005,28(11):1915 -1922.  \n[]徐燕.基于区分类别能力的高性能特征选择方法[].软件学报,2008,19(1):82 -89.  \n[12] Jain A K,Duin R W,Mao Jianchang. Statistical patern recogni-tion:A review []．IEEE Transactions on Patern Analysis andMachine Intelligence,2000,22(1) :4-37.  \n[13]Lee JA,Verleysen M. Quality assessment of dimensionality reduc-tion:Rank-based criteria []．Neurocomputing,2009,72（9)：1431 -1443.  \n[4]季铎,郑伟,蔡东风.潜在语义索引中特征优化技术的研究.中文信息学报,2009,23(2):69-76.  \n[5]郝占刚,王正欧.基于潜在语义索引和遗传算法的文本特征提取方法.情报科学 $, 2 0 0 6 , 2 4 ( 1 ) : 1 0 4 - 1 0 7$   \n[6]Wilson DL.Asymptotic properties of nearest neighbor rules usingedited data []．IEEE Transactions on Systems，Manand Cyber-netics,1972,SMC-2(3):408 -421.  \n[7]DevijverP，KitePaenrecognitionAstatisalpM．Englewood Clis: Prentice Hall,1982.  \n[8] Hart PE. The condensed nearest neighbor rule $\\mathbb { I }$ . IEEE Transac-tions on Information Theory,1968,14(3) :515-516.  \n[9]李荣陆,胡运发.基于密度的 KNN文本分类器训练样本裁剪方法[.计算机研究与发展,2004,41(4):539-545.  \n[0] 张孝飞,黄河燕.一种采用聚类技术改进的 KNN文本分类方法.模式识别与人工智能,2009,22(6):936-940.  \n[1] 吴春颖,王士同.一种改进的 KNN Web 文本分类方法[ .计算机应用研究,2008,25(11):3275-3277.  \n[2]王晓晔,王正欧.K-最近邻分类技术的改进算法[.电子与信息学报,2005,27(3):487-491.  \n[3]Guo Gongde,Wang Hui,Bell D,et al.Using KNN model for au-tomatic text categorization [l．Soft Computing-A Fusion of Foun-dations，Methodologies and Application，2006，10(5)：423-430.",
    "context2": "每页10条记录，每个关键词的检索效果如表1所示：",
    "context3": "表1改进后PageRank 算法和传统PageRank算法查询对比",
    "context4": "<table><tr><td rowspan=2 colspan=1>关键词</td><td rowspan=1 colspan=2>改进后PageRank算法</td><td rowspan=1 colspan=2>传统PageRank 算法</td><td rowspan=2 colspan=1>查准提升率（%)</td></tr><tr><td rowspan=1 colspan=1>相关记录数（条）</td><td rowspan=1 colspan=1>准确率(%)</td><td rowspan=1 colspan=1>相关记录数（条）</td><td rowspan=1 colspan=1>准确率(%)</td></tr><tr><td rowspan=1 colspan=1>q1</td><td rowspan=1 colspan=1>76</td><td rowspan=1 colspan=1>76</td><td rowspan=1 colspan=1>55</td><td rowspan=1 colspan=1>55</td><td rowspan=1 colspan=1>21</td></tr><tr><td rowspan=1 colspan=1>q2</td><td rowspan=1 colspan=1>74</td><td rowspan=1 colspan=1>74</td><td rowspan=1 colspan=1>67</td><td rowspan=1 colspan=1>67</td><td rowspan=1 colspan=1>7</td></tr><tr><td rowspan=1 colspan=1>q3</td><td rowspan=1 colspan=1>71</td><td rowspan=1 colspan=1>71</td><td rowspan=1 colspan=1>63</td><td rowspan=1 colspan=1>63</td><td rowspan=1 colspan=1>8</td></tr><tr><td rowspan=1 colspan=1>q4</td><td rowspan=1 colspan=1>83</td><td rowspan=1 colspan=1>88</td><td rowspan=1 colspan=1>71</td><td rowspan=1 colspan=1>71</td><td rowspan=1 colspan=1>12</td></tr><tr><td rowspan=1 colspan=1>q5</td><td rowspan=1 colspan=1>79</td><td rowspan=1 colspan=1>79</td><td rowspan=1 colspan=1>64</td><td rowspan=1 colspan=1>64</td><td rowspan=1 colspan=1>15</td></tr><tr><td rowspan=1 colspan=1>q6</td><td rowspan=1 colspan=1>68</td><td rowspan=1 colspan=1>68</td><td rowspan=1 colspan=1>59</td><td rowspan=1 colspan=1>59</td><td rowspan=1 colspan=1>9</td></tr><tr><td rowspan=1 colspan=1>q7</td><td rowspan=1 colspan=1>73</td><td rowspan=1 colspan=1>73</td><td rowspan=1 colspan=1>66</td><td rowspan=1 colspan=1>68</td><td rowspan=1 colspan=1>7</td></tr><tr><td rowspan=1 colspan=1>q8</td><td rowspan=1 colspan=1>79</td><td rowspan=1 colspan=1>79</td><td rowspan=1 colspan=1>62</td><td rowspan=1 colspan=1>62</td><td rowspan=1 colspan=1>17</td></tr><tr><td rowspan=1 colspan=1>q9</td><td rowspan=1 colspan=1>77</td><td rowspan=1 colspan=1>77</td><td rowspan=1 colspan=1>68</td><td rowspan=1 colspan=1>68</td><td rowspan=1 colspan=1>9</td></tr><tr><td rowspan=1 colspan=1>q10</td><td rowspan=1 colspan=1>81</td><td rowspan=1 colspan=1>81</td><td rowspan=1 colspan=1>66</td><td rowspan=1 colspan=1>66</td><td rowspan=1 colspan=1>15</td></tr></table>",
    "context5": "对表1数据进行分析可知,改进算法的准确率比传统PageRank算法有明显提高,分别在 $7 \\%$ 至 $21 \\%$ 不等，虽然准确率的提升具有一定的波动性，其原因在于不同的检索关键词表达方式的种类多少不一，如果某一职位存在较多称谓的话,那么使用改进后算法的准确率提升效果会比较明显。"
  },
  "5结语": {
    "context1": "好的网页排序算法应该使那些与用户查询主题密切相关的网页尽量排在前面。由于经典的PageRank算法存在主题漂移现象，致使大量与用户查询主题无关的网页排在了查询结果的前面。本文提出的基于特殊主题的PageRank改进算法,从研究用户查询行为入手，根据实际情况进行改进来弥补算法中的缺陷，为解决主题漂移和网页时效性问题建立排序模型，计算出科学合理的PageRank值,同时可以解决现实中的网站作弊行为。该算法不需要额外文本信息，也不增加算法时空复杂度，能极大地减少主题漂移现象，并使用户满意的有效网页能更多地排在搜索结果集的靠前位置，易于用户获得所需要的信息，从而提高查询质量。"
  },
  "参考文献：": {
    "context1": "[]Jon M K．Authoritative sources in a hyperlinked envoriment [].Journal of the ACM,1999,46(5):604-632.  \n[]Brin S,Page L.The anatomy of a large-scale hypertextual Websearch engine EB/O1]．[2012-06-27] .http://www.db.stan-ford.edu/\\~backrub/google.html.  \nβ]曹军.Google的PageRank 技术剖析[．情报杂志,2002(10)：15-18.  \nCai Deng，Yu Shipeng，Wen Jirong，et al．VIPS:A vision basedpage segmentation algorithm[R]．Microsoft Technical Report，MSR2-TR2-20032-79，2003.  \n]刘军．基于Ontology的面向主题的网络信息采集算法[]．图书情报工作， $^ { 2 0 0 6 , 5 0 \\left( 5 \\right) : 7 8 - 8 2 }$   \n邵晶晶．基于PageRank 排序算法改进的若干研究[D].武汉：华中师范大学，2009.  \n王钟斐,王彪．基于锚文本相似度的PageRank 改进算法．计算机工程,2010(12):258-260.  \n宋聚平,王永成,尹中航,等．对网页PageRank 算法的改进[.上海交通大学学报， $2 0 0 3 , 3 7 \\mathrm { ( \\ 3 ) } : 3 9 7 - 4 0 0 .$",
    "context2": "(作者简介）王建雄,男,1970年生，高级实验师，技术部主任，发表论文20余篇。"
  },
  "(上接第100页)": {
    "context1": "[24]郭躬德,黄杰,陈黎飞.基于KNN模型的增量学习算法[.模式识别与人工智能，2010，23(5)：701-707.  \n[5]黄杰,郭躬德,陈黎飞.增量KNN模型的修剪策略研究[.小型微型计算机系统,2011,5(5):845-849.  \n[6] Zhang Bin,Srihari S N.Fast k-nearest neighbor clasification usingcluster-based trees $\\mathbb { I }$ .IEEE Transactions on Pattern Analysis andMachine Intelligence,2004,26(4) :525-528.  \n[7] 余小鹏,周德翼.一种自适应k-最近邻算法的研究[].计算机应用研究,2006(2)：70-72.  \n[28]邓箴,包宏.用模拟退火改进的KNN分类算法[.计算机与应用化学,2010,27(3):303-307.  \n[9]李欢,焦建民.简化的粒子群优化快速KNN分类算法.计算机工程与应用 $, 2 0 0 8 , 4 4 \\bigl ( 3 2 \\bigr ) : 5 7 - 5 9 .$   \n[0] 陆玉昌,鲁明羽,李凡,等.向量空间法中单词权重函数的分析和构造[.计算机研究与发展,2002,39(10)：1205－1210.  \nβ1]刘海峰,姚泽清,汪泽焱,等.基于位置的文本特征加权方法研究[.微电子学与计算机,2009,26(2):188-192.  \nB2] 尚文倩，黄厚宽,刘玉玲,等.文本分类中基于基尼指数的特征选择算法研究.计算机研究与发展,2006,43(10):1688－1694.  \n[3] 胡清华,谢宗霞,于达仁.基于粗糙集加权的文本分类方法研究[.情报学报,2005,24(1):59 -63.  \nB4]Mladeni $\\acute { \\mathrm { ~ c ~ D ~ } }$ ,Brank J,Grobelnik M,et al.Feature selection usinglinear classier weights: Interaction with classication models [C] $/ /$ Proceedings of the $2 7 \\mathrm { t h }$ Annual International ACM SIGIR Confer-ence on Research and Development in Information Retrieval.NewYork:ACM Press,2004:234-241.  \nβ5] 刘海峰,陈琦,刘守生,等.一种基于数据偏斜的改进KNN文本分类[.微电子学与计算机,2010,27(3):51-53,58.  \nB6] Keller JM,Gray MR,Givens JA. A fuzzy k-nearest neighbor al-gorithm [．IEEE Transactions on Systems，Man，and Cybernet-ics,1985,SMC -15(4):580-585.",
    "context2": "(作者简介）奉国和，男，1971年生，副教授，硕士生导师，发表论文40余篇。吴敬学，男，1986年生，硕士研究生，发表论文3篇。"
  }
}