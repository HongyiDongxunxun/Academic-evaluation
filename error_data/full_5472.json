{
  "original_filename": "full_5472.md",
  "情报学视角下的预训练语言模型研究进展\\*": "",
  "■胡昊天1.2 邓三鸿1.² 王东波2³ 沈思²4　沈健威5": {
    "context1": "南京大学信息管理学院南京210023  \n² 江苏省数据工程与知识服务重点实验室 南京210023  \n南京农业大学信息管理学院 南京210095  \n南京理工大学经济管理学院 南京210094  \n5 江苏省质量和标准化研究院 南京210029",
    "context2": "摘要：[目的/意义]对预训练语言模型在情报学与情报工作中的相关研究进行系统性的梳理与分析，为后续预训练模型与情报研究的融合提供借鉴。[方法／过程]首先，简述预训练模型的基本原理与发展历程，汇总情报研究中应用较为广泛的预训练模型。其次，宏观上分析预训练模型在国内外情报研究中的热点方向，微观上从情报组织、情报检索、情报挖掘等方面调研预训练模型相关研究成果，并细致分析归纳预训练模型的应用方式、改进策略与性能表现。最后，从预训练模型的语料、训练、评价、应用等方面总结当前预训练模型在情报学科中面临的机遇与挑战，展望未来发展。[结果/结论]当前BERT及其改型在情报处理中应用最广、表现最优。结合神经网络与微调的范式被用于各研究场景，尤其是领域信息抽取与文本分类任务。继续预训练、外部知识增强、架构优化等策略可进一步提升性能。如何平衡训练语料的规模与质量、提升模型易用性与安全性、高准度与多维度评价模型真实能力、加速学科知识挖掘工具落地应是未来考虑的关键问题。",
    "context3": "关键词：情报学　情报工作 预训练语言模型　自然语言处理 PLM",
    "context4": "分类号：G250",
    "context5": "DOI: 10.13266/j.issn.0252-3116.2024.03.012",
    "context6": "引用本文：胡昊天，邓三鸿，王东波，等.情报学视角下的预训练语言模型研究进展[J].图书情报工作，2024,68(3):130- 150.(Citation: Hu Haotian,Deng Sanhong,Wang Dongbo,etal.Advances in Pre-trainedLanguage Models from thePerspective of Information Science[J].Library and Information Service,2024, 68(3): 130-150.)"
  },
  "1引言 /Introduction": {
    "context1": "深度学习、人工智能等现代计算机信息技术的发展，对情报学与情报工作产生了巨大影响[1]。作为一门研究情报获取、组织、检索、分析、评估的学科，情报学的学科发展与情报工作的效能提升需要情报技术与方法的创新[2]，这也是促进学科交叉融合[3],提升情报学的跨学科优势，完善“大情报观”下情报学理论方法[4的关键驱动力。",
    "context2": "情报学领域的自然语言处理（natural languageprocessing,NLP）由自然语言理解（natural languageunderstanding,NLU）与自然语言生成（natural lan-guage generation,NLG）组成，研究内容主要包括面向情报处理、情报分析与情报检索的序列标注、自动分类、文本生成等各类有监督任务。以条件随机场（conditional random field,CRF）、支持向量机（supportvectormachine,SVM）为代表的经典机器学习技术方法为情报处理引入了人工特征工程结合语料库的研究范式。以双向长短期记忆神经网络（long short-termmemory,LSTM）和卷积神经网络（convolutionalneuralnetwork,CNN）为代表的深度学习技术使得情报挖掘方法转向基于大规模有监督数据集的自动特征提取。近年来，以BERT、GPT为代表的预训练语言模型基于自监督特征抽取与迁移学习技术，再度提升了文本表示与知识挖掘能力，在情报处理与分析中得到了广泛的应用。",
    "context3": "预训练语言模型（pre-trained language models,PLM）是指在大规模无监督语料上训练的神经网络架构，例如LSTM、Transformer等，学习了通用语言表示。在开展下游任务时，可作为初始化权重直接加载进行领域微调（fine-tuning），从而无需再次从头训练模型。通用PLM由于缺乏情报学领域知识,因此通常需经过领域化改造从而更适用于情报学研究。与经典机器学习和深度学习模型类似，PLM并非出自情报学，而是作为一种情报处理技术被引入情报实践。通过梳理哪些PLM在情报学研究中得到了何种应用，预训练技术在情报学实践中经过了哪些领域化改造，推动了哪些领域情报工作的开展[3],取得了哪些研究与应用成果，对于加速自然语言处理研究，探究情报技术的改造与创新，构建具有情报研究特色的分析方法学[5]具有重要意义。",
    "context4": "当前有关PLM的综述性文献大多关注技术原理[、发展历程[7、改进方式[8]等，但是鲜有对预训练技术在情报学学科及情报分析流程中应用的文献进行系统梳理及整体回顾。因此，本文立足PLM在情报学领域研究中的应用进行归纳总结，重点分析其在情报学各研究方向下的领域化改造与应用现状，并提炼当前存在的缺陷与不足，寻求情报学研究与预训练技术间新的结合方法与发展路径。本研究的整体框架见图1，主要贡献如下： $\\textcircled{1}$ 探究PLM在情报学与情报工作中的主要应用点； $\\textcircled{2}$ 按照情报学不同领域方向分别梳理PLM的领域化研究进展；$\\textcircled{3}$ 针对当前PLM在情报分析与挖掘中可改进的问题展望未来研究重点与趋势。",
    "context5": "![](images/3383cb4972645ed0152c29dd3734a53065dcbf351ee040eef754c3b5ae794f8f.jpg)  \n图1本文总体研究框架  \nFigure 1 Overall research framework"
  },
  "2文献检索 /Literature retrieval": {
    "context1": "笔者首先在中外文文献数据库中检索情报学领域PLM相关文献，用于支撑后续文献梳理与分析。中文文献在中国知网中检索，选取篇关摘字段检索，刊物为图书馆、情报与文献学CSSCI来源期刊（2021—2022），时间范围为2018年2月至2023年3月。具体检索式为：TKA=（‘预训练模型’ $^ +$ ‘预训练语言模型 $^ +$ “预训练技术' $^ +$ ‘ELMo' $^ +$ ‘BERT'' $^ +$ ‘XLNET'$^ +$ ‘GPT’ $^ +$ ‘T5’ $^ +$ ‘ALBERT’ $^ +$ ‘RoBERTa'$^ +$ ‘BART’ $^ +$ ‘ERNIE’ $^ +$ ‘BLOOM’ $^ +$ ‘OPT’ $^ +$ ‘TO’ $^ +$ ‘GLM’ $^ +$ ‘PALM’ $^ +$ ‘FLAN’ $^ +$ ‘LLa-MA’） AND $\\mathrm { L Y = } ($ ‘大学图书馆学报’ $^ +$ ‘档案学通讯’ $^ +$ ‘档案学研究’ $^ +$ ‘国家图书馆学刊’ $^ +$ ‘情报科学’ $^ +$ ‘情报理论与实践” $^ +$ ‘情报学报’ $^ +$ ‘情报杂志 $+$ “情报资料工作' $^ +$ “数据分析与知识发现\" $^ +$ ‘图书馆建设’ $^ +$ ‘图书馆论坛’ $^ +$ ‘图书馆学研究’ $^ +$ ‘图书馆杂志' $^ +$ ‘图书情报工作' $^ +$ ‘图书情报知识' $^ +$ ‘图书与情报' $^ +$ ‘现代情报' $^ +$ ‘信息资源管理学报' $^ +$ ‘中国图书馆学报’）。",
    "context2": "英文文献在WebofScience 核心合集中检索，选取标题和摘要字段，刊物为图书情报领域SSCI来源期刊,学科类别为“information science& library science”时间范围与中文一致。由于检索式太长，此处为精简后的检索式：( ${ \\mathrm { A B } } { = } ($ ‘pre-train\\* model’ OR‘pre-train\\*language model’ OR‘pre-train\\* technology’OR‘BERT'OR etc.) OR TI=(‘pre-train\\* model’ OR‘pre-train\\*language model’ OR‘pre-train\\* technology’OR‘BERT'OR etc.)) AND (WC $=$ ‘information science & library sci-ence’ OR SO=（‘INTERNATIONAL JOURNAL OF IN-FORMATIONMANAGEMENT’OR‘JOURNALOFSTRATEGIC INFORMATION SYSTEMS’ OR etc.))。经过筛选，最终保留CSSCI索引文献126篇、WOS核心合集索引文献104篇用于分析。"
  },
  "预训练模型概述 /Overview of pretrained models": "",
  "3.1预训练模型发展进程": {
    "context1": "在早期情报分析与知识挖掘过程中，通常采用经典概率模型建模语言，例如N元语法模型（N-gram）。该方法理论基础坚实，操作简单易行，但是容易出现数据稀疏、难以建模长距离关系等问题。随着深度学习技术在情报处理流程中的不断应用，以Word2Vec[10]、GloVe[11]、FastText[12]为代表的静态词嵌入技术通过训练低维稠密的词向量提升了文本表示水平，但是对解决一词多义和捕获深层文本依赖关系的能力仍然较弱。ELMo[13]模型的出现打开了动态词嵌入技术的大门，该模型能够针对上下文语境动态调整词义表示，从而极大增强文本表示的能力。同样基于动态嵌入的GPT[14]和BERT[15]模型则分别基于 Trans-former的解码器与编码器进行深层文本表示，并通过自监督任务学习词汇与句子间依赖关系。自此，基于Transformer架构的动态嵌人PLM成为语言建模和文本表示的主流。PLM的发展过程见图2。需要说明的是，本文所述的预训练模型（PLM）均为采用动态嵌人技术的深度文本表示模型。",
    "context2": "![](images/00372a867a9aa1acf537bf3f93a10b4b925e5a6de5fc1e23b981ab303e52c4f2.jpg)  \n图2预训练模型发展历程  \nFigure 2 Development of the pre-trained models",
    "context3": "PLM根据建模思想的不同，主要可以分为3类。第一类是以GPT[4 为代表的自回归模型。由于采用Transformer的解码器架构，本质上为单向语言模型，仅通过上文信息预测下一个词汇，因此在生成式任务中表现优异。但这也致使其在文本表示中缺失下文信息，因此在NLU任务中应用较少。GPT系列模型[16-17]促使了“提示一生成补全（prompt-completion）”范式和语境学习（in-context learning）的兴起。近期在通用自动问答领域爆火的ChatGPT模型同样基于这一范式训练而来。",
    "context4": "第二类是以BERT[15]为代表的自编码模型，同时也是当前应用最为广泛、改型最多的PLM。由于通过双向Transformer编码器架构和掩码语言模型实现了两个方向文本信息的同时获取，在各类NLU任务上表现出众。但掩码机制的引入导致预训练和微调阶段数据不匹配，一定程度上影响了结果预测。此外，自编码模型在NLG上的天然劣势也导致BERT很难完成自动摘要、机器翻译等任务。BERT极大地推广了“预训练一微调”[13]研究范式，基于BERT的衍生模型当前仍然是情报学NLP研究中的主流选择。",
    "context5": "第三类是以XLNet[18]为代表的排序语言模型。此类模型融合了上述两类模型的优势，通过单向语言模型避免了预训练过程中人为掩码的引入，保证了预训练与微调两阶段数据的一致性，又通过对输入序列中词汇的随机排序使得单向语言模型也可以等效学习到双向语境信息。该模型还使用了Transformer-XL机制来建模更长的文本序列。排序语言模型的引入使得XLNet在多项NLU任务上超越了GPT和BERT的表现。",
    "context6": "面向情报处理与分析流程，本文提出情报学视角下的预训练模型分类体系，将上述3类PLM建模思想映射至具体信息资源管理流程： $\\textcircled{1}$ 以BERT为代表的自编码模型具备出色的自然语言理解能力，主要面向情报组织与情报检索流程。 $\\textcircled{2}$ 以GPT为代表的自回归模型因其独特的文本生成能力，主要面向自动摘要、问答等情报服务流程。 $\\textcircled{3}$ 以XLNet为代表的 PLM由于兼具自然语言理解与生成能力，因此可以同时覆盖上述流程。"
  },
  "3.2主流预训练模型汇总": {
    "context1": "BERT一经推出就被广泛应用于各领域下游研究中。同时，研究人员也从更改预训练任务、融入外部",
    "context2": "知识、重构网络框架、压缩模型参数等多个方面对BERT进行了改进，催生出了一大批性能更佳、适用",
    "context3": "于不同领域的PLM。在情报研究中应用较为主流的PLM如表1所示："
  },
  "表1情报研究中主要应用的预训练模型": {
    "context1": "Table1 Main pre-trained models in information science research",
    "context2": "<table><tr><td rowspan=1 colspan=5>模型名称   模型架构 预训练任务           数据集</td><td rowspan=1 colspan=4>特点                         适用任务</td></tr><tr><td rowspan=2 colspan=4>ELMo    Bi-LSTM    BiLMGPT    Transformer    LM</td><td rowspan=1 colspan=1>WMT news crawl data</td><td rowspan=18 colspan=4>首次提出动态文本表示，结合语境建模语义关系判断、自动问答、命多义词，引人预训练+微调方式       名实体识别、情感分析首次引入Transformer，采用从左向右的生成式问答、自动摘要、相似单向语言模型，擅长NLG任务        度计算、机器翻译、文本分类同时实现深度双向文本表示，能够捕捉自动问答、自然语言推理、文长距离语义依赖，催生大批BERT改型本分类、命名实体识别融合GPT和 BERT优点，既能建模双  阅读理解、自动问答、自然语向语言，又避免人为噪声，捕获更长的言推理、文本分类、文档排序语义依赖使用动态掩码机制，更大mini batch,   自动问答、自然语言推理、文更多训练数据，全面超越BERT表现   本分类、命名实体识别缩小嵌入层维度，引入跨层参数共享，  自动问答、阅读理解、文本分显著减少参数量并维持性能           类、命名实体识别基于生成器和判别器架构，引入更高效自然语言推理、自动问答、相的替换词检测任务，低成本高性能     似性预测、情感分析提出掩码矩阵，共享双向、单向和 seq2seq自然语言推理、机器翻译、对模型参数，实现统一的NLU与NLG    话生成、生成式问答</td></tr><tr><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>LM</td><td rowspan=1 colspan=1>BooksCorpus</td></tr><tr><td rowspan=2 colspan=2>BERT</td><td rowspan=1 colspan=1>Decoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM+NSP</td><td rowspan=2 colspan=1>TransformerMLM+NSP BooksCorpus+English Wikipedia</td></tr><tr><td rowspan=3 colspan=2>XLNet</td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=2 colspan=1>Transformer-</td><td rowspan=2 colspan=1>PLM</td><td rowspan=2 colspan=1>BooksCorpus+English</td></tr><tr><td rowspan=1 colspan=2>融合GF</td></tr><tr><td rowspan=3 colspan=2>RoBERTa</td><td rowspan=2 colspan=1>XL</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Wikipedia+Giga5+ClueWeb+</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Common Crawl</td><td rowspan=1 colspan=1>语</td></tr><tr><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM</td><td rowspan=1 colspan=1>CC-News+BooksCorpus+English</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Wikipedia+OpenWebText+Stories</td></tr><tr><td rowspan=1 colspan=2>ALBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM+SOP</td><td rowspan=1 colspan=1>MLM+SOP BooksCorpus+English Wikipedia</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=2>ELECTRA</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM+RTD</td><td rowspan=1 colspan=1>BooksCorpus+English</td></tr><tr><td rowspan=2 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Wikipedia+Giga5+ClueWeb+</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Common Crawl</td></tr><tr><td rowspan=2 colspan=2>UniLM</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>LM+MLM+</td><td rowspan=1 colspan=1>Transformer LM+MLM+ BooksCorpus+English Wikipedia</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>NSP</td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=2>ERNIE-Baidu</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM+DLM</td><td rowspan=1 colspan=1>Chinese Wikepedia+Baidu</td><td rowspan=2 colspan=3>新增短语和实体层面的掩码建模长语义依赖，用多源语料提升语义建模能力</td><td rowspan=2 colspan=1>中文场景自然语言推理、语义相似度、情感分析、自动问答</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Baike+Baidu news+Baidu Tieba</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>SciBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM+NSP</td><td rowspan=1 colspan=1>rMLM+NSP Full text paper of Semantic</td><td rowspan=2 colspan=3>在自然科学领域本文上NLU表现优于BERT</td><td rowspan=2 colspan=1>自然科学领域命名实体识别、关系抽取、文本分类、依存</td></tr><tr><td rowspan=2 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Scholar</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3></td><td rowspan=1 colspan=1>解析</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>SsciBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM</td><td rowspan=1 colspan=1>Abstracts and titles of SSCI</td><td rowspan=1 colspan=3>在人文社科领域本文上NLU表现优于</td><td rowspan=1 colspan=1>人文社科领域文本分类、篇章</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>papers</td><td rowspan=1 colspan=3>BERT</td><td rowspan=1 colspan=1>结构识别、命名实体识别</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>CSSBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM</td><td rowspan=1 colspan=1>full text paper in CSSE</td><td rowspan=1 colspan=3>在中文人文社科领域本文上NLU 表现</td><td rowspan=1 colspan=1>中文语境下人文社科领域学科</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3>优于BERT</td><td rowspan=1 colspan=1>分类、命名实体识别</td></tr><tr><td rowspan=1 colspan=2>SikuGPT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>LM</td><td rowspan=1 colspan=1>繁体文渊阁《四库全书》</td><td rowspan=2 colspan=3>在数字人文研究中的古文生成式任务中表现优异</td><td rowspan=2 colspan=1>古代汉语场景机器翻译、文本生成、文本分类</td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>Decoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td></tr><tr><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>SikuBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM</td><td rowspan=1 colspan=1>繁体文渊阁《四库全书》</td><td rowspan=1 colspan=3>在数字人文研究中的古文NLP任务中</td><td rowspan=2 colspan=1>古代汉语场景文本分类、词汇处理、断句标点</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3>表现优异</td></tr><tr><td rowspan=1 colspan=2>GuwenBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM</td><td rowspan=1 colspan=1>简体殆知阁古代文献</td><td rowspan=1 colspan=3>适合古文NLP 中标注语料不足的小数</td><td rowspan=2 colspan=1>古代汉语场景命名实体识别、断句标点</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3>据集</td></tr><tr><td rowspan=1 colspan=2>BioBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM</td><td rowspan=1 colspan=1>PubMed abstract+PMC</td><td rowspan=1 colspan=3>在生物医学领域文本挖掘任务中表现优</td><td rowspan=2 colspan=1>生物医学领域命名实体识别、关系抽取、自动问答</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=3>于BERT</td></tr><tr><td rowspan=1 colspan=2>PubMedBERT</td><td rowspan=1 colspan=1>Transformer</td><td rowspan=1 colspan=1>MLM+NSP</td><td rowspan=1 colspan=4>从头预训练BERT,避免通用PLM噪声，</td><td rowspan=1 colspan=1>生物医学领域实体识别、关系</td></tr><tr><td rowspan=1 colspan=2></td><td rowspan=1 colspan=1>Encoder</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=4>表现优于BioBERT</td><td rowspan=1 colspan=1>抽取、文本分类、自动问答</td></tr></table>",
    "context3": "上述情报学领域应用的主流PLM可以分为两类：第一类是领域无关模型，例如BERT、ELECTRA等，此类模型侧重对模型架构和自监督预训练任务的改进与调优，从而提升通用文本表示能力。第二类是领域特异性模型，例如 SciBERT、SikuGPT等，这类模型专注于提升特定领域文本上的语言建模能力，为领域情报处理提供工具。"
  },
  "预训练模型在情报学领域的研究热点/Research hotspots of pre-trained modelsin information science discipline": {
    "context1": "基于VOSviewer可视化软件从宏观上对PLM在情报学领域的主要应用方向进行可视化热点分析。根据分析结果，下文将从微观上按照PLM在情报学领域的不同研究方向进行总结归纳。",
    "context2": "图3是针对情报学领域CSSCI期刊文献进行关键词共现所绘制的网络。深度学习、BERT、数字人文、命名实体识别、文本分类等关键词出现的频次较高，结合聚类信息可知当前情报学领域PLM的研究热点主要集中于对BERT模型的改进与应用以及特征融合的探索，应用较为广泛的任务为命名实体识别和文本分类，在学术文本和古汉语文本上得到了较多的挖掘，其实验过程多出现与CRF和BiLSTM等机器学习与深度学习模型的对比等。",
    "context3": "图4是WoS 核心合集中情报学领域外文文献的关键词共现网络。可以总结出对于外文文献，当前研究热点主要为基于BERT模型及其改型的自动分类、情感分析、信息抽取、关系抽取、命名实体识别等，实验语料主要为社交媒体文本和学术文本，在数据增强、零样本学习方面也有研究。",
    "context4": "![](images/616c49684b126f21e337ed79a5b5af514279b228787e8aaebb05b350d4b7ea11.jpg)  \n图3CSSCI中文文献关键词共现",
    "context5": "![](images/4006fb9f757e9509ba02fbfee26d4254ba313d056f52c409ed934de37a811f7f.jpg)  \nFigure 3 Co-occurrence of Chinese literature keywords from CSSCI   \n图4WoS英文文献关键词共现  \nFigure 4 Co-occurrence of English literature keywords from WoS",
    "context6": "综合国内外文献，可以归纳当前使用最为广泛的PLM是BERT族模型，在情报学领域的应用主要集中在自动分类和信息抽取相关研究。预训练模型在情报学研究中主要应用方向如图5所示：",
    "context7": "![](images/7b9d9123433114566b4f1d5d0d1bb9b221a566bf6af9a4e4a2a1c3dce0104867.jpg)  \n图5预训练模型在情报学研究中主要应用方向  \nFigure 5Main application of pre-trained models in information science",
    "context8": "基础标注任务的精度直接关系到后续情报分析的表现，主要涉及词汇和句子层面的自动切分与特征标注。信息抽取任务侧重于对实体和实体间关系的判别，可划分为各领域命名实体与关系抽取、实体歧义消除与共指消解。文本分类是信息组织的重要内容，包括对学术文本、古汉语文本、社交媒体文本等多源数据的多维度自动归类任务。信息检索和自动摘要主要面向下游应用与情报服务，其核心技术分别为相似度计算与文本生成。下文按照PLM在不同研究方向的应用情况进行分类别论述。"
  },
  "预训练模型在情报研究中的应用现状/Current applications of pre-trained modelsin information research": {
    "context1": "在情报学视角下，本文将PLM在情报研究中的主要应用方向进行重新组织，面向信息资源管理流程进行梳理与分析。在情报组织环节，PLM主要应用于文本分类任务；在情报检索环节，PLM的应用点为信息检索；在情报挖掘环节，PLM被用于信息抽取相关研究；对于基础标注和自动摘要任务，本文将其归类为其他情报处理环节。"
  },
  "5.1在情报组织中的应用": {
    "context1": "情报组织是指对分散无序的情报进行序化加工，涵盖编目、标引、分类、索引等过程。在具体实践中，PLM主要应用于文本分类相关任务。"
  },
  "5.1.1 情感分析": {
    "context1": "情感分析是一项经典文本分类任务，将含有显式或隐式情感的文本划分为特定情感类别，对于舆情研判、风险管控等具有重要意义。虽然在领域文本上微调通用PLM可以更新模型参数，但在学习情感相关语义特征时仍存在不足。因此，领域知识增强、模型架构修改和提示学习技术被用于提升PLM情感特征抽取能力。",
    "context2": "基于知识增强技术的相关研究见表2，主要包括引入情感词典、融合领域特征和增强语义表示三类改进。虽然融入外部知识显著提升了情感分类精度，但难以迁移至其他领域。因此，部分学者通过改进现有 PLM的嵌人方式、网络架构与注意力机制，构造领域无关模型，相关研究见表3。",
    "context3": "表2基于知识增强的情感分析研究  \nTable2 Sentiment analysis based on knowledge enhancement",
    "context4": "<table><tr><td>文献</td><td>PLM</td><td>知识增强</td><td>数据集/语料</td><td>特点</td></tr><tr><td>L. Zhao等[19]</td><td>BERT</td><td>引入外部句法依赖</td><td>SemEval</td><td>方面级，能够同时捕获词间关系和依赖</td></tr><tr><td>E.W. Pamungkas 等[20]</td><td>Multilingual BERT</td><td>引入多语言仇恨词词典</td><td>跨语言仇恨言论数据集</td><td>实现低资源语言仇恨言论分类</td></tr><tr><td>潘宏鹏等[21]</td><td>BERT</td><td>融合反讽语义特征</td><td>微博评论</td><td>结合反讽与情感语义共同识别情感极性</td></tr><tr><td>常城扬等[22]</td><td>BERT</td><td>加入推文时间特征</td><td>美国政客推特</td><td>识别政客动态政治情感极性</td></tr><tr><td>W.Jin等[23]</td><td>BERT,DeBERTa</td><td>融合牛津词典释义</td><td>Twitter, SemEval</td><td>方面级，模块化架构，验证多种知识注入方式 与模型</td></tr></table>",
    "context5": "表3基于架构修改的情感分析研究  \nTable3Sentiment analysis based on architectural modifications   \n表5基于架构优化的结构功能识别研究",
    "context6": "<table><tr><td>文献</td><td>PLM</td><td>架构改进</td><td>数据集/语料</td><td>特点</td></tr><tr><td>Y T. Lei、Y. T. Li[24]</td><td>BERT、 GPT-2、XLNet</td><td>添加情感嵌入层</td><td>亚马逊评论</td><td>提升跨领域情感分类性能</td></tr><tr><td>刘继、顾凤云[25]</td><td>BERT</td><td>均值与池化隐藏层</td><td>新冠相关微博话题</td><td>进一步构建情感特征词网</td></tr><tr><td>余本功、张书文[26]</td><td>BERT</td><td>加入多头自注意力机制</td><td>SemEval、twitter 推文</td><td>方面级，捕获方面词长距离依赖</td></tr><tr><td>T. T. Yang 等[27]</td><td>BERT</td><td>加入标签感知注意机制</td><td>微博树洞评论</td><td>可解释的预测抑郁程度与原因</td></tr></table>",
    "context7": "为了提升低资源场景下情感分类性能，受生成式模型[17]启发，赖宇斌等[28]引人P-Tuning 提示学习,在RoBERTa输入序列前拼接可训练的连续\"软提示”，实现突发事件初期微博情感分析。黄泰峰和马静[29]则同时采用离散型和连续型提示模板，进一步提升了RoBERTa的情感分类表现。",
    "context8": "当前的研究大多针对一个句子或段落进行整体上的情感极性分类，PLM在细粒度方面级情感分类上的应用仍然较少。不仅如此，绝大部分研究只能识别字面意义的情感倾向，难以区别反讽等在社交和评论文本中常见的情感表现方式，这也是PLM当前面临的主要问题之一。"
  },
  "5.1.2学术文本分类": {
    "context1": "（1）结构功能识别。学术文本的结构功能又称语步结构。通常可以根据论文文本所反映的功能层次与内容意图，将其划分为引言、方法、结果、讨论等功能性文本。对结构功能的自动识别与分类可以为实现定制化文本检索与信息服务提供条件，支撑特定文本的细粒度知识挖掘任务。PLM结合领域微调是较为常用的结构功能识别策略，相关研究如表4所示：",
    "context2": "表4采用领域微调的结构功能识别研究  \nTable 4Structure function identification based on domain fine-tuning",
    "context3": "<table><tr><td>文献</td><td>PLM</td><td>结构功能</td><td>特点</td></tr><tr><td>陆伟等[30]</td><td>BERT</td><td>关键词角色功 能</td><td>优于Word2vec- BiLSTM</td></tr><tr><td>秦成磊、章成 志[3]</td><td>BERT</td><td>篇章结构</td><td>提出层次注意力分类 模型，超越BERT</td></tr><tr><td>马晓慧等[32]</td><td>BERT</td><td>句子、段落、 章节结构功能</td><td>各层次下均优于 CNN、LSTM</td></tr><tr><td>胡忠义等[33]</td><td>ERNIE</td><td>摘要结构功能</td><td>结合DPCNN，同时捕 获局部与全局信息</td></tr><tr><td>杜新玉、李宁[34]</td><td>RoBERTa</td><td>全文语步</td><td>22种细粒度语步识别, 多阶段微调</td></tr></table>",
    "context4": "论文的正文、摘要、关键词等均具备一定的结构功能。最常见的研究手段是采用领域文本微调PLM，通常识别准确率高于传统文本表示方式与神经网络模型。部分学者进一步采用新的预训练自监督任务、更改模型架构进行特征融合等策略优化PLM,相关研究如表5所示：",
    "context5": "Table5 Structure function identification based on architecture optimization",
    "context6": "<table><tr><td>文献</td><td>PLM</td><td>结构功能</td><td>改进方式</td><td>特点</td></tr><tr><td>G. Yu等[35]</td><td>BERT</td><td>摘要语步</td><td>提出遮蔽句子 模型</td><td>优于BERT 和 SciBERT</td></tr><tr><td>王末等[36]</td><td></td><td>SciBERT 论文语步</td><td>融合句子位置</td><td>基于MLP分 类，优于改进前 SciBERT</td></tr><tr><td>张国标等[37]</td><td>BERT</td><td>关键词语 义功能</td><td>融合位置特征 与先验知识</td><td>引入注意力机制， 优于单独特征</td></tr><tr><td>郭航程等[38]</td><td>BERT</td><td>篇章结构</td><td>融入段落划分 向量</td><td>结合CRF，优于 BERT和BiLSTM</td></tr></table>",
    "context7": "可以发现，在BERT中加入句子位置与段落信息是当前最为主流的改进方式，能大幅提升对句子位置与先验知识的学习能力。此外，面向科学文本继续预训练的SciBERT也在结构功能识别中得到了应用，且最终分类效果优于通用BERT。",
    "context8": "（2）学科类别分类。学科类别分类是指按照特定的学科分类体系对学术文本的所属类别与类目层次进行划分。无论是传统的文献编目，还是细粒度的信息组织，甚至是学科交叉融合的测度，其前提都是合理的学科分类，单层级类目自动分类的相关研究如表6所示：",
    "context9": "表6单层级学科分类研究  \nTable 6Single-level subject classification",
    "context10": "<table><tr><td>文献</td><td>PLM</td><td>语料</td><td>分类标准</td><td>特点</td></tr><tr><td>罗鹏程等[39]</td><td>BERT、 ERNIE- Baidu</td><td>CSSCI论文 元数据</td><td>教育部一 级学科</td><td>优于NB、 RNN，探讨了优 化方向</td></tr><tr><td>周泽聿等[40]</td><td>BERT</td><td>中文医学论 文元数据</td><td>中图法</td><td>结合BiLSTM- SGCN，优于 BERT-BiLSTM</td></tr><tr><td>A. Carvallo 等[41</td><td>BERT、 BioBERT</td><td>医学论文元 数据</td><td>医学细分 标准</td><td>基于主动学习, 优于Word2Vec、 GloVe</td></tr><tr><td>刘江峰等[42]</td><td>Sentence- BERT、 RoBERTa</td><td>CSSCI论文 标题、摘要</td><td>CSSCI类 目体系</td><td>以层次聚类后 CSSCI学科组作 为学科类别</td></tr><tr><td>吕琦等[43]</td><td>SciBERT、 ALBERT</td><td>WOS 核心 合集摘要</td><td>ECOOM 学科体系</td><td>结合1DCNN- BiLSTM，测算 跨学科度</td></tr></table>",
    "context11": "对于单层级学科类别分类，除了BERT模型外，BioBERT、SciBERT、ERNIE-Baidu 等采用了知识增强的PLM同样性能优异。通过Bi-LSTM、CNN、",
    "context12": "GCN 等神经网络进一步抽取经PLM嵌入后的文本特征能够提升分类精准度。",
    "context13": "对于多层级类目的自动分类，赵肠等[44]微调多个BERT构建了层级分类模型，实现对中文医学科技文献摘要所属二级和三级类目的阶梯式分类，并通过模型固化与微服务部署提升了预测速度。戎璐和张亚洲[45]以序列到序列的思想开展了层次分类，通过LSTM和段内段间注意力机制学习经过BERT嵌入的序列特征，最后以文本生成的方式同时输出中图法一至四级类目。由于多层级类目分类的实现难度较大，PLM在实验的复杂度和分类结果的实用性方面仍然有所欠缺。在当前学科融合交叉的背景下，如何利用PLM在相似文本表示上的优势，对跨学科文本进行多标签分类，依旧是对分类算法的考验。"
  },
  "5.1.3古汉语文本分类": {
    "context1": "古文自动分类是数字人文研究的重要一环，其主要任务是按照典籍分类体系或特定分类标准，自动化地对古文短语、片段、章节等进行整理、归类与抽取。作为数字人文研究的中间环节与重要步骤，古汉语文本自动分类的性能决定了后续人文计算的精准度。",
    "context2": "古汉语文本大多数为单字词，这很适合BERT等以字为单位进行文本表示的PLM。梁媛等[4]发现BERT在《春秋》及“春秋三传”上古籍同事异文的分类性能优于LSTM、SVM,进一步分析发现《穀梁传》和《公羊传》的语言风格最为相似。周好等[47]对比了多种模型在《论语注疏》《春秋左传正义》等古籍上的引书上下文识别性能，Bi-LSTM整体效果优于BERT，但具体类别识别BERT更优，引文的下文识别精度高于上文。胡昊天等[48]采用SikuBERT和SikuRoBERTa自动分类《四库全书》子部典籍，效果优于BERT、RoBERTa及引入全词遮蔽的PLM,还开发了典籍自动分类工具。张力元和王军[49基于BERT自动分类先秦典籍所属诸子类目，性能强于TextCNN，并对《荀子》和《管子》的互著与别裁情况进行了识别与探讨。",
    "context3": "当前研究大多通过对比现有PLM在特定文本上的性能差异，选取最适合的模型，并进一步开展后续知识挖掘与人文计算研究。目前也已经出现了面向古文自然语言处理任务的SikuBERT、GuwenBERT等PLM，其在古文分类等任务上的表现也优于通用模型。从具体任务来看，当前研究大多从宏观层面开展典籍语言风格、文法体裁的类别划分，但对于句子、短语层面细粒度的分类探索较为欠缺。"
  },
  "5.2在情报检索中的应用": {
    "context1": "情报检索是指用户遵循特定检索语言编制检索式，在指定文档集合中查询出符合需求的文档的过程。PLM在文本相似度计算和相关性排序这两个核心步骤均有应用。"
  },
  "5.2.1 相似度计算": {
    "context1": "与传统静态词嵌入模型相比，PLM能够结合语境信息动态表示文本含义，从而对于同义词的不同义项具有更强的区分能力。采用相似度计算方法对映射到向量空间中的句子进行相关性计算，即可开展句子对齐、相似文本挖掘等任务。PLM在相似度计算中的主要应用如表7所示：",
    "context2": "表7基于预训练模型的相似度计算研究  \nlable 7Similarity calculation based on pre-trained models",
    "context3": "<table><tr><td>文献</td><td>PLM</td><td>相似度算法</td><td>任务</td><td>特点</td></tr><tr><td>高瑞卿等[50]</td><td>BERT</td><td>余弦相似度</td><td>老子思想相似典籍挖掘</td><td>从头预训练BERT，《墨子》最为相似</td></tr><tr><td>Y. Q. Mao 等[51]</td><td>BERT、BlueBERT</td><td>余弦相似度</td><td>UMLS 概念间语义相似度计算</td><td>继续预训练 BlueBERT，对比了文本与图嵌入 及其组合</td></tr><tr><td>R.S.Li等[52]</td><td>FCNN、LSTM</td><td>余弦相似度</td><td>中文和英文短语相似度计算</td><td>效果优于 BERT，ALBERT</td></tr><tr><td>李纲等[53]</td><td>BERT</td><td>类 Jaccard 相似度、余技术供需文本匹配 弦相似度</td><td></td><td>计算共现，语义，语句多层次文本相似度</td></tr><tr><td>Q. Xie等[54]</td><td>BERT、Multilingual BERT</td><td></td><td>LDA、BERT 向量均值 图情领域主题演化与相似性分析</td><td>结合LDA概率值与BERT 向量均值计算相似度</td></tr><tr><td>梁继文等[55]</td><td>BERT、ERNIE、 RoBERTa-large-pair</td><td></td><td></td><td>Softmax分类特征值科技项目与成果文献间相关性计算融合三种嵌入，引入对抗训练与伪标签学习</td></tr><tr><td>郑洁等[5]</td><td>BERT、MS-BERT</td><td>三元组损失函数</td><td>司法相似案件匹配</td><td>融合基于对抗训练的对比损失增强鲁棒性</td></tr><tr><td>J. Ding等[57]</td><td>BERT</td><td>欧氏距离</td><td>诺奖得主不同时期论文与获奖论文 间相关性</td><td>继续预训练BERT，发现博士期间研究对于获 奖论文有较大贡献</td></tr></table>",
    "context4": "从上述研究可知，PLM主要作为文本向量化表示的途径，将原始本文嵌人为多维向量。在此基础上,采用余弦相似度、K-means 等相似度算法进一步度量文本相似情况。与传统文本表示方法相比，PLM包含更多词汇与句子的上下文关联信息，因此更能反映真实文本的相关性。为了更好地表达面向特定任务的语义特征，部分学者也尝试采用新的训练架构，或是通过嵌入融合的方式，采用多种PLM共同表示文本，从而充分利用各模型优势。"
  },
  "5.2.2信息检索与自动问答": {
    "context1": "信息检索与自动问答是向量相似度与相关性计算的经典应用，其核心问题是对查询与文档的相似性计算及检索结果的相关性排序。",
    "context2": "经过PLM嵌入后的查询与文档能够动态表征语义信息，提供更精确的响应结果。牛海波等[58]基于引文上下文继续预训练BERT，在同一向量空间表征被引文献及其引文上下文，提升了语义检索准确率。W.Sakata等[59]面向问题一答案对检索，同时考虑用户查询与问题及答案间的相关性得分，将查询一答案对输入BERT计算相关性。吕学强等[60]采用BERT嵌入拼接后的查询与文档，通过GCN提取图拓扑结构特征，最终基于CEDR联合排序模型完成匹配分数计算。可以发现，在查询与文档相关性匹配过程中，BERT、ELMo 等模型主要被用于文本嵌入。采用领域适应性预训练的PLM能更好地捕获查询与文档的语义信息。",
    "context3": "对于相关性排序，罗鹏程等[61]使用SimCSE和BM25将查询与文档向量化并构建候选数据集，采用基于BERT的排序模型确定检索结果顺序。R.Nogueira等[62]使用BM25检索并迭代构建候选文献库，采用SciBERT分类器判断文献与查询的相关性并排序引文。J.M.Wang等[63]提出了一种结合相关性匹配与语义相似度伪相关反馈检索方法，通过BERT分类器重新排序BM25提供的检索结果以提高反馈文档质量。Z.Zheng等[64提出了查询拓展模型 BERT-QE，基于进一步微调后的 BERT重新排序反馈文档块以拓展查询，并根据加权求和等方法进行最终文档排序。在相关性排序阶段，通常基于BERT文本分类架构进行查询结果重要性判断，根据类别概率大小优化查询文档的最终排序，其效果优于DRMM、DSSM等经典模型的表现。",
    "context4": "区别于信息检索，自动问答研究更关注多轮信息查询。学者主要通过在BERT原始嵌人中加入领域知识层、历史答案层的方式提升模型对于多轮连续对话的建模能力。王日花[65]提出了一种融合问题、答案等多源数据的多层异构网络，通过K-means对BERT嵌入的答案聚类构建答案簇，作为问题的类别标签。",
    "context5": "程子佳和陈[通过BERT实体语义相似度计算，从语料库中匹配与查询最相似的未登录实体作为查询词。为了提升多轮对话模型对历史信息的学习能力，L.Li等[通过下一个对话预测任务增强BERT对问题和回答的上下文连续性建模，并采用多种注意力机制抽取对话与响应间交互特征，最终计算匹配分数。"
  },
  "5.3在情报挖掘中的应用": {
    "context1": "情报挖掘是指以情报处理与知识挖掘技术为手段，对表层信息进行深度加工从而形成新的知识的过程。信息抽取（informationextraction,IE）作为情报挖掘的重要任务，引入了大量基于PLM的文本挖掘方法。该任务又可以细分为命名实体识别（namedentity recognition,NER）、关系抽取（relation ex-traction,RE）、知识融合等子任务[68]。基于PLM识别出特定类型知识元间相互关系，从而实现关联知识组织与挖掘，服务于本体知识库及知识图谱的构建任务，促进知识推理、知识元检索和可视化分析与呈现。"
  },
  "5.3.1命名实体识别": {
    "context1": "命名实体识别是一种从领域文本中识别并抽取特定人名、地名、机构名、时间等专有词汇的任务。",
    "context2": "（1）情报学文本。面向情报学领域文本，沈思等[9]基于BERT对iSchool学校课本中的课程实体进行抽取，综合分析了课程异同与分布情况，并给出了情报学课程设计的几点启示。胡昊天等[70]发现在小规模语料上PLM的情报学招聘实体的识别效果逊于传统机器学习模型，并从实体角度分析了情报学岗位需求与人才培养方式。梁媛等[7发现BERT对于上下文关系不强的短句特征提取能力弱于Bi-LSTM-CRF，基于数据科学岗位任职要求实体分析了情报学未来课程设置与培养方案。刘浏等[72]发现BERT对iSchools培养计划中的能力和研究领域实体的抽取能力强于Bi-LSTM-CRF，并总结了对我国情报学发展与教育的启示。",
    "context3": "对于情报学领域实体识别，通常采用领域微调后的BERT模型或是结合PLM与神经网络架构进行处理。在对比实验中，通常仅采用BERT模型即可取得超越传统词向量加神经网络的实体识别效果，而结合BERT嵌入和神经网络架构后，性能提升更为明显。但是对于上下文关联较弱的情报学文本，BERT无法发挥深度文本表示的优势，因此可能会弱于经典神经网络模型的表现。",
    "context4": "（2）生物医学文本。PLM在临床和生物医学",
    "context5": "NER中应用广泛。区别于情报学NER，研究者在使用PLM时除了采用领域文本微调，还尝试了继续预训练和特征融合等方式。不同策略下的相关研究如表8所示："
  },
  "表8生物医学文本命名实体识别研究": {
    "context1": "Table8 Biomedical text named entity recognition",
    "context2": "<table><tr><td>应用方式</td><td>文献</td><td>PLM</td><td>数据集/语料</td><td>实体类型</td><td>特点</td></tr><tr><td rowspan=\"2\"></td><td>领域微调景慎旗、赵又霖[73]</td><td>BERT</td><td>电子病历文书</td><td>电子病历实体</td><td>采用GCN-CRF 建模依赖结构</td></tr><tr><td>Y.D.Fan 等[74]</td><td>BERT, Clinical BERT</td><td>CDR 临床笔记</td><td></td><td>膳食补充剂与事件实体 预训练模型优于Bi-LSTM-CRF，BERT最佳</td></tr><tr><td rowspan=\"2\">继续预训 练</td><td>X. Yang 等[75]</td><td>BERT、RoBERTa、 ALBERT、ELECTRA</td><td>i2b2,n2c2</td><td>临床概念</td><td>在通用和临床文本上分别继续预训练，临床 RoBERTa最优</td></tr><tr><td>J. C. Du等[76]</td><td>BERT、BioBERT、 MIMIC BERT</td><td>VAERS 疫苗不良 事件报告</td><td>神经系统疾病实体</td><td>在 VAERS上继续预训练 BioBERT构建 VAERS BERT，引人集成学习</td></tr><tr><td rowspan=\"2\">特征融合</td><td>B.Fan等[77]</td><td>BERT</td><td>药物评论文本</td><td>药物不良反应实体</td><td>融合 BERT词嵌入与句子嵌入，使用 BiLSTM抽取特征</td></tr><tr><td>张云秋等[78]</td><td>RoBERTa-www</td><td>CCKS2017电子 病历数据集</td><td>电子病历实体</td><td>动态融合编码器权重，使用BiLSTM抽取特 征</td></tr></table>",
    "context3": "基于BERT、RoBERTa等模型的动态文本表示方式通常可以捕获更丰富的语言学特征，但是在有监督领域文本较为匮乏的情况下，BERT微调后的性能可能会弱于经典神经网络模型。为了提升语义抽取能力，部分研究先在生物医学领域文本上继续预训练通用PLM，再进行微调，从而显著提升NER表现。向量融合技术也在生物医学领域PLM中得到应用，将字、词、句子等多粒度向量与BERT原始向量融合，或是对网络各层参数融合，均能以相对较低的训练成本提升实体识别性能。",
    "context4": "（3）古汉语文本。古文实体识别是数字人文研究的基础。崔竞烽等[79]基于BERT模型抽取了菊花古典诗词中的人物、花名、节日等实体，并构建了诗词命名实体知识图谱。徐晨飞等[80]发现Bi-LSTM-CRF模型对于方志物产实体识别的F值高于BERT，但BERT对于人名实体识别效果更优，并进一步构建了云南方志物产知识库。杜悦等[81] 对比了多种深度学习模型的先秦典籍实体识别效果，发现在小规模语料上BERT优于Bi-LSTM-CRF，并构建了典籍实体自动识别平台。刘江峰等[82]基于SikuBERT与SikuRoBERTa模型抽取前四史等古籍实体，其性能优于通用BERT、RoBERTa 和面向古文的GuWenBERT。林立涛等[83]使用标注了动物实体的先秦典籍文本进一步微调 SikuBERT模型，自动抽取并分析了《史记》中动物知识。喻雪寒等[84对比了BERT、RoBERTa和GuwenBERT词嵌入方式下《左传》战争句事件抽取性能，发现RoBERTa模型的效果最优。",
    "context5": "基于领域适应性预训练技术在古文文本上继续训练后的模型更适用于古文自动处理任务，在古文实体识别能力上通常优于领域微调的通用BERT和经典神经网络模型。但受限于简繁字体、语料规模、实体特征等因素，PLM对于部分特定类别实体的抽取性能可能会弱于经典神经网络模型。",
    "context6": "当前NER研究大多采用3类模型训练策略：$\\textcircled{1}$ 领域微调，在领域文本上微调现有PLM，性能通常优于传统静态词嵌入模型word2vec 和机器学习模型LSTM-CRF等。该方法简单易行，但模型性能易受到语料规模和质量的影响。 $\\textcircled{2}$ 领域适应性预训练，采用领域文本继续训练通用PLM，学习领域文本特征，与领域微调相比提升领域实体抽取能力。但是，对于GPU等硬件资源的需求较高。 $\\textcircled{3}$ 通过调整模型架构，多种粒度嵌入组合以及词典知识引入的方式，融合更多本地与外部知识，提升对实体类别特征提取能力。此类方法定制化程度高，贴合具体任务需求，但难以直接迁移至其他领域。"
  },
  "5.3.2术语识别": {
    "context1": "术语指特定专业领域中一般概念的词语指称，是一种专业性、细粒度的命名实体[85]。由于术语包含的词汇更多，特征与语义一致性较弱，因此其边界更难确定，更考验模型对于词汇、句子及上下文的语义理解能力，相关研究如表9所示："
  },
  "表9术语识别研究": {
    "context1": "Table 9Terminology identification",
    "context2": "<table><tr><td>文献</td><td>PLM</td><td>语料</td><td>术语类型</td><td>！特点</td></tr><tr><td>刘浏等[86]</td><td>BERT</td><td>非遗项目 文本</td><td>非遗音乐 术语</td><td>优于CRF、LSTM-CRF</td></tr><tr><td>任秋彤等[85]</td><td>BERT</td><td>非遗项目 文本</td><td>非遗戏剧 术语</td><td>融入词性、领域特征， 结合BiLSTM-GCN</td></tr><tr><td>熊欣等[87]</td><td>BERT</td><td>地方志</td><td>方志术语</td><td>优于BiLSTM-CRF，构 建相关关系知识图谱</td></tr><tr><td>张卫等[88]</td><td>BERT</td><td>唐诗鉴赏 辞典</td><td>古诗情感 术语</td><td>优于多特征CRF，开展 多粒度数字人文研究</td></tr><tr><td>翟羽佳等[89]</td><td>BERT</td><td>计算机学 科文献</td><td>英文文献 算法术语</td><td>结合BiLSTM-CRF，探 究算法创新演化路径</td></tr></table>"
  }
}