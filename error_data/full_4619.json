{
  "original_filename": "full_4619.md",
  "基于混合模型的文本聚类研究综述1）": {
    "context1": "王方成颖柯青（南京大学信息管理学院，南京210023）",
    "context2": "摘要相较于其他聚类算法,模型聚类的实证研究结果表现出了独特的优势，越来越受到学界的关注。本文梳理了混合模型文本聚类的相关研究，根据聚类分析的技术路线，主要综述了文本建模、参数建模以及模型推理等三个主要模块,在此基础上总结了特征降维、半监督聚类以及聚类过程的系统整合等不同研究中的共性问题。最后，提出了本领域未来可能的研究方向。",
    "context3": "关键词 模型聚类混合模型文本聚类"
  },
  "Mixture Model-based Text Clustering: A Review": {
    "context1": "Wang Fang, Cheng Ying and Ke Qing {School ofInformation Management,Nanjing University,Nanjing20023",
    "context2": "Abstract Model-based clustering has atracted more and more attention，and empirical studies also showed distinct advantage.This paperreviews thestatusof thedocument clustering basedon mixture models.According to the technical routes,it summarizes three mainparts，such as document modeling，parameter modeling，and model inference，and analysesthe common problems in diferent researches，including featurereduction，semi-supervised clusteringand the integration of clustering process.At last it presents possble future research directions in this field.",
    "context3": "Keywords model-based clustering，mixture model,document clustering"
  },
  "1引言": {
    "context1": "聚类方法可以分为判别式与生成式两类[1]。判别式方法基于数据的相似性（或距离）进行聚类，例如 $\\mathbf { k }$ -means以及层次聚类算法；生成式方法通过特定的模型表征簇，使得数据与模型的拟合最优，例如自组织地图（SOM）与混合模型（MixtureModels）聚类。其中混合模型聚类的基本思想是将聚类问题转化为数学建模问题，即利用简单概率分布的组合模拟复杂概率分布的统计建模方法，一个概率分布代表一个簇的特征分布，概率分布的组合即代表整个数据集的特征分布，核心环节是数据建模与模型推理。相对于判别式方法，混合模型聚类有坚实的概率论基础，通过数据分布的形状与结构而非相似性进行类簇的识别，更具灵活性，而且能够从概率统计的视角为每个簇提供直观的解释，具有良好的理论与应用前景。",
    "context2": "混合模型研究的早期存在模型推理过于复杂的瓶颈,1977年Dempster等[2]提出的EM算法较好地解决了该问题，促进了该模型在文本处理、生物医学、图像处理以及模式识别等领域的研究与应用，并取得了令人欣喜的成果。基于此，本文聚焦于基于混合模型的文本聚类研究，其流程见图1。混合模型文本聚类的主要模块有： $\\textcircled{1}$ 文本建模。即假定文本集符合某种统计模型，如多元伯努利（BernoulliMixture，BM）、多项式混合模型（MultinomialMixture，MM)或者vMF(vonMisesFisher）混合模型等； $\\textcircled{2}$ 模型推理。即利用基于BM等模型的数据似然（或后验概率)作为优化准则，通过EM算法、变分推理以及马尔可夫链－蒙特卡洛（MarkovchainMonteCarlo，MCMC)方法等推理算法得到模型参数的估计值以完成聚类； $\\textcircled{3}$ 聚类评估，常用指标有准确率、召回率以及 $\\pmb { F }$ 值等。除了这些主要模块之外，部分研究中还涉及参数建模（由于仅在部分研究中出现，图1中呈现为虚线），即对混合模型的参数进行建模，旨在实现模型优化，常用方法有狄利克雷分布以及狄利克雷过程等。",
    "context3": "在介绍文献来源之后，本文按照图1的线索组织。混合模型文本聚类的分词、词干提取、同义词归并等文本预处理工作与其他自然语言处理相似，不再赘述。聚类评估的相关指标及应用与判别式聚类相同，也不再展开。本文重点综述文本建模、参数建模以及模型推理的相关研究进展。此外，还对混合模型聚类的特征选取、特征抽取（featureextraction）、特征选择（featureselection）、半监督聚类以及聚类过程的系统整合等共性问题进行了总结与展望。",
    "context4": "![](images/c69ecedf528ad8f69858e7473b69c35666de9b21fa0480ae7b192cc2e5678cbf.jpg)  \n图1混合模型文本聚类流程图",
    "context5": "(4)文献类型包括期刊论文、硕博士论文以及会议论文。"
  },
  "2.2文献选取过程": {
    "context1": "（1)基于WOS核心合集（排除化学索引），时间跨度为 $1 9 0 0 \\sim 2 0 1 4$ 年，检索式为：主题 $\\mathit { \\Theta } = \\mathit { \\Theta } \\left( \\mathit { \\Theta } \\left( \\begin{array} { l } { \\mathfrak { n } } \\end{array} \\right. \\right.$ mixture model $^ *$ \"or”mixture distribution $^ *$ \"ormixtures）and cluster $^ *$ ）or\"probabilistic cluster $^ *$ or \"distributional cluster $^ { * }$ \"or\"model-based cluster$* ^ { \\ n }$ ）and（text $^ *$ or document $^ *$ ），检索结果为416篇文献,根据题名和文摘筛选后得到41篇相关文献。进一步检索：主题 $\\lneq$ （（multinomial or Bernoullior“von mises fisher” or vmf）and（text $^ *$ ordocument $^ { * }$ ）and cluster $^ { * }$ ）,检索得到63篇文献，补充得到2篇相关文献。",
    "context2": "(2）基于GoogleScholar搜索引擎,采用model-based、\"mixture model\"分别与\"documentcluster\"相组合，相应的检索结果为487、209，经筛选后得到7篇文献。",
    "context3": "(3)在中文文献方面，主要基于CNKI以及万方两个数据库，检索式为：（混合模型 $^ +$ 概率模型） $^ { * }$ （文本 $^ +$ 文档） $^ *$ 聚类，在CNKI得到92 篇结果，在万方得到100篇结果，筛选后得到6篇。",
    "context4": "(4)在文献阅读过程中根据参考文献不断扩展综述文献的范围，另获得20篇参考文献，最终合计相关文献76篇。根据权威的中国计算机学会推荐的国际学术会议和期刊目录选择了核心文献18篇；在此基础上，根据被引频次、发文量等指标确定了核心作者,如BanerjeeA,ZhongS，MeilaM等,由此得到核心文献19篇，二者去重后共得到核心文献 24篇（附录1），以此为线索，结合另外52篇相关文献完成本文。"
  },
  "2文献来源": "",
  "2.1文献选取原则": {
    "context1": "(1)综述文献的基本设定为：文本集由若干简单概率分布的混合模型生成,每一混合成分对应一个簇，文本属于其中一个(或若干)簇；",
    "context2": "(2)具体范围包括混合模型文本聚类中的文本建模、参数建模以及模型推理等研究型文献；"
  },
  "(3)语种为英文与中文；": "",
  "3文本建模": {
    "context1": "假设文本集 $D = \\left\\{ d _ { 1 } , d _ { 2 } , \\cdots , d _ { N } \\right\\} \\in \\mathbb { R } ^ { \\mathbb { V } } , \\Lambda$ $N$ 表示 $\\pmb { D }$ 中的文本数量， $\\pmb { \\mathcal { W } }$ 表示文本的特征维数， $d _ { n } = \\{ d _ { n 1 }$ ，$d _ { n 2 } , \\cdots , d _ { n w } \\}$ 表示第 $\\pmb { n }$ 个文本， $d _ { n w }$ 表示第 $\\pmb { n }$ 个文本的第 ${ \\pmb w }$ 个特征维度。 $\\pmb { D }$ 包含 $\\pmb { K }$ 个簇，即由 $\\pmb { K }$ 个成分的混合模型生成。现有的混合模型都符合朴素贝叶斯假设，即特征维度之间相互独立，文本之间也相互独立。 $\\{ d _ { 1 } , d _ { 2 } , \\cdots , d _ { N } \\} \\in \\mathbb { R } ^ { \\ast }$ 可以看做是随机向量 $d _ { n } \\in$ $\\pmb { \\mathbb { R } } ^ { \\pmb { \\mathbb { w } } }$ 的独立实现,从模型中生成 $d _ { n }$ 以及 $\\pmb { D }$ 的概率见公式（1）和公式(2）[3.4]。",
    "context2": "$$\np \\left(  { d _ { _ n } } \\mid \\Theta \\right) ~ = ~ \\sum _ { \\kappa = 1 } ^ { \\kappa } \\pi _ { k } p _ { k } \\left(  { d _ { _ n } } \\mid \\theta _ { k } \\right)\n$$",
    "context3": "$$\np \\big ( D \\mid \\Theta \\big ) \\ = \\ \\cal { I I } _ { n = 1 } ^ { \\cal { N } } \\big ( \\sum _ { \\kappa = 1 } \\pi _ { k } p _ { k } \\big ( \\mathcal { d } _ { n } \\mid \\theta _ { k } \\big ) \\big )\n$$",
    "context4": "其中， $\\mathfrak { T } _ { k } \\left( \\pi _ { k } \\geqslant 0 \\right.$ ，且 $\\sum _ { K = 1 } ^ { K } \\pi _ { k } \\ = 1 \\ )$ 代表混合成分 $\\pmb { k }$ 在混合模型中的比重， $p _ { k } ( \\mathbf { \\theta } \\cdot \\mathbf { \\alpha } )$ 表示混合成分 $\\pmb { k }$ 的概率分布， $\\pmb \\theta _ { k }$ 为混合成分 $\\pmb { k }$ 的参数。模型中的所有未知参数用 $\\Theta$ 表示，如果簇的数量 $\\pmb { K }$ 确定，则称之为有限混合模型， $\\boldsymbol \\Theta = \\{ \\pi _ { 1 } , \\pi _ { 2 } , \\cdots , \\pi _ { k } ; \\theta _ { 1 } , \\theta _ { 2 } , \\cdots , \\theta _ { k } \\}$ ；如果簇的数量无限,则称之为无限混合模型， $\\boldsymbol { \\Theta } = \\left\\{ \\right. \\boldsymbol { \\pi } _ { 1 }$ ，$\\pi _ { 2 } , \\cdots , \\pi _ { k } ; \\theta _ { 1 } , \\theta _ { 2 } , \\cdots , \\theta _ { k } ; K \\}$ ，假设 $\\pmb { K }$ 值趋近于无穷大[5\\~7] 。",
    "context5": "在具体的研究与应用中，需要根据数据集的类型选择特定的概率分布函数 $p ( \\mathbf { \\theta } \\cdot \\mathbf { \\alpha } )$ 。目前,在混合模型文本聚类中常用的模型有："
  },
  "3.1离散型混合模型": {
    "context1": "该类型主要有BM和 $\\mathbf { M M }$ 。其中，BM最早应用于文本分类的文本建模，并且得到了较好的效果[8,9]。BM只考虑项在文本中是否出现,每个特征维度的取值 $d _ { n w } \\in \\{ 0 , 1 \\}$ ，取1表示项 $\\pmb { w }$ 在 $d _ { n }$ 中出现，否则反之。MM还考虑了文本中项 ${ \\pmb w }$ 出现的频次，用 $d _ { n w }$ 表示。如果文本集的特征维度为 $\\pmb { W }$ ，则二者第 $k$ 个混合成分的参数为 $\\theta _ { k } = \\{ \\theta _ { k 1 } , \\theta _ { k 2 } , \\cdots , \\theta _ { k w } \\}$ ，$\\theta _ { k w }$ 代表簇 $k$ 中项 $\\pmb { w }$ 出现的概率。BM的文本生成概率见公式（3）[10,11],将其代人公式（2）即可得到文本集的生成概率（下同）。",
    "context2": "$$\np _ { k } \\left( \\begin{array} { l l l } { { d _ { \\mathfrak { n } } } } & { { \\vert } } & { { \\theta _ { k } } } \\end{array} \\right) ~ = ~ { \\cal I I } _ { \\mathcal { W } = 1 } ^ { \\mathcal { W } } \\theta _ { k w } ^ { ~ d _ { \\mathfrak { n } ^ { w } } } \\left( 1 ~ - ~ \\theta _ { k w } \\right) ^ { 1 - d _ { \\mathfrak { n } w } }\n$$",
    "context3": "令 $\\smash { l _ { p } }$ 为 $d _ { n }$ 的长度，多项式模型的文本生成概率见公式（4）[6,7,12,13] 。",
    "context4": "$$\np _ { k } \\left(  { d } _ { n } \\mid  { \\theta } _ { k } \\right) ~ = ~ \\frac { l _ { n } ! } { \\pi _ { w = 1 } ^ { w } d _ { n w } ! } \\pi _ { w = 1 } ^ { w } \\theta _ { k w } { } ^ { d _ { n w } }\n$$",
    "context5": "BM与MM未区分项主题表达能力的不同，即某些项与特定主题密切相关，在不同簇上的分布差异明显；而有些项则概念宽泛，在不同簇上的分布类似。对此,Li等[4]基于MM将 $\\theta _ { k }$ 细化为 ${ \\pmb \\theta } _ { T } ^ { k }$ 与 $\\theta _ { G } ^ { k }$ ，分别对应与主题内容相关的主题模型，以及与写作知识相关的通用模型，该方法与经典的MM相比，在Marco-F1指标上提高了 $40 \\%$ ；Bouguila[10]基于BM,引人参数 $\\lambda _ { w }$ 代替 $\\theta _ { k w }$ 表征所有无关特征项，参数 $\\pmb { \\rho } _ { \\pmb { \\ w } }$ 用于表征项 $\\pmb { w }$ 为相关特征项的概率。上述研究归纳起来基本思路都是区分核心（core）项与通用（general)项，研究中具体计算公式虽有差异，但都可以统一于公式（5）。其中， $\\theta _ { k w } ^ { b }$ 为通用项的参数，$\\theta _ { k w } ^ { t }$ 为核心项的参数，系数 $\\pmb \\varepsilon$ 用于平衡两者的效用比重,类似于语言模型中的平滑思想。",
    "context6": "$$\nP ( w \\mid k ) \\ = \\ ( \\ 1 \\ - \\ \\varepsilon ) \\theta _ { k w } ^ { b } \\ + \\ \\varepsilon \\theta _ { k w } ^ { t }\n$$"
  },
  "3.2连续型混合模型": {
    "context1": "Salton等[15]的研究表明,对文本向量进行归一化处理能够解决文本长度造成的偏差;Dhillon等[6]的研究显示，基于余弦相似度而非欧氏距离的 $\\mathbf { k }$ -means算法（spk-means）在文本聚类中效果更好。据此，Banerjee等[1]认为文本向量具有方向性数据(directional data,即 $\\mid \\mid d _ { n } \\mid \\mid = 1 )$ 的特征，并应用vMF（von Mises Fisher）分布完成文本建模[公式(6)][5,17]。",
    "context2": "$$\np _ { k } \\left( \\boldsymbol { d } _ { n } \\mid \\boldsymbol { \\theta } _ { k } \\right) \\ = c _ { w } \\left( \\kappa _ { k } \\mu _ { k } ^ { T } \\boldsymbol { d } _ { n } \\right)\n$$",
    "context3": "其中， $\\mid \\mid d _ { \\scriptscriptstyle n } \\mid \\mid = 1 , c _ { \\scriptscriptstyle w } \\left( \\kappa \\right)$ 为标准化常量，参数 $\\theta _ { k } =$ $( \\mu _ { k } , \\kappa _ { k } ) , \\mu _ { k }$ 为均值向量， $\\kappa \\left( \\kappa > \\mathbf { \\lambda } = 0 \\right)$ 为聚集参数（concentration parameter）,表示向量 $d _ { n }$ 在 $\\mu _ { k }$ 周围的集中程度， $\\pmb { \\kappa }$ 值越大说明 $d _ { n }$ 在 $\\pmb { \\mu } _ { k }$ 方向上的集中度越高，当 $\\pmb { \\kappa } = \\mathbf { 0 }$ ,密度函数降为一个均匀分布， $\\kappa  \\infty$ ，则密度函数趋向于一个点密度。",
    "context4": "研究表明vMF具有鲜明的优点：Banerjee等[17]发现vMF是余弦相似度基于参数模型的一般化，从而理论上vMF模型能在保证聚类质量的同时提升算法效率；Banerjee等[18]的实验表明该模型能够较好地解决高维空间中的簇重叠、簇的分布偏斜、簇规模过小等问题；Mardia等[19]还证实vMF是一种适用于高维空间方向统计的分布。不过, $\\mathbf { v } \\mathbf { M } \\mathbf { F }$ 也具有参数估计困难等缺点，由于参数 $\\pmb { \\kappa }$ 包含贝塞尔方程比值的倒数，这一复杂的非线性公式使得极大似然估计以及 EM算法中的 $M$ 步难以实现[20,21]。对此,Zhong等[1]在研究中采用了一种简化的vMF模型，即每次迭代中所有混合成分的参数 $\\pmb { \\kappa }$ 取值相同，然后逐次增大 $\\pmb { \\kappa }$ 值；Banerjee等[18]则运用 $\\pmb { \\kappa }$ 的近似替代 $\\pmb { \\kappa }$ ，该方法比固定的 $\\pmb { \\kappa }$ 值能得到更好的聚类结果，但是运算效率不高。"
  },
  "3.3一点思考": {
    "context1": "（1)模型性能。Zhong[]的对比实验显示，在聚类效果方面,大部分数据集上vMF优于MM,后者又优于BM，例如在NG20数据集上三者的NMI指标分别为0.57、0.54、0.19;在运行时间方面，基于NG20数据集在硬分配条件下三者的运行时间分别为 $1 7 . 5 \\mathrm { s } , 3 6 . 7 \\mathrm { s } , 4 3 \\mathrm { s } , \\mathrm { v M F }$ 呈现出良好的时间效率，但在软分配条件下分别为 $7 6 , 8 8 , 4 7 , 7 8 , 7 7 , 8 \\mathrm { s }$ ，显示vMF更容易受分配策略的影响,相比之下MM更具稳定的时间效率。总体而言，BM结构简单，但实验结果不够理想。MM时间复杂度较低且聚类效果较好，在实践中取得了广泛的应用。vMF相比前两者模型更加复杂，模型推理的时间复杂度更大，建模效果相对更优，不过尚存部分理论问题有待解决，是目前的研究热点。",
    "context2": "(2)文本长度的影响。McCallum等[9]的研究表明,在Yahoo数据集上，MM在1000词时准确率最高，而BM在200 词时准确率最高;在Industry Sector数据集上，MM在20000词时准确率最高，而BM在1000词时准确率最高；另外三个数据集也得到了相似的结论。由此可见，BM适合于短文本，而MM更适合于长文本的复杂聚类任务；研究还表明，MM更适合于文本长度变化较大的数据集，而BM则更适合于文本长度比较稳定的数据集。Novoviova[22]的研究表明，除非常小的项集之外MM的分类准确率总体上优于BM，MM最高能达到 $9 4 . 9 \\%$ ,比BM平均高 $4 \\%$ ；与BM相似， $\\mathbf { v } \\mathbf { M } \\mathbf { F }$ 也在较短的文本上表现优异[1]。",
    "context3": "(3)混合模型的综合应用。Zhu等[]提出每个域都由一个最适合的概率模型独立生成的建模思想，构建了域独立聚类模型（FieldIndependentClusteringModel，FICM），分别采用伯努利、多项式分布对不同文本域联合构建混合模型，相比于单一的BM、MM,三者在TREC数据集上的平均NMI分别为0.736、0.714、0.712，从而提示混合模型的综合应用是该领域值得探讨的研究方向。",
    "context4": "除了上述三种模型之外，早期对于混合模型的研究都是以高斯混合模型（Gaussian Mixture Model,GMM)进行建模，目前仍然应用广泛[23.24]。不过，因为文本的高维与稀疏特征不符合高斯分布，除了Liu 等[25]利用GMM 进行的探索性工作,在文本聚类中鲜见该模型。"
  },
  "4参数建模": {
    "context1": "MM模型难以识别项爆发（burstiness），对此，Madsen等[26]通过参数的先验分布对模型的参数进行建模，提高了建模效果。狄利克雷过程的引人成功地解决了MM以及vMF模型需要预先设定簇数量 $K$ 的不足。"
  },
  "4.1狄利克雷分布": {
    "context1": "狄利克雷分布是多项式分布的共轭分布，通常作为多项式参数的先验分布应用在贝叶斯估计中，在文本建模中已有较多应用[27]。Madsen等[26]采用DCM（Dirichlet Compound Multinomial）[28}对文本建模,即通过狄利克雷分布抽取多项式分布[公式（7)]，然后基于此生成文本[公式(8)]。",
    "context2": "$$\n\\begin{array} { r l } { p ( \\theta \\mid \\alpha ) } & { = \\cfrac { { \\cal T } ( \\displaystyle \\sum _ { w = 1 } ^ { \\Psi } \\alpha _ { w } ) } { { \\cal T } _ { w = 1 } ^ { w } { \\cal T } ( \\alpha _ { w } ) } { \\cal T } _ { w = 1 } ^ { \\Psi } \\theta _ { w } ^ { \\alpha _ { w } - 1 } \\qquad ( 7 } \\\\ { \\quad } & { } \\\\ { p ( d _ { * } \\mid \\alpha ) } & { = \\int p ( d _ { * } \\mid \\theta ) p ( \\theta \\mid \\alpha ) d \\theta = } \\\\ { \\cfrac { l _ { * } ! } { { \\cal T } _ { w = 1 } ^ { V } d _ { w } ! } \\cfrac { { \\cal T } ( \\displaystyle \\sum _ { w = 1 } ^ { \\Psi } \\alpha _ { w } ) } { { \\cal T } ( \\displaystyle \\sum _ { w = 1 } ^ { \\Psi } d _ { w } + \\alpha _ { w } ) } { \\cal T } _ { w = 1 } ^ { \\Psi } \\cfrac { { \\cal T } ( d _ { w } + \\alpha _ { w } ) } { { \\cal T } ( \\alpha _ { w } ) } } \\end{array}\n$$",
    "context3": "其中， $\\alpha = \\{ \\alpha _ { \\mathrm { i } } , \\alpha _ { 2 } , \\cdots , \\alpha _ { w } \\}$ 是狄利克雷分布的参数，该模型的优点弥补了MM无法识别项爆发（burstiness）的缺陷，即项在文本中出现一次后趋向于再次出现，参数 $\\pmb { \\alpha } _ { w }$ 的值越小，项越趋向于爆发，该模型在文本分类的研究中呈现了较好的效果。Elkan[29]鉴于文本的稀疏特性对公式(8)进行了简化，采用 $T ( \\alpha _ { w } )$ 代替 $\\frac { T ( d _ { n w } + \\pmb { \\alpha } _ { w } ) } { T ( \\pmb { \\alpha } _ { w } ) }$ 提出了一种近似的EDCM并应用于文本聚类,在NIPS数据集的对比实验中，EDCM 与 DCM 的 MI分别为0.84364、0.83705，运行时间分别为6.61s $, 7 5 1 . 3 \\mathrm { s }$ ,在聚类效果相差无几的情况下，EDCM的时间复杂度远低于DCM。Bouguila基于广义狄利克雷（generalizedDirichlet）[30]提出了MGDD（Multinomial GeneralizedDirichlet Distribution）[31],该方法与DCM以及 MM在WEBKB数据集上的聚类准确率分别为87.12、84.25、81.16,t检验表明MGDD对聚类的改善效果具有统计上的显著性。该结果提示，广义狄利克雷分布在混合模型文本聚类中的研究与应用是一个可选的方向。"
  },
  "4.2狄利克雷过程": {
    "context1": "$\\pmb { D P }$ 是狄利克雷分布在连续空间上的扩展，通常表示为： $G \\sim D P ( \\alpha , G _ { 0 } )$ ,其中 $G _ { \\mathfrak { o } }$ 是基分布， ${ \\pmb { \\alpha } } ( { \\pmb { \\alpha } } >$ 0)是集中度参数， $G$ 表示在 $G _ { \\mathfrak { d } }$ 和 $\\pmb { \\alpha }$ 上产生的随机分布。DP主要应用于概率模型中作为先验分布以构建狄利克雷过程混合模型（DirichletProcessMixture，DPM）[32]。DPM在文本聚类中有 stick-breaking和中国餐馆过程（Chinese RestaurantProcess,CRP)两种不同的模型构建视角，聚类思想见公式(9)和公式(10)，即文本 $d _ { n }$ 以正比于簇规模的概率属于一个既有的簇，或以正比于 $\\pmb { \\alpha }$ 的概率属于一个新簇,最终得到的参数集 $\\{ \\pmb { \\theta } _ { 1 } , \\pmb { \\theta } _ { 2 } , \\cdots , \\pmb { \\theta } _ { N } \\}$ 提供了聚类的依据[33]。",
    "context2": "$$\n\\theta _ { n + 1 } \\mid \\theta _ { 1 } , \\cdots , \\theta _ { n } , \\alpha , G _ { 0 } \\sim { \\frac { 1 } { n + \\alpha } } { \\sum } _ { k = 1 } ^ { \\kappa } t _ { k } \\delta _ { \\theta _ { k } ^ { * } } + { \\frac { \\alpha } { n + \\alpha } } G _ { 0 }\n$$",
    "context3": "$$\nz _ { n + 1 } \\mid z _ { 1 } , \\cdots , z _ { n } , \\alpha , G _ { 0 } \\sim { \\frac { 1 } { n + \\alpha } } { \\sum } _ { k = 1 } ^ { \\kappa } t _ { k } \\delta _ { \\theta _ { k } ^ { * } } + { \\frac { \\alpha } { n + \\alpha } } G _ { 0 }\n$$",
    "context4": "其中， $\\delta ( \\mathbf { \\partial } \\cdot \\mathbf { \\partial } )$ 为狄拉克 $\\pmb { \\delta }$ 函数，新参数 $\\theta _ { n + 1 }$ 的条件分布服从polya分布。Huang等[6.7]从CRP的角度运用DP,基于多项式模型先后提出了DPMFS和DPMFP模型,还采用简化的DMA模型[34]解决了DPM无法快速估计参数的问题。DPMFP[7]与DMA、EM-MN、K-MEANS、LDA 以及 EDCM 等模型在簇数量 $\\pmb { K }$ 值准确的前提下，NMI分别为0.534、0.512、0.531、0.229、0.559、0.510;当 $\\pmb { K }$ 值不准确，如设 $K = 1 0$ ,NMI分别为 $0 . \\ 5 3 4 , 0 . \\ 5 1 2 , 0 . \\ 4 8 6 ,$ 0.184、0.482、0.401。从而显示，DPMFP在K值未知的条件下更为稳健。Nguyen等[5]采用stick-breaking基于vMF提出了DPMvMF模型,即假设混合系数 $\\pi = \\left\\{ \\pi _ { k } \\right\\} _ { k = 1 } ^ { \\infty }$ 是一个无穷序列：",
    "context5": "$$\n\\boldsymbol { v } _ { k } \\sim \\mathrm { B e a t } ( 1 , \\alpha ) , \\pi _ { k } ( \\boldsymbol { v } ) = \\pi _ { k } \\pi _ { j = 1 } ^ { k - 1 } ( 1 - \\nu _ { j } ) ,\n$$",
    "context6": "$$\nG \\ = \\ \\sum _ { k \\ = 1 } ^ { \\infty } \\ \\pi _ { k } \\delta _ { \\theta _ { k } } , \\theta _ { n } \\sim G _ { \\circ }\n$$",
    "context7": "$\\theta _ { 1 } ^ { * } , \\theta _ { 2 } ^ { * } , \\cdots , \\theta _ { k } ^ { * }$ 是参数集 $\\{ \\pmb \\theta _ { 1 } , \\pmb \\theta _ { 2 } , \\cdots , \\pmb \\theta _ { n } \\}$ 的不重复序列， $t _ { k }$ 表示参数 ${ \\pmb \\theta } _ { \\pmb { k } } ^ { \\ast }$ 在 $\\{ \\pmb \\theta _ { 1 } , \\pmb \\theta _ { 2 } , \\cdots , \\pmb \\theta _ { n } \\}$ 中出现的频次。DPMvMF在与基于MM的聚类算法、基于vMF的软分配算法、基于 $\\mathbf { v } \\mathbf { M } \\mathbf { F }$ 的DA算法、CLUTO等四个模型的对比实验中也得到了与Huang等[6.7]类似的结果。"
  },
  "4.3一点思考": {
    "context1": "DPM作为一种非参数贝叶斯模型，具有无需预先给定簇数目K以及建模更具灵活性与适应性等优点，在模式识别领域已有较多的研究与应用，但在自然语言处理领域仍处于起步阶段，尤其在混合模型文本聚类中还有一些有待解决的理论与应用问题。",
    "context2": "HDP。DP可以对一组（group）文本进行聚类与分析，但在聚类多组文本时则显得力不从心，Teh等[35]提出的层次Dirichlet过程（HierarchicalDirichlet Processe，HDP)较好地解决了该问题。 $\\mathbf { H } \\mathbf { u }$ 等[36]采用IHDP-HMM模型结合主题模型的思想进行了文本聚类的研究,与谱聚类、k-means、SOM等算法的聚类准确率分别为94.6、92.9、89.5、90.8,结果显示该算法具有一定的优势，因此，有必要加强其在混合模型文本聚类中的研究与应用。",
    "context3": "分配策略。DPM聚类算法属于硬分配算法，无法处理簇重合的情况。Griffiths等[37]提出的IndianBuffet Processes(IBP)能实现实例同时属于多个簇，而且更适用于稀疏矩阵。该模型在图像处理、网络分析、特征抽取等领域已有一定的应用,相应推理算法的研究已取得较为丰富的成果，但是在自然语言处理领域研究与应用甚少[38]。吸收 IBP的研究成果将其应用于混合模型文本聚类，从而实现DPM的软聚类将是一个重要的研究方向。",
    "context4": "层次语义聚类。经典的DPM为扁平式方法，《中图法》等诸多分类法的类目体系为层次语义关系，因此，聚类算法需要揭示文本集的层次主题结构。Neal 等[39]提出的 Dirichlet Diffusion Trees(DDT)基本思想是：数据点根据先前数据点的路径运行，根据风险函数形成分叉，最终生成二叉树。基于Pitman Yor process,Knowles 等[4o]提出的 Pitman-Yordiffusion tree(PYDT)基本思想是：新的实例在分支点可以遵循已有分支中的任意一支，也可以开始新的分支，从而可以实现聚类结果的多叉树展示。目前，基于DPM的层次语义聚类研究与应用尚少，有待进一步深人。",
    "context5": "模型整合。DP与其他模型和算法的整合研究与应用已经取得了较好的成效，比如Liang等[41]将HDP与PCFG相结合实现了句法分析中语法标签数量与过拟合间的平衡;Goldwater等[42]将DP与一元语言模型结合用于语音分段，避免了需要预先设定词形（word type)数量的问题,还将HDP与二元语言模型结合，实现上下文依赖的语音分段，在与 NGS的对比实验中，分段准确率分别为72.3、68.3,词形准确率分别为59.1、55.7；在 $\\mathbf { n }$ -gram语言模型中,n越大则模型的表征能力越强，但也更容易出现过度拟合， $\\mathrm { T e h } ^ { [ 4 3 ] }$ 基于Hierarchical Pitman-Yor Processes提出的 HPYCV方法取得了较好的效果。DP与语言模型相结合研究的成果理应为混合模型文本聚类提供新的视角。"
  },
  "5模型推理": "",
  "5.1 EM算法": {
    "context1": "在公式(2)的基础上,定义隐性变量 $Z = \\left\\{ z _ { 1 } \\right.$ ，$z _ { 2 } , \\cdots , z _ { N } \\} , z _ { n } \\in \\{ 1 , 2 , \\cdots , K \\} , z _ { n } = k$ 表示文本 $d _ { n }$ 所对应的簇为 $k$ ， $\\mid ( d _ { n } , z _ { n } ) \\mid _ { n = 1 } ^ { N }$ 代表完整的数据集，基于完整的对数极大似然公式（或是最大后验概率，公式11）[24]即可进行EM的迭代过程,EM算法的具体描述参见文献［1,2]。在混合模型文本聚类中EM算法多用于有限混合模型的推理,研究主要涉及以下几个方面。",
    "context2": "$$\nL ( \\theta \\mid D , z _ { n k } ) ~ = ~ \\sum _ { { n } = 1 } ^ { N } \\log { \\sum } _ { { k } = 1 } ^ { K } z _ { n k } { \\pi } _ { k } p _ { k } ( { d } _ { { n } } \\mid \\theta _ { k } ) p ( \\theta _ { k } )\n$$",
    "context3": "分配策略的影响。混合模型文本聚类中使用最多的分配策略是软分配、硬分配（CEM)以及决定性退火技术（Deterministic Annealing，DA）,分配策略的差异对混合模型存在不同的影响。Zhong等[]对三者进行了实验比较，结果发现大部分数据集中DA优于软分配，后者又优于硬分配，其中DA对vMF的改善更为明显，但在BM中反而有所下降。Meila等[44]比较了基于MM的软分配与硬分配，得到了与Zhong等[]类似的结果。据此，可以得到聚类算法的质量与分配的“柔软度（softness）”相关的推论。在时间复杂度上，Zhong[]对三种分配策略测试的结果显示DA的时间复杂度远高于另外两者，硬分配最低，对vMF的影响尤甚。目前， $z _ { n , k }$ 值的计算大多通过当前参数获得，改进的计算方法则可以通过当前的条件概率密度模拟得到，如随机EM（stochastic EM，SEM）[45]、模拟退火EM与蒙特卡洛$\\mathbf { E M } ^ { [ 4 6 ] }$ 等。",
    "context4": "$\\pmb { K }$ 值。 $K$ 值是混合模型聚类和传统分割式聚类都需面对的问题，已经形成了丰富的研究成果[47]。在混合模型文本聚类中通常也沿用以往的方法对K值进行估计，但为了更好地解决该问题，学界做出了三个方向的探索：一是在模型推理中引入簇的修剪机制，即初始化一个较大的K，经过不断修剪得到最终合适的值;比如,Zeng等[3]在迭代中将 $\\pmb { \\pi } _ { k }$ 值低于最小阈值的成分从模型中剔除，最终得到一个成分数量较为合理的簇结构，而且避免了传统EM的稀疏簇问题。Figueiredo等[48]基于类似的机制改变了传统EM算法的极大似然估计或最大后验概率估计，尝试采用最小信息长度（MML)准则来平衡模型复杂度与拟合质量，也得到了较好的结果。该方法能够自动选择簇的数目，且不需要精确的初始化，同时避免了参数空间的边际问题。二是采用基于混合模型的凝聚式聚类[21.44]或分割式聚类[49],通过层次式方法避免 $\\pmb { K }$ 值确定问题；三是将有限混合模型扩展为无限,使用DP对文本建模， $K$ 值作为一个未知参数存在，直到算法结束 $\\pmb { K }$ 值才最终确定，从而将 $\\pmb { K }$ 值转化为非参数估计问题。",
    "context5": "特征空间的影响。Rigouste等[13]的实验表明，特征维度的减少能降低模型推理结果的困惑度（Perplexity），并且能明显增强参数估计的稳定性，减少初始化条件对算法的影响。基于此，Rigouste等提出了两种策略：一是根据词频对词表进行分区，从最小分区开始在迭代过程中逐步增大词表规模；二是舍弃稀疏词，通过多次实验选择最佳结果所对应的词表。在后续研究中Rigouste等[5o]进一步尝试将两种策略相结合，得到了更优的结果。"
  },
  "5.2吉布斯抽样": {
    "context1": "作为马尔可夫链－蒙特卡洛方法族的一员，吉布斯抽样为DPM混合模型的推理提供了非常有效的方法[51],研究表明该方法在优化模型聚类效果方面优于EM算法:52,53],算法的具体描述参见文献[54]。Rigouste等[13]在基于多项式混合模型的文本聚类中运用了吉布斯抽样以及Rao-Blackwellized吉布斯抽样；结果证明,Rao-Blackwellized抽样效果优于吉布斯，且收敛更快;吉布斯抽样与EM算法相近,但后者更加依赖于初始化。Huang等[]在基于多项式分布的DPM文本聚类中运用了Blocked吉布斯抽样,该方法将特征组成若干块（Block）,在迭代过程中每次更新一个块中的特征，与吉布斯相比收敛速度有所提高，当特征间相互依赖时，该方法更有效率。",
    "context2": "吉布斯抽样由于灵活、简单且易于实现等特点被广泛应用，但也存在MCMC方法两个主要的不足： $\\textcircled{1}$ 收敛速度缓慢，计算花销大； $\\textcircled{2}$ 收敛状态难以确定[55]。尤其在处理大规模文本时算法时间复杂度更高，因而DPM的进一步推广有赖于推理算法的改进或者模型的简化。相关的探索有： $\\mathbf { L i u } ^ { [ s 6 ] }$ 提出的Collapsed吉布斯抽样，是对一部分随机特征进行积分从而加快收敛速度。Gilks等[]提出的基于DPM的split-merge 抽样算法实现了在一个步骤中批量更新多个变量，但该方法在更新变量的过程中并未使用马氏链中的所有信息，导致了收敛速度慢以及碎片类的问题。基于Collapsed吉布斯，丁轶群[58]提出了自适应分裂合并抽样算法（Adaptivesplit-mergesampler，ASM）并将其运用于文本主题建模，实证表明ASM能有效提高收敛速度，解决了split-merge抽样算法的碎片类问题，并且得到更优",
    "context3": "的聚类结果。"
  },
  "5.3变分推理": {
    "context1": "相对于EM、吉布斯抽样，变分推理是一种确定性近似推理方法，在混合模型文本聚类中应用较多的是均值场变分推理，算法的具体描述参见文献[59]。",
    "context2": "DPM中的应用。Huang等[7]在多项式分布的DPM文本聚类中将均值场变分[]与Blocked 吉布斯做了对比，聚类结果的NMI分别为0.945、0.979，变分方法准确率略低于抽样方法，时间维度吉布斯方法运行20次需要3小时，而变分方法只需10分钟，后者远优于前者;Nguyen等[5在DPM文本聚类中采用了一种改良的均值场变分方法，也得到了较好结果。变分推理除了时间方面的优势之外，计算也更为方便[61],在各种应用中表现出了较好的泛化性能，即无论参数估计还是非参数估计都能得到满意的结果[59]。相关研究的例子如,Fan等[32]基于狄利克雷分布的DPM（即无限狄利克雷混合模型）从stick-breaking模型的视角进行建模，提出运用变分方法进行模型推理，在仿真数据、图像检测、视频分类等数据集中的试验都得到了较好效果。Ma等[62]把伽马分布作为狄利克雷分布的参数先验，用若干相互独立的伽马分布近似参数的先验分布与后验分布，在仿生数据实验中该方法优于EM算法。",
    "context3": "在线变分推理。在线变分推理是变分方法的一个研究方向，最早用于LDA以及HDP主题模型[63,64]。在大规模流数据应用情境,学术界采用了随机最优化方法以降低算法的时间复杂度，即通过不断重复子抽样，仅根据获得的子数据集调整变分参数，从而避免在每次迭代过程中遍历所有数据点。Bryant等[65]进一步提出了split-merge在线变分推理，使得截断水平（truncationlevel）可以动态变化；Fan等[66]将特征选择技术[67]引人狄利克雷混合模型(DM)的在线变分推理，在文本聚类与图像聚类的实验中都取得了较好的效果。",
    "context4": "非共轭模型。均值场变分方法存在一些潜在的不足[59,68]： $\\textcircled{1}$ 无法识别隐藏变量间的关系； $\\textcircled{2}$ 后验方差被低估； $\\textcircled{3}$ 难以适用于非共轭模型。针对第三点,Wang等[69]针对特定非共轭模型提出了两种变分均值场近似,即Laplace 变分推理和delta方法变分推理;Knowles等[70]通过推导出更低的边界以近似所需的期望，提出了一种变分信息传播（variational message passing）算法;Paisley 等[71]使用由蒙特卡洛积分得到的具有较低边界倾斜度特征的随机近似完成参数推理，同时使用基于控制变量的方差衰减方法以减少所需用于构建随机搜寻方向的样本。"
  },
  "5.4一点思考": {
    "context1": "推理性能。由于EM操作简单且效果较好，在混合模型推理中取得了广泛的应用。在DPM推理中则多使用吉布斯抽样与变分推理。就性能而言，多数实验结果表明,吉布斯抽样与变分推理优于EM;当测试条件为小样本时，吉布斯抽样的推理效果优于变分推理，但面对大样本时,Gao等[72]的结果表明变分方法更能适应大规模数据集的情况；在时间复杂度层面，变分方法远远优于吉布斯抽样。",
    "context2": "非批量式算法。经典的推理算法均为批量式方法，即在每次迭代过程中都需要遍历完整数据集对模型参数进行更新，当面对大规模高维度数据集时，时间复杂度较高，且推理效果没有必然优势。Zhong[73]认为在线参数更新能够改善模型效果，且更适合于数据流环境，Nigam等[74]研究表明增量式方法比批量式方法效果更好，Rigouste等[13]的特征增量式EM算法也证明了这一点。这些研究启示可以从增量式视角改进混合模型聚类研究。",
    "context3": "初始化。推理算法首先需要对模型参数赋予初值，不同的初始化可能导致不同的结果，尤其对依赖初始化条件的EM。目前仍没有普遍有效的初始化方法，Meila等[44]对比了随机方法、边际似然对数以及凝聚聚类初始化方法在EM文本聚类中的效果：在仿真数据中，随机方法劣于另外两种;在真实数据上，三者无明显差异；凝聚聚类与边际似然对数方法性能相仿，但后者更有效率，故边际似然对数是最优的选择。Zhong[75]对比了随机方法、边际似然对数、PERTUB以及KKZ初始化方法，结论显示边际似然对数和PERTUB与随机方法性能伯仲，KKZ优于前面三者。不过，多数实验研究中仍主要采取随机方法，具体做法是在相同条件下以不同的初始化值反复实验，最终取结果的均值或最优结果。"
  },
  "6总结": {
    "context1": "从文本建模、参数建模、模型推理的研究文献中可以发现：参数建模中的狄利克雷过程可以应用于连续型以及离散型混合模型,模型推理中的EM算法、变分推理以及马尔可夫链－蒙特卡洛方法也可以应用于两类文本建模方法之中，从而展现出三者间交叉组合的技术特点，也突显出混合模型聚类的灵活性。Zhong[]比较了EM算法在不同模型中的性能,Fraley 等[52.53]比较了推理算法在单一文本建模方法中的性能，三种推理算法在两类文本建模中的性能比较尚缺乏实证证据，将成为本文进一步的研究方向。此外，在文本建模、参数建模以及模型推理研究中尚存在一些共性的问题。"
  },
  "(1)特征选取": {
    "context1": "在自然语言处理的早期，文本特征通常表示为词袋模型，即将其看作是一系列独立项的集合，忽略其词序、语法与句法，项相互独立，即一元语言模型。研究者已经发现一元项对主题的表征能力有限，吴夙慧等对此进行了系统的梳理[76]。在混合模型文本聚类中，多数研究者仍主要采取一元模型，文本表示的相关探索甚少，少量的研究如Liu等[25]基于GMM同时使用项、命名实体以及词组构建文本特征向量，相比使用单一的项聚类效果有所提高。该研究提示，在混合模型文本聚类中可以借鉴传统聚类通过短语、项共现、主题等从语义层面揭示文本内容、提高文本表征准度的做法。在文本内容特征之外,也可以考虑诸如用户标注[77]、链接或引用等相关信息，从多特征的视角选取更丰富的文本表征信息。"
  },
  "(2)特征降维": {
    "context1": "文本集大多是稀疏的大规模、高维度数据，混合模型文本聚类在高维空间的表现往往并不理想[78],在实际应用中需要借助特征选择与特征抽取等降维技术减少特征维度。",
    "context2": "特征抽取。在文本聚类领域广泛使用的特征抽取方法包括LSA、PLSA、LDA等。Masada等[79]比较了LDA、PLSA以及随机映射的降维效果,实验表明LDA与PLSA比随机映射提供了更好的聚类结果，但二者之间效果差异不大，LDA模型推理的时间复杂度较大。Nguyen等[8]提出了基于项聚类的降维思路，该方法与文本频率（DF）、项共现（TC）、LSA等方法比较显示：其效果远优于DF和TC，与LSA相差不大，且在部分试验中优于LSA;该方法的不足是对初始化较为敏感，与LSA的共同点是子主题的数量需事先确定。Pessiot等[7]认为在同一上下文中出现相同频次的共现项则语义相关，以此对特征项进行分类实现降维，基于该方法的MM与原始MM、基于PLSA的MM、PLSA以及LDA在NG20测试集上平均准确率与运行时间分别为 $0 . 4 \\AA , 2 0 \\mathrm { m i n }$ ，",
    "context3": "$0 . 3 5 \\ 、 1 0 \\operatorname* { m i n } , 0 . 4 \\AA , 5 \\operatorname* { m i n } , 0 . 4 \\AA , 3 \\operatorname* { m i n } , 0 . 3 2 \\AA , 3 0 \\operatorname* { m i n }$ ,该方法与PLSA聚类准确性相当，但不具备时间优势，从而提示PLSA的特征抽取效果总体最优。",
    "context4": "特征选择。常见的特征选择方法有文本频率（DF）、项共现（TC）、项强度（TS）以及信息熵（En）等。与传统聚类算法不同，基于混合模型的聚类可以把特征选择转换为模型推理问题，即引入一个隐性二分向量 $\\gamma = \\{ \\gamma _ { \\scriptscriptstyle 1 } , \\gamma _ { \\scriptscriptstyle 2 } , \\cdots , \\gamma _ { \\scriptscriptstyle w } \\}$ 用于区分相关特征与非相关特征。Huang等[6.7]在多项式混合文本聚类中假设判别词与非判别词之间不相关，然后分别对其进行特征建模，实证结果显示采用该特征选择方法的聚类结果在大部分数据集上优于EDCM以及MM。Huang等[6.7]的方法都默认不同的簇中特征的显著程度均一致，即对所有簇都选择相同的特征子集。但因为每个簇实际的主题差异，其中的特征分布不尽相同，相关的特征子集也不同，因而需要采用子空间聚类。 $\\mathbf { L i } ^ { [ 1 2 ] }$ 基于MM将指示变量 $\\gamma _ { \\ast }$ 细化为 $\\gamma _ { w k }$ ，对于每个 $w \\ = \\ 1 , \\ 2 , \\ \\cdots , \\ W ,$ $\\gamma _ { w k } \\quad =$ [1,第 $\\pmb { w }$ 个特征在簇 $\\pmb { k }$ 中为相关特征结果显0,第 $\\pmb { w }$ 个特征在簇 $k$ 中为不相关特征示，其聚类效果显著优于K-means以及谱聚类。",
    "context5": "上述研究可见，特征选择与特征抽取方法在实现降维的同时都不同程度地改进了聚类效果；特征抽取的相关研究结果显示PLSA以及LSA的聚类效果更值得期待；Bouveyron等[24]研究指出“希望读者确信,使用简约模型、子空间聚类或者变量选择方法而非用降维进行预处理”，从而提示应优先选择特征选择而非特征抽取方法。"
  },
  "(3)半监督聚类": {
    "context1": "文本往往包含部分标签，或是其他约束条件，这类附加信息在半监督聚类中可以作为种子（即初始化条件）、限制(聚类过程中标签不变)或是反馈（聚类结束后根据标签进行调整)等用于改善聚类质量。Basu等[81]基于skmeans的对比实验发现，限制方法与种子方法效果相当;Zhong[75]基于MM的实验表明，在标签完整的情况下，限制方法优于其他两种；当标签不完整，甚至无法包括所有簇的情况下，反馈方法聚类效果更优。半监督聚类的重点在于发现未标签文本中的新簇，Zhong[75]提出了采用多元聚类或是条件信息瓶颈方法的思想。两项研究中使用的特定种子、限制或是反馈方法只是众多可能的设计之一，后续的研究中可以考虑采用其他特定方法或是将多种方法融合应用。"
  },
  "(4)聚类过程的系统整合": {
    "context1": "相关研究表明，特征选择 $\\mathbf { \\nabla } _ { \\mathbf { \\nu } _ { \\mathbf { \\lambda } } } K$ 值确定以及模型推理等几个步骤之间存在相互影响[82]，因而，将特征选择、 $K$ 值确定融合到模型推理过程中，在模型迭代推理的同时特征空间以及 $\\pmb { K }$ 值也相应地调整变化，将模型推理的过程整合为一个动态系统。Law等[4]、Kersten等[83]将 $\\pmb { K }$ 值确定、特征选择融入到了EM算法中,在与 SFFS-EM[84]、 $\\mathbf { R S E M } ^ { [ 8 5 ] }$ 等方法的比较中，其运行时间以及聚类质量都有较好的表现；Constantinopoulos等[8]在变分推理中将模型选择与特征选择相结合，同时得到聚类个数与特征显著性程度，实验结果表明该方法在高维稀疏数据集上更加稳健。变分方法在模型推理中融入了模型选择与特征选择，例如Fan等[87]把基于簇分割的 $\\pmb { K }$ 值选择[88]融合到DM的变分推理中，在文本聚类的实验中该方法的聚类准确率优于其他。上述研究显示聚类过程的系统整合能够有效的提高聚类效果，不过该领域的研究与实验尚少，有待深人探讨。",
    "context2": "总而言之，混合模型文本聚类中使用的基础模型主要是BM、MM以及vMF，在研究与应用中为了提高建模的精度或增加模型功能，需要对基础模型进行优化。具体的研究路径可以是模型的复杂化，例如引入新参数用于特征选择，增加模型层次进行参数建模，或是不同模型的综合应用等。目前，混合模型都受到严格的独立性假设的制约，对簇之间、文本之间以及特征之间的依赖关系建模是进一步的研究方向。另一条研究路径是模型的简化，随着模型越来越“精密”，推理计算的可行性以及易行性是必须面对的问题,例如vMF中参数 $\\pmb { \\kappa }$ 以及DPM，在提高计算效率的同时，增加模型的稳定性。此外，更加开拓性的方向是尝试新的混合模型，根据文本的性质,在vMF的基础上进一步对球形分布[89]进行拓展研究。"
  },
  "参考文献": {
    "context1": "[1]Zhong S,Ghosh J. Generative model-based document clustering:A comparative study[J]．Knowledge and Information Systems，2005,8(3） :374-384.   \n[2]Dempster AP,Laird N M,Rubin D B.Maximum likelihood from incomplete data via the EM algorithm （with discussion）[J]. Journal of the Royal Statistical Society, y.SeriesB（Methodological），1977,39(1）:1-38.   \n[3] Zeng Hong, Cheung Yiu-ming. Learning a mixture model for clustering with the completed likelihood minimum message length criterion[J].Pattern Recognition,2014, 47(5):2011-2030.   \n[4」Law M H C,Figueiredo M A T, Jain A K, Simultaneous feature selection and clustering using mixture models[J]. IEEE Trans.Pattern Anal. Mach．Intell,2004,26(9）: 1154-1166.   \n[5] Nguyen KimAnh,Nguyen The Tam,Ngo Van Linh.Document Clustering using Dirichlet Process Mixture Model of von Mises-Fisher Distributions ［C]//Proceedingsofthe Fourth Symposium on'Information and Communication Technology.New York,USA,2013:131-138.   \n[6]Yu G,Huang R,Wang Z.Document Clustering viaDirichlet Process Mixture Model with Feature Selection $[ \\mathrm { ~ C ~ } ] / /$ Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New York,USA,2010:763-772.   \n[7] Huang Ruizhang,Yu Guan，Wang Zhaojun，et al. Dirichlet processmixture model for document clustering with feature partition ［J]． IEEE Transactionson Knowledge and Data Enginering，2013，25（8）： 1748-1759.   \n[8] Nigam K,McCallum A K,Thrun S,et al.Text classification from labeled and unlabeled documents using EM[J]. Machine Learning,2000,39(2-3）:103-134.   \n[9] Andrew McCallum,Kamal Nigam. A comparsion of event models for naive Bayes text classification[C]//AAAI-98 Workshop On Learning For Text Categorization,Madison, Wisconsin,USA，1998:41-48.   \n[10] Bouguila N. On multivariate binary data clustering and feature weighting[J].Computational Statistics and Data Analysis,2010,54(1):120-134.   \n[11] Zhu S,Takigawa I,Zhang S,et al.A probabilistic model for clustering text documents with multiple fields[M]. Berlin Heidelberg：Springer,2007.   \n[12] Li Minqiang,Zhang Liang. Multinomial mixture model with feature selection for text clustering[J]．KnowledgeBased Systems,2008,21(7):704-708.   \n[13] Rigouste L,Capp'e O,Yvon F.Evaluation of a Probabilistic Method for Unsupervised Text Clustering $[ \\textbf { C } ] / /$ International Symposium on Applied Stochastic Models and Data Analysis.Brest,France,2005:114-123.   \n[14] Li X,Yu G，Wang D.MMPClust:A skew prevention algorithm for model-based document clustering[C]// Database Systems for Advanced Applications. Springer Berlin Heidelberg，2005：536-547.   \n[15] Salton G,McGill M J.Introduction to Modern Retrieval [M].2nd.New York:McGraw-Hill Book Company,1983.   \n[16] Dhillon I S，Modha D S.Concept decompositions for large sparse text data using clustering[J]. Machine Learning,2001,42(1):143-175.   \n[17] Banerjee A,Ghosh J.Frequency sensitive competitive learning for clustering on high-dimensional hyperspheres [J].IEEE Transactions on Neural Networks,2004,15 (3):702-719.   \n[18] Banerjee A,Dhillen I,Ghosh J,et al.Generative Modelbased Clustering of Directional Data[C]//Proceedings of the ninth ACM SIGKDD international conference on Knowledge discoveryanddata mining.New York，USA, 2003:19-28.   \n[19] Mardia KV，JuppPE.Directional statistics[M]．New York:John Wiley& Sons,2009.   \n[20] Suvrit Sra.A short note on parameter approximation for von Mises-Fisher distributions:and afast implementation of Is(x）[J].Comput Stat,2012,27(1）:177-190.   \n[21] Vaithyanathan S,Dom B.Model-based hierarchical clustering [C]//Proc.16th Conf.Uncertainty in Artificial Intelligence.San Francisco,CA,USA,200O:599-608.   \n[22] Novoviv& J,Malik A.Application of multinomial mixture model to text classification[M]//Pattern Recognition and Image Analysis.Berlin Heidelberg:Springer, 2003:646-653.   \n[23] Volodymyr Melnykov .Finite mixture models and modelbased clustering[J].Statistics Surveys，2010,4： 80-116.   \n[24] Bouveyron Charles,Camille Brunet-Saumard. Model-based clustering of high-dimensional data_ A review[J]. Computational Statistics and Data Analysis，2014,71: 52-78   \n[25] Liu X,Gong Y, Xu W,et al. Document clustering with cluster refinement and model selection capabilities [C]//Proc.25th Annual International ACM SIGIR Conf.on Research and Development in Information Retrieval. New York,USA,2002:191-198.   \n[26] Madsen R，Kauchak D，Elkan C.Modeling Word Burstiness Using the Dirichlet Distribution[C]//Proc. Int'l Conf.Machine Learning.New York，USA,2005： 545-552.   \n[27] Blei D,Ng AY,Jordan MI.Latent dirichlet allocation [J].Journal of Machine Learning Research,2003,3： 993-1022.   \n[28] Minka T.Estimating a Dirichlet distribution[OL]. http://research.microsoft.com/en-us/um/people/ minka/papers/dirichlet/.[2014-7-22].   \n[29] Elkan C.Clustering Documents with an Exponential-Family Approximation of the Dirichlet Compound Multinomial Distribution[C]//Proceedings of the ${ \\tt 2 3 r d }$ international conferenceonMachinelearning. New York， USA,2006： 289-296.   \n[30]Bouguila N,Ziou D.High-dimensional unsupervised selection and estimation of a finite generalized Dirichlet mixture model based on minimum message length[J]. Patern Analysis and Machine Intelligence， IEEE Transactions on,2007,29(10):1716-1731.   \n[31] Bouguila N.Clustering of Count Data Using Generalized DirichletMultinomialDistributions[J]．IEEE transactions on knowledge and data engineering,,2008, 20( 4) :462-474.   \n[32] Fan Wentao,Bouguila Nizar. Variational learning for Dirichlet process mixtures of Dirichlet distributions and applications[J].Multimed Tools Appl,2014,70(3）： 1685-1702.   \n[33] Blackwell D,MacQueen J.Fergusondistribution via Polya urn schemes[J]. The Annals of Statistics,1973,1(2): 353-355.   \n[34] Green P J,Richardson S. Modelling Heterogeneity with and without the DirichletProcess[J]. Seandinavian J. Statistics,2001,28(2):355-377.   \n[35] Teh Y W，Jordan M I,Beal MJ,et al.Hierarchical Dirichlet processes [J].Journal of the American Statistical Association,2006,101(476）:1566-1581.   \n[36] Hu Weiming,Tian Guodong,Li Xi,et al．An Improved Hierarchical Dirichlet Process-Hidden Markov Model and Its Application to Trajectory Modeling and Retrieval [J]．Int J Comput Vis,2013,105:246-268.   \n[37] Griffiths TL,Ghahramani Z.Infinite latent feature models and the indian buffet process[R].London:Gatsby. Computational Neuroscience Unit,2005.   \n[38] Grifths TL,Ghahramani Z. The indian bufet process： an introduction and review[J]． Journal of Machine Learning Research,2011,12:1185-1224.   \n[39] Neal R M.Density modeling and clustering using Dirichlet diffusion trees[J]．Bayesian Statistics，2003，7： 619-629.   \n[40] Knowles D A,Ghahramani Z.Pitman-Yor diffusion trees [J]．arXiv preprint arXiv:1106.2494,2011.   \n[41] Liang P,Petrov S,Jordan MI,et al.The Infinite PCFG Using Hierarchical Dirichlet Processes[C]//EMNLPCoNLL.Prague,Czech Republic,2007：688-697.   \n[42] Sharon G,Griffiths TL,Johnson M.A Bayesian framework for word segmentation：Exploring the effects of context [J].Cognition,2009,112：21-54.   \n[43] Teh Y W.A hierarchical Bayesian language model based on Pitman-Yor processes [ C]//Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. USA： Associationfor"
  }
}