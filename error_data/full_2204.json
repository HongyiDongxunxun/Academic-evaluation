{
  "original_filename": "full_2204.md",
  "关键词提取研究综述\\*": {
    "context1": "胡少虎张颖怡章成志（南京理工大学经济管理学院 南京 210094)",
    "context2": "摘要：【目的】对关键词提取研究的主要方法、相关特征以及评价方法进行总结梳理,为后续的关键词提取研究提供借鉴。【文献范围】以“Keyword Extraction”、“Keyword Generation” 、“Keyphrase Extraction”、“Keyphrase Generation”、“关键词抽取”、“关键词生成\"等检索式在Web of Science、DBLP、Engineering Index、Google Scholar、CNKI和万方等数据库进行检索,结合个人积累与文献溯源得到代表性文献89篇。【方法】梳理关键词提取的发展脉络,从研究方法、相关特征与评价方法三个主要方面对关键词提取的相关研究进行深入分析与总结。【结果】关键词提取方法随着机器学习技术的发展,逐步从特征驱动的模型转向数据驱动的模型,并面临数据标注、评价规范等问题。【局限】更为关注关键词提取研究中主流的方法。【结论】本文通过对关键词提取方法,尤其是关键词生成方法进行总结,阐明了关键词提取方法的研究重心从特征转向数据的趋势与原因,并指出现有关键词提取评价体系所存在的缺陷。",
    "context3": "关键词：关键词提取关键词抽取关键词生成 分类号：TP393G250 DOI: 10.11925/infotech.2096-3467.2020.1103",
    "context4": "引用本文：胡少虎，张颖怡，章成志.关键词提取研究综述[J].数据分析与知识发现，2021,5(3)：45-59.(Hu Shaohu, Zhang Yingyi, Zhang Chengzhi. Review of Keyword Extraction Studies[J]. Data Analysis andKnowledge Discovery,2021, 5(3): 45-59.)"
  },
  "1引言": {
    "context1": "关键词通常为一个或多个能够描述文档主题信息的词语或词组[1]。关键词的自动提取技术指通过一定的规则或数理模型等手段获得文档的关键词，该技术在文本挖掘领域被称为关键词提取（KeywordExtraction）,而在信息检索领域被称为自动标引（Automatic Indexing)[2]。一般来说,可以将其细分为关键词抽取技术（Extractive KeyphraseExtraction）与关键词生成技术（AbstractiveKeyphraseGeneration）。其中,关键词抽取技术指从文档中筛选得到能表达文档主题的单词(词组),该关键词必然在文档中出现，而关键词生成技术指从词表中选择与文档主题相近的单词(词组)作为该文档的关键词,与该关键词是否在文档中出现无关。",
    "context2": "随着自然语言处理研究的逐步深人，关键词作为表达文档主题意义的最小单位[3在多项自然语言处理任务诸如文本摘要[4、文本分类[5中都发挥了重要的作用。与此同时，呈指数型增长的文本数据使得传统的人工标注手段已无法满足社会的实际需求，所以关键词的自动提取技术已成为学术界与工业界共同关注的问题。",
    "context3": "自Luhn在1957年首次提出基于词频的关键词自动抽取方法以来[6,关键词的自动提取方法历经60多年的发展已经衍生出许多类别。已有诸多学者对关键词提取方法进行梳理[2-3.7-9],但他们大多着重于抽取式的关键词提取方法，很少设置专门的章节对生成式的关键词提取方法进行总结。这并不是由于生成式的关键词提取不重要,Meng等在实验中发现学术论文中作者给出的关键词中有近半数并不会在正文中出现[10]。诸多研究集中于抽取式关键词提取的主要原因是生成关键词的难度相较抽取来说更大，一直缺少成熟的方法来解决这一问题。随着深度学习技术的发展,生成模型的性能得到极大提升，越来越多的研究开始专注于生成式的关键词提取。",
    "context4": "本文分别对抽取式与生成式的关键词提取方法进行综述，分析各类关键词抽取方法的主要思想与发展脉络。首先对抽取式与生成式关键词提取方法的一般模式进行总结,之后对各子类方法进行梳理，并分析不同方法的优缺点。接着，对关键词提取任务中常见的特征进行总结并介绍关键词提取中主流的评价方法，最后对关键词提取方法进行总结与展望。本文的主要贡献如下。",
    "context5": "(1)从关键词抽取与关键词生成两个方面对关键词提取研究进行综述。",
    "context6": "(2)总结关键词提取方法的一般模式,并依此对不同的研究方法进行分析。",
    "context7": "(3)结合关键词特征的发展对关键词提取方法的演化趋势进行分析。"
  },
  "2关键词抽取研究概述": {
    "context1": "文档是词语的集合，而关键词是其中与文档主题最为契合的若干个词语或是词组,因而关键词抽取的任务是从文档中筛选得到这些词。本文将关键词抽取方法分为基于非监督的方法与基于有监督的方法两类。"
  },
  "2.1基于非监督的关键词抽取方法": {
    "context1": "基于非监督的关键词抽取方法最早应用于关键词提取任务。历经多年的发展，该类方法在关键词提取任务中仍然占据重要的地位。本节先总结该类研究方法的一般模式,之后对各子类的研究方法进行介绍与分析。",
    "context2": "（1）基于非监督的关键词抽取的模式概述",
    "context3": "非监督的关键词抽取方法出现得最早,种类也最多，但是在抽取关键词的过程中它们存在相通之处。基于非监督的关键词抽取方法的一般模式如图1所示。",
    "context4": "![](images/c337d333a7a34fddbd922e72e1d4635f9bd749655a07915cad14887697883fab.jpg)  \n图1非监督方法一般模式  \nFig.1General Pattern of Unsupervised Methods",
    "context5": "非监督的关键词抽取方法一般都遵循上述模式对文档进行关键词抽取。",
    "context6": "$\\textcircled{1}$ 文本预处理。该步骤的目的是将文本切分为较小的粒度,并删去与文档主题关联不大的词汇，主要包括分词、去停用词等操作。",
    "context7": "$\\textcircled{2}$ 确定候选词集。这一步骤的目的是从文本中筛选出可能成为关键词的单词或者短语,常见的筛选方法主要有三类：第一类是基于N-gram选择出现频率较高的单词或短语;第二类是基于外部资源如维基百科来选择有价值的单词;第三类是基于一些指标如词频-逆文档频率(Term Frequency-InverseDocument Frequency,TF-IDF）、词性、单词所处文档中的位置对候选词进行筛选。",
    "context8": "需要指出的是，在早期的一些工作中步骤 $\\textcircled{2}$ 常被略去,即将所有词汇都视作候选词。",
    "context9": "$\\textcircled{3}$ 候选词排序。在确定关键词的候选词集后，就需要从中选择与文档主题更为契合的候选词作为关键词。一般是通过设定一系列的指标对候选词的重要程度进行量化并排序，然后将候选词的排名先后作为依据来筛选关键词。",
    "context10": "$\\textcircled{4}$ 评估。在获得文档的关键词之后，需要对该方法的关键词提取效果进行评估，专家学者提出了多种评估方法，这一部分的内容将在第5节中详细展开。"
  },
  "（2）基于简单统计的方法": {
    "context1": "基于简单统计的方法是对候选词的一些特定指标进行统计，然后根据统计的结果将候选词进行排序。在该方法中的指标可以分为两类，第一类包括以N-gram[1]、TF-IDF[12]、词频[6]、词共现[13]等为代表的用以评价单词在文档中重要程度的指标，但是这类指标忽略了单词自身的属性。因此，有学者提出使用单词的词性[14]、在文档中出现的位置[15]等指标为单词设置不同的权重。",
    "context2": "基于简单统计的方法的优势在于简单易用，计算量低,但其劣势也比较明显：",
    "context3": "$\\textcircled{1}$ 该方法的适用性较差，由于许多指标是基于文档自身的特性计算的，所以同样的方法在不同数据集上的效果可能会出现巨大差异[16]。",
    "context4": "$\\textcircled{2}$ 该方法准确率不高，因为各种指标的实际作用等价于规则，而这些规则源于专家对于语言规则的认识，这就使得这类方法只能涵盖单词之间较为浅层的语义信息，难以发现单词之间更深层次的关联。"
  },
  "（3）基于图的方法": {
    "context1": "基于图的方法的主要思想是将文档中的候选词视为一个个节点，然后按照一定的规则建立节点间的联系，最后通过计算每个节点的权重为其排序从而得到文档的关键词。由此可以看出基于图的模型中最为关键的三个要素：节点，即候选词;构建节点之间连接的规则;节点间的权重计算方法。",
    "context2": "基于图的方法的设计与改进也基本围绕这三个要素展开,按照不同的图模型设计思路可以将其细分为以下4类：基于统计的方法、基于外部资源的方法、基于主题的方法以及基于语义的方法。"
  },
  "$\\textcircled{1}$ 基于统计的方法": {
    "context1": "TextRank是基于统计的图模型中最为典型的代表,通过词性标签筛选出文本中的形容词与名词,然后为在同个文本窗口中出现的候选词之间建立边，最后赋予每个节点相同的初始值并运行PageRank算法直至收敛[17]。该方法的缺点在于只要两个候选词在同一个文本窗口中出现,就认为它们之间存在关联,但是该关联的强弱却没有得到很好的区分，即共现一次与共现多次的关联强度被认为是相等的。为此，Wan等提出了SingleRank,用单词之间的共现次数来度量各节点间边的权重[18]。基于这种想法，SGRank使用单词的首次出现位置、词长等统计指标为候选词的边赋值[19],而PositionRank对单词的位置信息不再局限于首次出现,而是考虑单词在文档中出现的所有位置[20]。综上可以看出,基于统计的图模型的关键思想是认为候选词之间的关联应当有强弱的区分，并尝试用各种统计指标来度量节点间的关联强弱，进而得到更好的图模型。",
    "context2": "$\\textcircled{2}$ 基于外部资源的方法",
    "context3": "TextRank、SingleRank 等方法针对每一个文档构建了一个无向图,并从中选出文档的关键词。这种方法存在一定的风险，因为每个文档的网络图都是根据其自身信息构建的,所以当文档的信息十分匮乏时,图模型的效果也会大打折扣。为此,Wan等提出ExpandRank方法，该方法利用与目标文档相近的文档来辅助构建词图,即节点间边的权重是从一系列文档中计算得到的[18]。与之相似的,Gollapalli等提出CiteTextRank方法，该方法在计算边权重时利用论文的引文上下文来对目标论文的信息进行补充[21]。这一类方法的主要思想是通过外部资源来弥补文档自身信息的不足，在小文档里能够发挥较好的作用。",
    "context4": "$\\textcircled{3}$ 基于主题的方法",
    "context5": "基于主题的方法是将候选词按照不同的主题进行划分，然后选择各个主题中的中心词作为文档的关键词。Liu等通过单词之间的共现频次与维基百科来计算候选词之间的相似度并将候选词进行聚类,之后按照文档所包含主题分配相应主题的核心词[22]。该方法的缺陷在于文档所涵盖的不同主题的重要性被认为是相等的,因此会抽取出大量与文档主题关联较弱的单词。Bougouin等提出TopicRank,目的是借助图模型为文档中不同主题的候选词进行排序，在提取关键词时仅考虑与文档主题关系密切的候选词[23]。MultipartiteRank则是在此基础上加入了单词的位置信息与一些启发式的规则[24]",
    "context6": "上述方法是通过聚类来实现单词主题的划分，而TopicalPageRank(TPR)则是使用LDA主题模型来计算单词在各个主题下出现的概率,并在每个主题下运行PageRank算法，最终得到每个主题下最核心的单词[4]。由于TPR方法的计算量过于庞大,Sterckx等提出 Single-TPR方法，区别于TPR考虑每个单词在每个主题下的概率,Single-TPR只需要考虑每个单词在所有主题下的概率，在不显著影响模型性能的情况下极大地降低了计算量[25]SalienceRank也采用相同的方法对TPR做了改进，而且通过计算单词在不同主题下的特异性避免了与主题相关性较弱的候选词干扰[26]。",
    "context7": "$\\textcircled{4}$ 基于语义的方法",
    "context8": "基于语义的方法可以分为两类。第一类在计算边权重时考虑候选词之间的语义关联。如Wang等使用文献[27]在维基百科上训练得到的词向量，通过词向量间的欧氏距离描述候选词之间的语义相关性,并在计算边权重时加以考虑[28]。同样,可以利用候选词的语义相似性生成文档的关键词[29]。区别于上述使用外部资源计算语义的研究,Mahata等使用FastText算法在特定领域的语料上训练词向量并取得了更好的结果[30]。",
    "context9": "上述基于图的方法在构图时均通过设定文本窗口来建立单词之间的联系，但这会导致在文本中距离较远的候选词间即使存在语义关联，也没有边将它们联系起来。第二类基于语义的方法通过引入外部网络的方式解决这一问题。Shi等首先基于候选词间的语义相似度将其进行聚类，同时建立候选词在知识图谱中的映射,最后在此基础上构建候选词的图网络[31]。Yu等提出的WikiRank与之类似,区别在于他们用于建立候选词间连接的媒介由知识图谱改为维基百科[32]。"
  },
  "(4)基于语言模型的方法": {
    "context1": "N-gram模型在非监督的基于语言模型的方法中有着重要地位。Tomokiyo等在目标文档与背景文档中分别使用Bigram与N-gram构建了4个语言模型，然后通过KL散度计算模型间的信息损失，单词的信息量可以用其在不同模型上的差异表示，最后按照每个短语的词组性与信息量来筛选关键词[3]。该方法建立在一个假设之上，即包含信息越多的候选词就越可能是关键词。该假设带有一定的主观性,且该研究也没有提供相应的评估方法，所以很难评价其实际效果。"
  },
  "2.2基于有监督的关键词抽取方法": {
    "context1": "基于有监督的关键词抽取方法由于其良好的抽取效果而受到广泛关注,机器学习与深度学习的快速发展更是为其注人了新的活力。",
    "context2": "（1）基于有监督的关键词抽取的模式概述",
    "context3": "基于有监督的关键词抽取方法的一般模式大致可以分为两类。第一类将关键词抽取视为分类任务，即通过分类模型将候选词分为关键词与非关键词两类,其模式如图2所示。",
    "context4": "![](images/3803006e73712f53dc4a06c0c2698b88302594c751cff46bde9c8feab90fc166.jpg)  \n图2关键词分类一般模式  \nFig.2General Pattern of Keyword Classification",
    "context5": "与非监督的方法不同，有监督的方法需要事先给定一个带标签的数据集对模型进行训练,因而包含数据集构建环节。此外,需要说明的是在部分研究中会出现筛选候选词的环节,但在本文给出的关键词分类一般模式图中将其略去。这是由于在基于有监督的方法中这一环节并不是必需的,研究者更倾向于通过词频、词性等特征让模型来学习关键词所具备的特性。在这一模式中，特征构建与模型分类是影响分类模型结果的两个重要环节,这一类型的研究基本都是基于这两个环节对模型进行改进。",
    "context6": "第二类有监督的关键词抽取方法将其视为序列标注任务，即利用序列标注模型学习已标注关键词的句子序列中单词之间的关系，进而为未标注的句子序列进行标注,抽取出句子中的关键词,其一般模式如图3所示。",
    "context7": "![](images/8554c5584702421992083e5fda6dd631c8386818aa0252e85d4533b581685654.jpg)  \n图3面向关键词提取的序列标注模式 Fig.3Sequential Tagging Pattern for Keyword Extraction",
    "context8": "在分类模型中，单词被视作一个个独立的个体，一个单词是否被判断为关键词取决于其自身的特征属性，显然这种想法是存在局限的。如果单词的上下文发生了变化,那么它在整个句子中的重要程度也应当随之改变。序列标注模型的特点即为在考虑当前节点状态时会综合前文所传递的信息，能够兼顾单词自身的特征属性与上下文构成的语境信息。因此,序列标注模型是目前较为主流的关键词抽取方法之一。此外,相较于关键词分类的一般模式,序"
  },
  "48 数据分析与知识发现": {
    "context1": "列标注模式中增加了词典构建与文本编码环节，其主要目的是对文本进行编码从而使文本变成数字化的序列。"
  },
  "（2）基于传统机器学习的方法": {
    "context1": "相较于基于非监督的关键词抽取方法,基于机器学习的方法流程更为标准规范,在不同数据集上的适应能力更强且往往具有较好的实验结果，所以许多研究注重于使用机器学习方法来抽取关键词。",
    "context2": "1999年,Frank等提出KEA方法，该方法使用朴素贝叶斯模型(NaiveBayes,NB)对候选词进行分类,使用的特征有单词的TF-IDF值与位置信息[34]。2000 年,Turney在关键词抽取任务中对比了遗传算法与C4.5决策树的效果[1]。2005年，Wang等采用支持向量机（SupportVector Machine,SVM)筛选关键词,使用的特征包括单词的词频与位置信息[35]。2008年，Zhang等使用条件随机场（ConditionalRandomField,CRF)尝试抽取中文文档中的关键词,采用的特征有词频、位置信息与词性等[36]。2011年，Ding等选择单词的TF-IDF值、位置信息、文档覆盖率等特征,采用二元整数规划方法(Binary IntegerProgramming,BIP)来抽取关键词[37]。2015年，Haddoud等使用逻辑回归算法来解决关键词提取任务,采用的特征包括单词的词长、词频等[38]。",
    "context3": "以上为基于传统机器学习的关键词抽取方法中较有代表性的，后续有许多研究对其进行了改进。本文将这些改进总结为两个类别：特征改进与模型改进,分别对应图2中的特征构建与模型分类环节。"
  },
  "$\\textcircled{1}$ 特征改进": {
    "context1": "在基于传统机器学习的关键词抽取方法中，特征改进主要表现在两个方面：特征数量的增加与特征复杂性的提高。如Turney在NB模型中加入点互信息（Point-wise Mutual Information,PMI)以表现单词间的语义信息[39]。Nguyen等加入单词的词性与后缀等特征[40]。毛伊岛系统(Maui)在此基础上加入了源于外部资源(维基百科)的特征[41]。用于关键词抽取的特征由最初的词频和相对位置逐渐发展为一个庞大的集合，与此同时，特征的复杂度也随之增加。例如早期研究中经常使用的词频特征,现在大多被单词的TF-IDF值所替代,并衍生出了更为复杂的对数TF-IDF（logTF-IDF)[42]、布尔TF-IDF（TF-",
    "context2": "IDF-Over)[43]、引文TF-IDF(Citation TFIDF)[43]和上下文TF-IDF(Context TFIDF)[44]等。由于特征的类别过于繁杂,本文将在第4节对此展开详细介绍。",
    "context3": "$\\textcircled{2}$ 模型改进",
    "context4": "集成学习是基于传统机器学习模型的主要改进方向[45]。模型的集成是指模型在横向上的组合，即利用多个模型提取同一文档中的关键词,并按照一定的权重综合多个模型的结果来给出文档最终的关键词。如Hulth利用Bagging算法集成三种抽取算法进行关键词抽取[46。Ercan等结合单词基于WordNet得到的分数与单词的位置信息使用Bagged决策树抽取关键词[47]。Sterckx等在抽取关键词时所使用的Boosted决策树[48]与Krapivin等构建的随机森林分类器[49]都属于通过集成的方式对模型进行改进。"
  },
  "（3）基于深度学习的方法": {
    "context1": "基于神经网络的模型在多项自然语言处理任务中取得了优异的成果。随着网络层数的增加,神经网络模型的性能愈发强大，同时也为关键词提取任务加入了新的动力。由于深度神经网络模型的种类较多，一概而论难以体现不同模型间的区别与其整体的发展趋势，所以本文将用于关键词提取的深度学习模型按照其所面向的任务进行综述。",
    "context2": "$\\textcircled{1}$ 基于分类的深度学习方法",
    "context3": "2012年,Sarkar等在关键词提取任务中使用了多层感知机模型（MultiLayer Perceptron,MLP）,在与传统的朴素贝叶斯模型和C4.5决策树模型的对比实验中发现MLP模型的结果优于前两者[50]。在该研究中，Sarkar等使用的特征包括单词的TF-IDF值、位置信息、词长等。2015年,Aquino等提出一种基于自联想网络的关键词提取方法，其在西班牙学术论文上的抽取效果要优于KEA与Maui等方法[51]。",
    "context4": "虽然上述研究都证明了深度学习模型相较于非监督模型和传统机器学习模型更有优势,但是它们并没有打破原有的研究框架。与基于传统机器学习的关键词抽取方法相比，上述研究仍然将关键词抽取视为分类任务，难以体现上下文语境对单词所造成的影响。",
    "context5": "$\\textcircled{2}$ 基于序列标注的深度学习方法",
    "context6": "循环神 经 网 络（Recurrent Neural Network,RNN)在处理序列时会将前一时刻的信息传递到下一时刻，使得序列中每个节点的状态同时受到自身属性与先前节点的影响。如果在同一序列上同时建立正向与逆向的RNN网络，就可以保证信息在序列中双向的流动。基于这种思想,将RNN模型应用于关键词提取任务中可以有效地捕获候选词与上下文之间的联系。",
    "context7": "Zhang等提出一种具有两个隐层的RNN模型，该模型通过第一层RNN网络捕获单词的信息,再通过第二层RNN网络实现对序列中关键词的标注[52]。因为在较长的序列中，RNN网络反向传播中容易出现梯度消失的问题，所以Basaldella等将第一层RNN网络替换为双向长短时记忆模型（Bi-directional Long Short-Term Memory,BiLSTM)[53]。",
    "context8": "考虑到CRF模型在序列标注任务上优异的性能，Alzaidy等提出BiLSTM $^ +$ CRF模型,在与多种模型的对比中发现,该模型无论在适应性或是准确率上都有较大的优势[54]。",
    "context9": "（4）关键词抽取研究总结",
    "context10": "对主流的关键词抽取方法进行梳理,结果如表1所示。可以发现基于非监督的方法主要优势在于简单易行,对标注数据的需求较少,劣势在于抽取结果的准确率不高，而且抽取效果在不同的数据集上往往会产生较大波动。基于有监督的方法的优势在于操作流程比较规范，主观因素较少，抽取的准确率较高,劣势在于需要大量的标注数据支持。从整体上来看，目前主流的关键词抽取研究更偏向基于有监督的方法，研究中所选用的模型也从分类模型转向序列标注模型。",
    "context11": "表1关键词抽取方法分类一览表  \nTable1List of Keyword Extraction Methods",
    "context12": "<table><tr><td colspan=\"2\">方法类型</td><td>方法描述</td><td>优点</td><td>缺点</td></tr><tr><td rowspan=\"3\">非监督方法 图结构</td><td>简单统计</td><td>基于N-gram[11]、TF-IDF[12]、词频[6]、词共现[13]等统计指标抽取关 键词</td><td>操作简单易行</td><td>准确率不高,且在不同数 据集上的表现不稳定</td></tr><tr><td></td><td>基于图结构对候选词进行排序,如 TextRank[17]、SingleRank[18]、 SGRank[19]等</td><td>间的联系</td><td>可以体现候选词 准确率有限,且不适用于 短文本</td></tr><tr><td>语言模型</td><td>通过语言模型计算候选词的信息量,并以此作为单词重要程度的 依据[33] 选择特征表示单词并通过模型将其进行区分,常</td><td>操作简单易行</td><td>带有较强的主观性,缺少 严谨的评价指标</td></tr><tr><td rowspan=\"3\">有监督方法</td><td>分类模型</td><td>传统机器学习 见的模型包括NB[34,39]、SVM[35]、决策树[47-49]等 利用深度学习模型对关键词与非关键词加以区 深度学习</td><td>抽取准确率较高</td><td>忽略了上下文的语境对 候选词的影响</td></tr><tr><td>序列标注模型</td><td>分,如MLP[50] 在判断当前单词的标签时会考虑上下文的信息， 传统机器学习</td><td></td><td>需要大规模的标注语料</td></tr><tr><td>深度学习</td><td>常见的模型为CRF36,55-56] 利用循环神经网络实现对序列的标注[52-54]</td><td>抽取准确率较高</td><td>支持</td></tr></table>"
  },
  "3关键词生成研究概述": {
    "context1": "与关键词抽取技术不同，关键词生成相关的研究进展相对来说比较缓慢。这是由于关键词生成技术的难度较大，一直缺少成熟的方案来推进该方面的研究。得益于深度学习技术的发展,近年来关键词生成研究得到了极大发展。"
  },
  "3.1关键词生成模式概述": {
    "context1": "与序列标注模型不同的是，序列到序列模型(Seq2Seq)可以解决机器翻译中原文与译文长度不一致的问题,这与关键词生成的场景十分相近，因而有研究提出采用Seq2Seq模型来解决关键词生成任务并取得了一定的成果。目前主流的关键词生成方法都建立在此基础上,其一般模式如图4所示。",
    "context2": "与序列标注模型类似，在该模式下同样存在文本预处理、数据集构建与文本编码环节，其主要区别在于Seq2Seq模型弱化了特征构建的环节且在模型层面做出了改进。Seq2Seq模型一般可以拆解为编码模块与解码模块,前者负责将文档的信息进行编码压缩，而后者负责对其进行解压，从而实现两个非等长序列间的映射关系。",
    "context3": "![](images/30c846594ec9ad71133cc931ab0524b362453ac117a15efcac9a8c978cc1fb59.jpg)  \n图4面向关键词生成的\"编码-解码\"模式  \nFig.4“Encoding-Decoding” Pattern for Keyword Generation"
  },
  "3.2关键词生成方法概述": {
    "context1": "早在2011年,Liu等就提出从机器翻译的角度来看待关键词提取任务，因为关键词提取与机器翻译从某种程度上来说都是为了建立原始文本与目标文本之间的映射关系[57]。由于当时还没有 Seq2Seq的概念，所以他们构建的关键词提取框架为统计机器翻译(Statistical Machine Translation,SMT)[58]与单词对齐模型（Word Alignment Model,WAM)[59]。2014年,Cho等提出最早的基于RNN的 Seq2Seq模型,为机器翻译任务带来了巨大的变革[60],也为关键词生成研究提供了新的契机。Seq2Seq模型的一大优势在于它能够解决原始序列与目标序列长度不相等的问题,这就要求模型的编码器将原始序列的信息压缩到固定长度,而解码器能够从语义层面上对原始序列的编码信息进行解码操作。",
    "context2": "2017年,Meng等采用基于Seq2Seq模型生成学术文本中的关键词，为解决长序列中的信息传递问题，他们采用双向GRU网络代替简单的RNN网络并提出CopyRNN模型[10]。CopyRNN模型的优势在于其不仅提升了关键词提取的性能，而且能够召回$20 \\%$ 不在原始序列中出现的关键词。其主要缺点在于该模型没有考虑语义覆盖度与语义相关性,导致抽取结果无法涵盖原文的主题且会抽取语义重复的关键词。因此,2018年Chen等提出CorrRNN模型,在关键词提取时施加约束[61]。同年,Zhang等提出加入注意力机制来解决上述研究中存在的问题[62]。2019年,Chen等在编码模块加入文本的主题信息，提高了关键词生成的准确性[3]。2020年,Chen等构建了一种排他的分层解码结构,可以有效地增加提取关键词的多样性[64]。",
    "context3": "此外，为提升关键词生成的性能,许多研究对Seq2Seq模型进行了较大的改进。如Chen等提出一种包含两个编码器的Seq2Seq模型,两个编码器分别对语料的全文与语料的关键内容进行编码，从而提高模型的性能[5]。Wang等则是将主题模型与Seq2Seq模型相结合，使得生成的短语更加契合文本主题[66]。Chan等则是通过加入强化学习的方式对其进行改进[67]。",
    "context4": "基于Seq2Seq模型的方法虽然有着较好的抽取效果,但是它的不足也同样明显,最为突出的问题是需要大量的标注数据对模型进行训练。Ye等提出两种方法来解决这一问题：第一种是首先通过非监督的方法TF-IDF与TextRank为未标记的数据集进行\"标注”,然后将其与已标记的数据集混合后作为Seq2Seq模型的训练数据;第二种是直接使用其他任务训练好的编码网络，这样只需要对解码网络进行训练,减少了模型对数据的需求[68]。Wang等则是从知识迁移的角度，提出从资源丰富的领域转移知识从而减少对本领域标注数据需求的想法，具体来说,他们提出了一种基于主题的对抗神经网络(Topic-based Adversarial Neural Network,TANN)[69]。"
  },
  "3.3关键词生成研究小结": {
    "context1": "与关键词抽取不同的是，关键词生成的相关研究在2017年之后才较为集中地出现,这与深度学习模型的发展是密切相关的。影响关键词生成结果的主要因素分为两个方面：第一是生成关键词的语义覆盖度问题,因为单纯的Seq2Seq模型并没有考虑关键词之间的相似程度，所以生成的关键词在语义上互相覆盖的程度较高;第二是标注数据的问题，深度学习模型是数据驱动的模型,数据规模的大小直接影响了模型的生成结果,这限制了模型的应用场景。因此，目前对关键词生成的研究也集中在这两个方面,研究者一般通过加入表示文档主题的模块让模型能够理解文本的主题分布,通过弱标注、知识迁移等角度解决数据稀缺的问题。"
  },
  "4关键词提取中使用的相关特征": {
    "context1": "特征在关键词提取中有着重要的地位，即便不改变模型结构，一个有效特征的加入就可以极大地提升关键词提取的效果，所以许多研究围绕特征展开。"
  },
  "4.1基于统计的特征": {
    "context1": "基于统计特征是关键词提取中应用最早,种类也最多的特征。虽然基于统计的特征类目繁多，但从整体上看，可以将其分为两类：频率特征与长度特征。",
    "context2": "频率特征的典型代表是单词的词频。从直觉上来说,在文档中出现频率越高的单词对文档的重要性就越大，文献[6]即是直接使用词频特征。随着关键词提取研究逐渐深人,研究者意识到词频特征存在的局限,所以逐渐使用TF-IDF[1.34.37,40-170-71]来替代词频并衍生出许多TF-IDF的改良指标，如对数TF-$\\mathrm { I D F } ^ { [ 4 2 ] }$ 、布尔 TF-IDF[43]等。",
    "context3": "长度特征通常指候选关键词中单词的个数，一般认为候选关键词所包含的单词个数越多，它所能表达的信息越全面。基于类似的想法，文献[42,51]将句子的长度特征,即句子中包含的单词个数,用来度量句子中所包含信息的多寡。"
  },
  "4.2基于位置的特征": {
    "context1": "基于位置的特征是关键词提取中最为常用的特征,因为关键词经常出现在文档中一些特定的位置。在结构层次较为明显的Web网页、学术论文等语料中,位置特征显得尤为重要。当文档的结构层次不明显时，一般会采用候选关键词的首次出现位置[38,42.51,72]、平均出现位置[42]、最后出现位置[41,51,73]等特征。当文档的结构层次明显时,存在两种方式来表示候选关键词的位置特征：第一种是直接增加新的特征维度来表明候选关键词是否在文档的特定位置出现,如文献[51,56,73]将候选关键词是否出现在文章的标题作为特征加入模型;第二种是对文档按照结构层次进行划分，在不同的层次中计算候选关键词的位置特征，如文献[40]利用布尔值表示候选关键词是否在特定的章节中出现。"
  },
  "4.3基于语言的特征": {
    "context1": "基于语言的特征是指候选关键词的语言属性，最为典型的是单词的词性特征[40.46]。在一些特定的文档中，单词的形态特征，如单词的后缀[40]、大小写[16,73]、是否为粗体[16.56.73]等往往也能起到较好的作用。撇开单词自身的属性,单词在整个句子中的依存关系[55]、修辞手法[74]也可以作为特征使用。"
  },
  "4.4其他特征": {
    "context1": "本文将应用于关键词提取的其他特征总结为以下三类。"
  },
  "（1）特定方法的特征": {
    "context1": "基于图的方法在关键词提取任务中有着重要的地位，由于该方法特殊的图状结构，出现了许多特定应用于该方法的特征。例如文献[75]中提出使用图的中心度量指标来提取关键词并提出了度中心度（Degree Centrality）、接近中心性（ClosenessCentrality）、介数中心性（Betweenness Centrality）特征向量中心性(EigenvectorCentrality)等指标。"
  },
  "(2）基于外部资源的特征": {
    "context1": "基于外部资源的特征主要有两类：第一类是由于目标文档的信息较为匮乏，通过加入外部资源来进行补充[18.21],并未加入新的特征。第二类是使用外部资源中的特征作为补充，较为常见的有维基词条[76]、维基百科关键词度[77]、搜索引擎评分[76]等。此外，区别于常见的来源于传统知识库的特征，文献[78-79]使用人的眼动特征进行英语关键词抽取。"
  },
  "（3）基于词嵌入的特征": {
    "context1": "Wang等首次在关键词提取任务中加入单词的词嵌人特征来增强候选关键词之间的语义关系[28]。随着深度学习的发展,越来越多的研究证明了词嵌入对单词特征的表达能力。常见的基于词嵌入的特征有Word2Vec[80]、Glove[81]。"
  },
  "4.5关键词提取使用特征总结": {
    "context1": "对关键词提取任务中较为常见的特征进行总结梳理,如表2所示。",
    "context2": "纵观关键词提取特征的发展过程，可以发现三个比较明显的阶段,本文将其总结为萌芽阶段、发展阶段与学习阶段。在萌芽阶段,关键词提取的概念刚刚诞生,相关研究大多采用基于非监督的方法，此时应用于关键词提取的特征类别主要为词频特征与位置特征,计算方式也比较简单。直到机器学习方法开始应用于关键词提取，关键词提取的特征进入"
  },
  "52 数据分析与知识发现": "",
  "表2关键词提取相关特征分类": {
    "context1": "Table 2Related Feature Classification of Keyword Extraction",
    "context2": "<table><tr><td>特征类别</td><td>特征描述</td><td>例子</td></tr><tr><td></td><td>基于统计的特征 统计单词的某一属性作为特征,常见的有词频特征与长度特征</td><td>TF-IDF[1,34,37,40-41,70-71]、对数TF-IDF[42]、布尔TF-IDF[43]</td></tr><tr><td>基于位置的特征</td><td>将单词出现在文档(句子)中的位置作为特征,在结构层次分明 的文档中尤为有效</td><td>首次出现位置[38,42.51,72]、平均出现位置[42]、最后出现位置 等特征[41,51,73]等</td></tr><tr><td>基于语言的特征</td><td>单词在语言属性上的特征,一般指词性,但也包括单词的形态 特征等</td><td>词性[40,46]、后缀[40]、大小写[16.73]、是否为粗体[16.56,73]等</td></tr><tr><td rowspan=\"3\">其他特征</td><td>特定方法的特征,如图模型的特殊结构使其具有一些特有的 特征</td><td>度中心度、接近中心性、介数中心性、特征向量中心性 等[75]</td></tr><tr><td>基于外部资源的特征,利用外部资源对原有特征进行补充，或 是采用外部资源的指标作为特征</td><td>维基词条[76]、维基百科关键词度[77]、搜索引擎评分[76]、眼 动特征[78-79]等</td></tr><tr><td>基于词嵌入的特征,利用词嵌入表示单词间的语义关系</td><td>Word2Vec[80]、Glove[81]</td></tr></table>",
    "context3": "了发展阶段。",
    "context4": "在发展阶段,基于语言的特征与基于外部资源的特征得到广泛应用，基于统计的特征与基于位置的特征也得到迅速发展。在这一阶段,特征的发展可以分为两个方面：特征计算的复杂化与特征类别的细分化。特征计算的复杂化是指对现有特征的计算方式进行改进，使得特征能够更为精准地描述关键词与非关键词之间的差异,文献[42-44]对TF-IDF特征所做的改良即是属于这一类。特征类别的细分化是指将文档拆分为多个子文档,在每个子文档里都对特征进行计算，如文献[40,42,51]。",
    "context5": "这一时期的主流思想是将关键词抽取视为分类任务，应用的模型属于特征驱动的分类模型，所以许多研究将重心放在特征上。这类特征局限性主要体现特征的适用性与客观性上。特征的适用性是指特征能否在不同类型的数据集上通用,但诸如大小写[8]、是否为粗体[16.56.73]等特征往往只在特定类型的数据集上存在，难以推广使用。特征的客观性是指特征的构建是否足够客观合理,但在这一阶段,特征的构建很大程度上依赖于历史经验与专家知识，带有一定的主观性。",
    "context6": "随着深度学习的发展，用于关键词提取的特征进入了学习阶段。研究者通过不断实践发现,利用神经网络对文本进行学习所得到的特征相较于显式特征来说表现更好[8]。如文献[82]即通过增加上下文的编码模块来表达目标文本的上下文信息。虽然特征的解释性有所下降，但是神经网络的优势在于能挖掘出候选关键词更深层次的特征，这使得研究重心开始倾向于如何使神经网络更好地理解文本，应用的模型也从特征驱动的传统机器学习模型转向数据驱动的深度学习模型。当然，这并不是说特征不再重要,由于神经网络模型在学习时的不确定性，难以保证模型在训练时切实地学习到文本中有效的信息，所以研究者会采用一些手段对模型的学习过程进行引导，如文献[78-79]通过注意力机制加入了外部特征对模型的训练过程进行引导，从而提升基于深度学习的关键词提取模型性能。"
  },
  "5关键词提取的评价方法": {
    "context1": "如何评估关键词提取的效果是一个重要环节，评估方法的优劣直接影响了实验的可信度，所以本节介绍几种有代表性的关键词提取评价方法。由于不同模型的输出结果存在差异,Manning等将评估方法分为针对无序结果的评估方法和针对有序结果的评估方法两类[83]。但是两个类别都是在精准匹配的前提下对关键词提取的结果评估,并不能囊括模糊匹配的评价方法,因此本文在其基础上增加了其他评估方法类别作为补充。"
  },
  "5.1针对无序结果的评估方法": {
    "context1": "早期研究将平均抽取准确词汇个数(Average ofCorrectlyExtractedKeyphrases,ACEK)作为无序结果的评估方法,但随着准确率(Precision,P）、召回率(Recall,R）与 $\\mathrm { F } _ { 1 }$ 值( $\\mathrm { ~ F _ { \\mathrm { 1 } } ~ }$ -measure, $\\mathrm { F } _ { 1 } .$ )指标的提出，ACEK已不再应用于关键词提取结果的评估。",
    "context2": "$P , R , F _ { 1 }$ 是目前关键词提取任务中应用最为广泛的评估方法。其计算公式如公式(1)-公式(3)",
    "context3": "所示。",
    "context4": "$$\nP = { \\frac { T P } { T P + F P } }\n$$",
    "context5": "$$\nR = \\frac { T P } { T P + F N }\n$$",
    "context6": "$$\nF _ { 1 } = { \\frac { 2 \\times P \\times R } { P + R } }\n$$",
    "context7": "其中， $T P$ 表示预测准确的样本中的正例个数，$F P$ 表示预测为正例但实际为负例的样本个数， $F N$ 表示预测为负例但实际为正例的样本个数。"
  },
  "5.2针对有序结果的评估方法": {
    "context1": "由于部分关键词提取方法并不会给出一个确切的关键词,而是会生成一个候选词的序列，所以有研究提出了对候选词序列进行评估的方法。这一评估方法的主要思想是对序列中关键词的排名进行计算，从而对两个序列的优劣进行评估。Voorhees 提出的 Mean Reciprocal Rank(MRR)[84]与Liu 等提出的Mean Average Precision(MAP)[85]都属于此类方法。以MRR为例,其计算公式如公式(4)所示。",
    "context2": "$$\nM R R = { \\frac { 1 } { | D | } } \\sum _ { d \\in D } { \\frac { 1 } { r a n k _ { d } } }\n$$",
    "context3": "其中， $r a n k _ { d }$ 表示关键词在序列中首次出现的位置编号， $D$ 表示所有文档的集合， $d$ 表示集合中的单个文档。如果在候选词序列中，关键词的排名越靠前,该序列的MRR值越低,证明该方法更为有效。"
  },
  "5.3其他评估方法": {
    "context1": "以上两类评估方法是在精确匹配的前提下对关键词提取的效果进行评价。在该标准下，模型提取出关键词的近义词仍会被认为是错误的,这导致上述评估方法会存在缺陷。因而有许多研究在精准匹配的基础上会加入模糊匹配的方法作为补充。具体来说，可以从词形相似度与语义相似度两个方面对关键词间的相似度进行计算,从而更有效地评估提取的效果。前者一般采用编辑距离算法[8进行计算,后者的计算方式因研究而异,如Dagan等利用概率模型计算单词间的相似度[87]。考虑到单一的相似度方法存在局限,文献[88]在计算时综合考虑了关键词间语义相似度与词形相似度。"
  },
  "6关键词提取研究存在的问题与未来展望": {
    "context1": "本文首先对关键词提取的两个主要内容——关键词抽取与关键词生成的主要方法进行了系统的梳理,并总结两类方法的一般模式。接着对应用与关键词提取的特征进行了分类与演化分析。最后对关键词提取的评价方法进行综述。通过对现有研究进行总结与分析不难发现，深度学习模型已成为现阶段主要的关键词提取方法，而数据规模已经成为影响关键词提取性能的重要因素，许多研究开始专注于如何降低模型对标注数据的依赖。对于关键词提取中的特征而言，现有研究不再局限于追求特征的数量或是复杂度，更多地是通过加入特征模块的方式让模型学习文本中所包含的特征。"
  },
  "6.1关键词提取研究存在的主要问题": {
    "context1": "（1）标注数据规模瓶颈",
    "context2": "深度学习模型因为其优异的性能已成为主流的关键词提取方法，但其性能十分依赖于标注数据的规模。采用人工标注的方式代价比较高，而且大规模的数据标注会存在标注一致性的问题。因此如何减少模型对标注数据的需求成为一个亟待解决的问题。"
  },
  "（2）标注规范问题": {
    "context1": "尽管目前有不少开源的标注数据集，但由于语料类型、标注粒度、标注人员专业素质等条件的不同,在不同数据集上的标注结果存在较大差异。如何制定一套统一规范的标注框架仍然是关键词提取任务中的重要问题。"
  },
  "（3）评价方法的局限": {
    "context1": "目前主流的关键词提取评价方法都是基于精准匹配进行计算，这种计算方法存在一定的局限性。尤其是关键词生成模型得到的单词中存在大量的近义词,如果纯粹使用P、R、F指标将难以准确地对抽取结果进行评估。因此如何评价单词语义上的相关程度将会是一个重要的问题。"
  },
  "6.2关键词提取的研究展望": {
    "context1": "（1）深入挖掘外部资源的特征",
    "context2": "虽然已有许多研究使用了基于外部资源的特征[18.21],但是这些特征的来源大多局限于搜索引擎、知识库且大多属于文本特征。文献[78-79]通过使用人的眼动特征来度量句子中候选关键词的重要程度，从而提升了关键词抽取的效果,这无疑为关键词提取的特征选择开拓了新的方向。未来的研究可以"
  }
}