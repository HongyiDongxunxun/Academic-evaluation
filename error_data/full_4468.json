{
  "original_filename": "full_4468.md",
  "基于图神经网络的会话推荐方法综述": "",
  "张雄涛」祝 娜² 郭玉慧": {
    "context1": "1（北京科技大学经济管理学院北京100083）  \n2（西南大学国家治理学院 重庆 400715)  \n3（中国人民大学信息学院北京100872）",
    "context2": "摘要：【目的】聚焦图神经网络技术，对会话推荐方法进行述评。【文献范围】分别以“Session-BasedRecommendation”“Graph Neural Network”“会话推荐”“图神经网络\"为检索词,在Web of Science、中国知网等数据库中筛选出82篇国内外文献。【方法】从框架、评价和趋势三个视角,对基于图神经网络的会话推荐方法进行归纳与对比，总结现有评价资源，讨论未来研究趋势【结果】图神经网络是当前实现会话推荐系统的主流技术,基于图神经网络的会话推荐方法主要围绕\"会话图构建”“会话图学习\"和\"会话兴趣表示\"三个核心问题展开。【局限】本文仅评述主流研究,并未将所有研究逐一列出。未从可解释性、鲁棒性、多样性和公平性等方面深人研究【结论】图神经网络是会话推荐系统的主流实现技术,未来可结合会话推荐的特定场景,通过发展图神经网络技术进一步改进现有研究不足。",
    "context3": "关键词：图神经网络会话推荐序列推荐推荐系统深度学习  \n分类号：TP393G250  \nDOI: 10.11925/infotech.2096-3467.2022.1282",
    "context4": "引用本文：张雄涛，祝娜，郭玉慧.基于图神经网络的会话推荐方法综述[J].数据分析与知识发现，2024，8(2）：1-16.(Zhang Xiongtao, Zhu Na, Guo Yuhui. A Survey on Sesion-Based Recommendation Methods withGraph Neural Network[J]. Data Analysis and Knowledge Discovery, 2024, 8(2): 1-16.)"
  },
  "1引言": {
    "context1": "信息过载背景下，会话推荐系统通过分析用户当前会话序列(在一段时间内用户与站点发生的连续交互行为)学习用户会话兴趣,实现实时推荐[]。会话推荐系统不仅可以捕获用户的短期事务型意图[2],还可以依据不完整用户信息实现有效推荐[3],已成为当前热门研究方向。",
    "context2": "基于模式/规则的方法和基于马尔科夫链（MarkovChain，MC)的方法是实现会话推荐系统的早期方法。其中,基于模式/规则的方法,如：关联规则[4]、基于物品的协同过滤[5]等,主要依据项目之间的相似度或共现模式实现会话推荐;基于MC的方法，如 FPMC（Factorizing Personalized MarkovChains）模型[6]、FMDPs（Factored Markov Decision",
    "context3": "Processes)模型[7等，主要通过估计连续两个项目之间的转移概率实现会话推荐。然而,这两类早期方法仅关注特定步长的项目转移，无法捕捉项目之间的长期依赖,使会话兴趣学习不充分[8]。为提升会话推荐性能,基于循环神经网络(Recurrent NeuralNetwork，RNN）的会话推荐方法，如GRU4REC(Gated RecurrentUnits for Recommendation)模型[9]、NARM(Neural Attentive Recommendation Machine)模型[10]等,通过考虑项目之间的长期依赖对会话兴趣表示进行学习。凭借序列建模的显著优势,RNN一度成为实现会话推荐的主流技术[]。然而,基于RNN的会话推荐方法过于强调项目之间的顺序依赖，无法对项目之间结构化的复杂关系进行有效建模[12]。为进一步提升会话推荐性能,基于图神经网络（Graph Neural Network，GNN)的会话推荐方法,如 SR-GNN（Session-Based Recommendation withGraph Neural Networks)模型[13]、GCE-GNN(GlobalContext Enhanced Graph Neural Networks）模 型[14]等,将会话数据组织为会话图,并借助GNN的信息传播机制对会话图进行学习。由于GNN可以通过建模项目之间的复杂转移关系更充分地理解用户的会话兴趣[15],因此基于GNN的会话推荐方法在会话推荐领域广泛流行。",
    "context4": "为全面了解和对比不同会话推荐方法的研究现状，本文对2013年1月至2022年12月近10年间的国内外相关研究进行人工检索和统计分析，结果如图1所示。其中，“传统的会话推荐方法\"包括\"基于模式/规则的会话推荐方法\"和\"基于MC的会话推荐方法”。在Webof Science 数据库中,以“Session-Based Recommendation”分别与“Pattern Mining”“Markov Chain”、“Recurrent Neural Network”和“GraphNeuralNetwork\"组合生成检索式对相关研究进行检索和统计;在中国知网数据库中，以\"会话推荐\"分别与\"关联规则”“马尔科夫”“循环神经网络\"和\"图神经网络\"组合生成检索式对相关研究进行检索和统计。从图1中可以观察到：会话推荐研究在国内外的研究数量呈递增趋势;基于GNN的会话推荐方法在近期研究中的占比呈现递增趋势。由此可见,会话推荐研究符合国内外的热门研究趋势，且GNN是实现会话推荐系统的热点技术。",
    "context5": "![](images/fc128269172b8ce963222827487f4bdb4e29b3b4ce43e634c703292997ab6d89.jpg)  \n图1不同会话推荐方法的研究统计  \nFig.1Research Statistics of Different Session-Based Recommendation Methods",
    "context6": "近年来，国内外有关会话推荐的研究综述已取得一定成果。例如：文献[11]分别从含义、应用场景、主要算法、实验设置等方面对会话推荐系统进行全面分类与总结。文献[15]采用实证研究方法对神经网络和非神经网络会话推荐方法进行对比分析，并指出两类会话推荐方法在准确率和召回率指标上不存在绝对的优劣之分。然而，这些综述主要将",
    "context7": "RNN作为实现会话推荐的前沿技术，尚未在当前主流的GNN技术背景下对会话推荐方法的新型研究范式展开系统性评述。本文聚焦GNN技术背景下会话推荐方法呈现出的新特点,对GNN会话推荐方法的研究框架、评价资源以及未来研究趋势进行系统性归纳、总结和讨论，为进一步优化会话推荐方法提供借鉴与参考。具体而言,本文的研究工作体现",
    "context8": "在三个方面：",
    "context9": "(1)依据GNN会话推荐方法的研究特点，从会话图构建、会话图学习和会话兴趣表示三个方面归纳研究框架,并在研究框架下总结对比相关方法的研究思路和优劣势。",
    "context10": "(2)依据GNN会话推荐方法的实验设置，从数据集、评价指标和基线模型三个方面总结评价资源，并分析现有评价资源存在的不足，为验证和发展GNN会话推荐方法提供实验基础。",
    "context11": "(3)基于现有研究的不足，结合推荐系统领域的研究热点，对GNN会话推荐方法在可解释性、鲁棒性、多样性和公平性方面的研究趋势与可行性思路进行讨论。"
  },
  "2技术分析": {
    "context1": "本文的技术基础是GNN,其主要思想是迭代地聚合来自邻居节点的信息，并将聚合的邻居信息与中心节点表示进行整合，实现信息传播[8]。相较于CNN、RNN、注意力机制等深度学习方法,GNN在会话推荐中的优势主要体现为：",
    "context2": "(1)图数据处理。会话序列可依据项目转移关系对应地组织为图数据，而图数据为建模项目之间的复杂转移关系提供了结构化基础[16]。然而,其他深度学习技术仅在欧氏空间探究会话序列直接反映的会话兴趣，无法在非欧氏空间借助图结构挖掘蕴含在项目复杂转移关系中的隐式兴趣[8],致使会话兴趣表示不充分。",
    "context3": "(2)高阶连通性建模。用户行为的随机性使会话序列中不可避免地包含噪声[17],仅利用项目的邻接转移可能导致会话兴趣表示存在冗余。项目之间的高阶连通性包含丰富的结构化语义,引入高阶项目转移关系有利于削弱噪声交互对会话兴趣建模的影响[18]。相较于其他深度学习技术,GNN可利用信息传播机制有效地建模图中节点之间的高阶连通性,因此更有利于建模准确的会话兴趣表示。"
  },
  "3研究框架": {
    "context1": "依据GNN会话推荐方法在图数据、图学习、图利用三方面的侧重,本文将研究框架归纳为三个方面，如图2所示。会话图构建类研究重点关注如何科学地将会话数据组织为会话图，会话图学习类研究聚焦于GNN中信息传播机制的选择与设计，会话兴趣表示类研究重点关注项目整合方式的选择与设计。围绕研究框架中的三个核心问题对相关研究进行文献梳理,对比每个核心问题下相关方法的优势与劣势,全面且系统地了解现有研究范式。",
    "context2": "![](images/0b3be30f90f421ee5e196c97010020b5e8ce6feff7483fdaa0fafd27d72ad401.jpg)  \n图2基于GNN的会话推荐方法研究框架 Fig.2Research Framework of Session-Based Recommendation Method with GNN"
  },
  "3.1会话图构建": {
    "context1": "在基于GNN的会话推荐方法研究中，构建合理的会话图模型是成功应用GNN的基础[]。依据会话数据的丰富性,将会话图构建方法分为两类：基于单会话的图构建方法和基于多会话的图构建方法。两类研究的代表性方法如图3所示，实线箭头表示会话内的项目转移关系，虚线箭头表示构建的项目关系,椭圆表示节点之间存在超边。"
  },
  "（1）基于单会话的图构建方法": {
    "context1": "基于单会话的图构建方法主要依据单个会话构建会话图,依据是否考虑会话内项目的非连续关系，相关研究可进一步分为两类：",
    "context2": "$\\textcircled{1}$ 基于会话内连续关系的图构建方法，如图3(a)所示。此类方法操作相对直接,主要将单个会话内的每个项目视作图中的节点，通过在两个连续项目之间添加边构建会话图模型。如SR-GNN模型[13]、GC-SAN(Graph Contextualized Self-AttentionNetwork）模型[19]、TAGNN（Target Attentive GraphNeural Networks)模型[20]、Beyond Clicks 模型[21]、FA-",
    "context3": "![](images/fda9472e7a20889acf00317f9e7e154aaa21591e2ce41107bd0b14a2361e9533.jpg)  \n图3会话图构建方法示意图  \nFig.3Schematic of Session Graph Construction Methods",
    "context4": "GNR（Feature Augmentation Based Graph NeuralRecommendation)模型[22]等经典研究均采用此类方法对会话图进行构建。然而,在实际推荐场景中，用户会话序列的长度较短[13],如Diginetica数据集（一种典型的会话推荐数据集)上的会话平均长度仅为5.71,导致会话图模型中的结构化信息不充分。",
    "context5": "$\\textcircled{2}$ 考虑会话内非连续关系的图构建方法，如图3(b)所示。此类方法主要利用非连续项目关系弥补连续项目关系数据的稀疏性，以综合连续和非连续关系构建会话图模型。例如,假设一个会话中的任意项目会对后续多个项目产生影响，MA-GNN（Memory Augmented Graph Neural Networks）模型[23]为当前项目节点和三个后续项目节点之间添加边。考虑到仅在连续项目之间添加边可能会忽略远程项目关 系，SGNN-HN（Star Graph NeuralNetworks with Highway Networks)模型[24]在会话内引入一个虚拟的\"星\"节点作为会话中心,并将其与当前会话中的所有项目连接。由于考虑项目之间的非连续关系可以有效稠密化会话图结构[23-24],所有此类方法往往比基于会话内连续关系的图构建方法表现出更优异的会话推荐性能。然而,此类研究仅聚焦单个会话，仍然无法摆脱单个会话固有的数据稀疏性。"
  },
  "（2）基于多会话的图构建方法": {
    "context1": "基于多会话的图构建方法的出发点是丰富单个会话中的有限交互行为[14]，利用多个会话中的项目关系构建更丰富的会话图模型。依据会话图在结构上的差异,相关研究可进一步分为两类：",
    "context2": "$\\textcircled{1}$ 基于超图的图构建方法，如图3(c)所示。此类方法主要通过规定超边为项目建立联系，以有效利用项目之间的非成对关系构建会话图模型。如DHCN （Dual Channel Hypergraph ConvolutionalNetworks）模 型[25]和 HIDE（Hypergraph NeuralNetworks with IntentDisentanglement)模型[26]将每个会话作为一个超边，通过集成全局会话构建超图。类似地，SHARE（Session-Based HypergraphAttention Network for Recommendation)模型[27]依据滑动窗口对超边进行定义，并在全局层面为每个会话构造一个超图。此外,COTREC（Self-SupervisedGraph Co-Training for Session-BasedRecommendation)模型[28]结合会话之间的关系构建以会话为节点的超图，为显式地整合跨会话信息提"
  },
  "4 数据分析与知识发现": {
    "context1": "供了充分基础。然而，由于同属一个超边的节点之间无法有效体现时序关系，此类研究在时序关系利用上存在限制。",
    "context2": "$\\textcircled{2}$ 基于常规图的图构建方法，如图3(d)所示。此类方法主要利用额外会话中的项目关系丰富单个会话中的项目转移关系，为跨会话项目建立联系。现有研究中，额外会话具体包括其他交互类型会话[21,29-31]、同一用户的历史会话[32-33],整个数据集中的部分或全部会话[34-35]、知识相关的会话序列[36-37]等。例如,Beyond Clicks模型[21]综合考虑点击、购买等多种交互类型的会话序列构建会话图，以充分捕捉更有效的用户会话兴趣。A-PGNN（Personalized GraphNeural Networks with Attention Mechanism）模型[32]和 HetGNN （Heterogeneous Global Graph Neural",
    "context3": "Networks)模型[33]在用户信息已知的情况下，将用户的历史序列与当前序列相结合，丰富项目与项目之间的联系。DAT-MDI(Dual Attention Transfer withMulti-Dimensional Integration）模型[34]和 GEL-GNN(Global Enhanced Graph Neural NetworkwithLSTM)模型[35]利用所有会话中的项目转换协助当前会话中的项目转移,进而综合利用局部和全局上下文实现会话推荐目的。然而，此类研究在引入额外会话时不可避免地增加了噪声信息引入的风险，因此如何避免噪声引入或消解引入的噪声是此类研究的重点。",
    "context4": "综上,不同会话图构建方法均具有显著特点，各类方法的研究思路及其优点和缺点总结如表1所示，可为后续研究提供借鉴和参考。",
    "context5": "表1会话图构建方法的研究思路及优缺点  \nTable 1Research Ideas，Advantages and Disadvantages of Session Graph Construction Methods",
    "context6": "<table><tr><td colspan=\"2\">图构建方法</td><td>研究思路</td><td>优点/缺点</td></tr><tr><td rowspan=\"2\">基于单会话的 图构建方法</td><td>关系</td><td>基于会话内连续 将单个会话内的每个项目视作图中的节点,通过在·优点:操作相对直接,构建逻辑简单; 两个连续项目之间添加边构建会话图模型。</td><td>·缺点：无法克服会话数据固有的稀疏性问题。</td></tr><tr><td>续关系</td><td>考虑会话内非连 利用非连续项目关系弥补连续项目关系数据的稀疏·优点：利用非连续关系有效稠密化会话图结构； 性，以综合连续和非连续关系构建会话图模型。</td><td>·缺点：无法克服会话数据固有的稀疏性问题。</td></tr><tr><td rowspan=\"2\">基于多会话的 图构建方法</td><td>基于超图</td><td>通过规定超边为项目建立联系,以有效利用项目之 间的非成对关系构建会话图模型。</td><td>·优点：利用项目之间的非成对关系有效丰富会 话图结构; ·缺点：在时序关系利用上存在限制。</td></tr><tr><td>基于常规图</td><td>利用额外会话中的项目关系来丰富单个会话中的项 目转移关系,以有效利用跨会话项目关系。</td><td>·优点：利用跨会话项目转移关系有效丰富会话 图结构； ·缺点：增加了噪声信息引入的风险。</td></tr></table>"
  },
  "3.2会话图学习": {
    "context1": "在基于GNN的会话推荐方法中,会话图学习主要利用GNN的信息传播机制对项目之间的转移关系进行学习。依据信息传播机制的差异，现有的会话图学习方法主要分为4类：基于直推式图卷积网络（Graph Convolution Network,GCN）的图学习方法、基于归纳式图卷积网络（Graph Sample andAggregate,GraphSAGE)的图学习方法、基于图注意网络(Graph Attention Network,GAT)的图学习方法和基于门控图神经网络（Gated Graph SequenceNeuralNetworks,GGNN)的图学习方法。4种会话图学习方法的示意如图4所示， $i _ { 2 } \\ r , i _ { 3 } \\ r , i _ { 4 }$ 和 $i _ { 5 }$ 均为中心节点 $i _ { 1 }$ 的一阶邻居节点, $w _ { 1 j }$ 表示任意邻居节点 $i _ { j }$ 对中心节点 $i _ { 1 }$ 的重要性。"
  },
  "（1）基于GCN的图学习方法": {
    "context1": "GCN的信息传播过程如图4(a)所示，采用池化操作不加区分地对所有邻居节点进行聚合，然后以聚合信息更新中心节点表示[38]。由于GCN具有简单的运算逻辑,基于GCN的图学习方法被广泛应用于会话图学习。如为适应流式会话推荐，GAG(Global Attributed Graph Neural Network)模型[39]基于GCN开发出一种带有Wasserstein库的全局属性感知的GCN。为结合项目之间的潜在协同信息实现会话推荐,DCTN(Dual-Channel Graph TransitionNetwork)模型[40]将目标会话和邻居会话构建为单个图，并以改进的GCN学习项目在不同会话之间的转移关系。CAGE(Context-Aware Graph Embedding)模型[4I通过在新闻会话图上执行GCN有效实现了基于会话的新闻推荐系统。尽管此类方法广泛应用于会话图学习,但由于GCN需要在会话图中通过聚合所有邻居节点实现信息传播，故此类方法效率较低、难以应用于大规模会话数据。",
    "context2": "![](images/ceef797fdcb3c0094e510e8979fa5574a8e9d685c86ed5054ae910ecb2ce9b1b.jpg)  \n图4会话图学习方法示意图  \nFig.4Schematic of Session Graph Learning Methods"
  },
  "（2）基于GraphSAGE的图学习方法": {
    "context1": "GraphSAGE的信息传播过程如图4(b)所示，为每个中心节点采样固定数量的邻居,然后采用池化操作对邻居信息进行聚合，进而结合聚合的邻居信息对中心节点表示进行更新。由于GraphSAGE具有较强的可扩展性和灵活性[42]，相关研究常利用GraphSAGE对规模较大或关系复杂的会话图进行学习。如为有效实现多行为感知的会话推荐，Beyond Clicks模型[21]综合点击、加购和购买等多种交互行为构建会话图，并采用GraphSAGE对会话图进行学习，以获取不同类型邻居感知的项目表示。ISSR (Inter-Sequence Enhanced FrameworkforPersonalized Sequential Recommendation）模型[43]采用GraphSAGE对用户-项目二部图和全局项目图进行学习，结合跨序列项目关联增强时序推荐的性能。MA-GNN模型[23]利用共享内存网络增强GraphSAGE,捕获长期和短期的用户兴趣实现更有效的时序推荐。尽管GraphSAGE可以有效增强GCN的灵活性，但此类方法在会话图学习中无法考虑邻居节点的权重，故在信息传播过程中无法合理区分不同邻居节点对中心节点的重要性。"
  },
  "（3）基于GAT的图学习方法": {
    "context1": "GAT的信息传播过程如图4(c)所示，主要利用注意力机制差异化整合节点的邻居节点信息，进而结合邻居节点信息更新中心节点的表示。由于GAT可以结合邻居节点的权重更准确地学习会话图，基于GAT的会话图学习方法在相关研究中得到广泛关注。如GCE-GNN模型[14]利用GAT分别对会话图和全局图中的项目关系进行学习，综合项目的全局和局部上下文实现会话推荐。此外，DGRec(Session-Based Social Recommendation via DynamicGraphAttentionNetworks)模型[44]采用GAT对用户之间的社交关系进行学习，综合来自好友用户的动态社交影响学习准确的社会化会话兴趣表示。AMAN(Adaptive Multi-AttentionNetwork)模型[30]采用GAT在有向异质图上对多行为感知的项目表示进行学习，综合不同行为重要性学习更准确的会话意图。然而,GAT主要用于建模非顺序感知的无向会话图，故此类研究在顺序感知的会话图学习方面存在限制。"
  },
  "（4）基于GGNN的图学习方法": {
    "context1": "GGNN的信息传播过程如图4(d)所示，将中心节点的前项邻居和后项邻居信息进行聚合，然后将聚合信息输入门控循环单元(GateRecurrentUnit,GRU)实现节点表示更新[45]。由于GGNN可以有效考虑项目之间的顺序关系，且会话图中的节点本质上具有顺序关联，因此基于GGNN的会话图学习方法在基于GNN的会话推荐中应用更为广泛。作为将GNN引入会话推荐的开创性工作,SR-GNN模型[13]采用GGNN对会话图进行学习，并证明了图神经网络在会话推荐中的优势。类似地,GC-SAN模型[19]和TAGNN模型[20]同样采用GGNN对会话图进行学习，结合项目之间的转移关系对项目表示进行有效学习。尽管GGNN在会话图学习的过程可以有效地考虑项目之间的顺序关联,但文献[2]指出仅利用GGNN并不总利于学习充分的会话表示,其往往需要搭配注意力网络、去噪机制等合理的情景建模方法才可实现有效的会话推荐。",
    "context2": "综上,不同会话图学习方法均具有显著特点且存在差异，各类方法的研究思路及其优点和缺点总结如表2所示，可为后续研究提供借鉴和参考。",
    "context3": "表2会话图学习方法的研究思路及优缺点  \nTable 2Research Ideas,Advantages and Disadvantages of Session Graph Learning Methods",
    "context4": "<table><tr><td>图学习方法</td><td>研究思路</td><td>优点/缺点</td></tr><tr><td>基于GCN</td><td>采用池化操作等同地对所有邻居节点进行信息聚·优点：具有简单的运算逻辑; 合，然后以聚合信息更新中心节点表示。</td><td>·缺点：不适用于大规模会话图学习。</td></tr><tr><td>基于GraphSAGE</td><td>为每个中心节点采样固定数量的邻居,然后采用·优点：具有较强的可扩展性和灵活性; 邻居信息对中心节点表示进行更新。</td><td>池化操作对邻居信息进行聚合,进而结合聚合的·缺点:在信息传播过程中无法合理区分不同邻居节点对中心节点的</td></tr><tr><td>基于GAT</td><td>息，然后结合邻居节点信息更新中心节点的表示。</td><td>利用注意力机制差异化整合节点的邻居节点信·优点:在信息传播过程中,结合邻居节点权重更准确地学习会话图; ·缺点：在顺序感知的会话图学习方面存在限制。 将中心节点的前项邻居和后项邻居信息进行聚·优点:在会话图学习的过程中有效地考虑项目之间的顺序关联；</td></tr><tr><td>基于GGNN</td><td></td><td>合,然后将聚合信息输入GRU实现节点表示更新。·缺点:需要搭配合理的情景建模方法才可实现有效的会话推荐。</td></tr></table>"
  },
  "3.3会话兴趣表示": {
    "context1": "GNN中的信息传播不能有效地捕获项目之间的远程依赖关系[46]，且会话图会造成序列信息损失[2,因此依据GNN学习到的任意项目表示都无法充分代表用户的当前会话兴趣。为有效表示会话兴趣,整合会话序列中的多个项目是基于GNN会话推荐方法的重要一步。依据项目整合方式的不同，现有会话兴趣表示方法主要分为三类：基于注意力的会话兴趣表示方法、基于RNN的会话兴趣表示方法和基于位置的会话兴趣表示方法，如图5所示。",
    "context2": "![](images/c066f5aa1a2f573c538d337d50bcd4eec44dec7ebc366c447fbee6953cb5a7b2.jpg)  \n图5会话图构建方法示意图  \nFig.5Schematic of Session Interest Representation Methods"
  },
  "（1)基于注意力的会话兴趣表示方法": {
    "context1": "基于注意力机制的会话表示方法如图5(a)所示，将会话中最后一次交互视作局部会话兴趣;将局部会话兴趣视作查询项,采用注意力机制对会话中所有项目进行加权聚合，获得全局会话兴趣;最后融合局部和全局会话兴趣，得到完整的会话兴趣表示。基于该思路,SR-GNN模型[13]、GAG模型[39]、SGNN-HN 模 型[24]、DCTN 模型[40]和 GNN-BiGRU-TA(Integration of GNN，Bi-GRU and the Attention",
    "context2": "Mechanism)模型[47]对基于GNN学习到项目表示进行差异化整合，以合理地对会话兴趣进行表示。然而,仅利用注意力机制往往会忽略项目在会话中的位置信息，因此相关研究通常将注意力机制应用于位置信息感知的项目表示。"
  },
  "（2）基于RNN的会话兴趣表示方法": {
    "context1": "基于RNN的会话兴趣表示方法主要通过建模项目之间的顺序依赖建模会话兴趣表示，如图5(b)所示，采用RNN获取各个项目的隐藏状态，然后通过整合各个项目的隐藏状态输出会话兴趣的最终表示。如长短时记忆网络(Long Short-TermMemory,LSTM）模型、DGRec 模型[44]和 EFRec（EnhancingSession-Based Social Recommendation）模型[48]通过对会话内部的项目依赖进行学习，有效建模了社会化会话推荐场景下不同用户的会话兴趣。此外，基于 GRU、FGNN(Full Graph Neural Network)模型[49]利用序列中的项目表示迭代更新用户偏好,以有效平衡连续转移模型和灵活转移模式之间的关系。然而,此类研究无法克服噪声项对会话兴趣表示造成的影响,因此如何在RNN基础上引入噪声去除机制是改进此类方法的主要思路。"
  },
  "（3）基于位置的会话兴趣表示方法": {
    "context1": "基于位置的会话兴趣表示方法如图5(c)所示，将项目在当前会话中的位置编码为位置向量，然后组合位置向量与项目表示，将位置信息融入项目表示，实现更准确的会话兴趣建模。在相关研究中，NISER(Normalized Item and Session Representationswith Graph Neural Networks)模型[50]、GCE-GNN 模型[14]和 MGS（Mirror Graph Network for Session-BasedRecommendation)模型[51]增加了反映物品相对顺序的位置嵌入，从而有效地获得位置感知的物品表示。DSAN(Dual Sparse Attention Network)模型[2将候选项目的位置信息作为查询项，采用双重注意力机制信息，消除噪声对会话兴趣学习造成的消极影响。由于位置信息本质上仅考虑了项目自身信息，故仅考虑项目位置无法综合会话内项目间关系完整表示会话兴趣。通常情况下，基于位置的会话兴趣表示方法会将项目位置编码与注意力机制或RNN相结合来学习会话兴趣表示。综上,不同基于位置的会话兴趣表示方法的研究思路及其优点和缺点总结如表3所示。",
    "context2": "表3会话兴趣学习方法的研究思路及优缺点  \nTable 3Research Ideas,Advantages and Disadvantages of Session Interest Learning Methods",
    "context3": "<table><tr><td>会话兴趣表示方法</td><td>研究思路</td><td>优点/缺点</td></tr><tr><td rowspan=\"3\">基于注意力</td><td>将会话中最后一次交互视作局部会话兴趣;将局部会 话兴趣视作查询项,采用注意力机制对会话中所有项</td><td>·优点：通过差异化整合会话中的项目表示更准确地表示会话</td></tr><tr><td>目进行加权聚合,获得全局会话兴趣;融合局部和全</td><td>兴趣；</td></tr><tr><td>局会话兴趣，得到完整的会话兴趣表示。</td><td>·缺点：仅利用注意力机制会忽略项目在会话中的位置信息。</td></tr><tr><td rowspan=\"2\">基于RNN</td><td>采用RNN获取各个项目的隐藏状态,通过整合各个·优点：利用项目之间的顺序依赖建模会话兴趣表示;</td><td></td></tr><tr><td>项目的隐藏状态输出会话兴趣的最终表示。</td><td>·缺点：无法有效克服噪声项对会话兴趣表示造成的影响。</td></tr><tr><td rowspan=\"2\">基于位置</td><td>将项目在当前会话中的位置编码为位置向量,然后组·优点:将位置信息融入项目表示,实现更准确的会话兴趣建模;</td><td></td></tr><tr><td></td><td>合位置向量与项目表示,将位置信息融入项目表示。·缺点：无法综合会话内项目间关系完整表示会话兴趣。</td></tr></table>"
  },
  "3.4研究框架总结与不足分析": {
    "context1": "GNN会话推荐方法的相关研究主要体现为会",
    "context2": "话图构建、会话图学习和会话兴趣表示三个核心问题,本节内容的代表性研究总结如表4所示。",
    "context3": "表4研究框架中代表性工作研究总结  \nTable 4Summary of Representative Works in the Research Framework",
    "context4": "<table><tr><td>实现过程</td><td>方法</td><td>代表性工作</td></tr><tr><td rowspan=\"2\">会话图构建</td><td>基于单会话的图构建方法</td><td>文献[13]，文献[19]，文献[20]等</td></tr><tr><td>基于多会话的图构建方法</td><td>文献[21]，文献[25]，文献[27]等</td></tr><tr><td rowspan=\"4\">会话图学习</td><td>基于GCN的图学习方法</td><td>文献[39]，文献[40]，文献[41]等</td></tr><tr><td>基于GraphSAGE的图学习方法</td><td>文献[21]，文献[43]，文献[48]等</td></tr><tr><td>基于GAT的图学习方法</td><td>文献[14]，文献[30]，文献[49]等</td></tr><tr><td>基于GGNN的图学习方法</td><td>文献[13]，文献[19]，文献[20]等</td></tr><tr><td rowspan=\"3\">会话兴趣学习</td><td>基于注意力机制的会话兴趣学习方法</td><td>文献[13]，文献[39]，文献[40]等</td></tr><tr><td>基于循环神经网络的会话兴趣学习方法</td><td>文献[44]，文献[48]，文献[49]等</td></tr><tr><td>基于位置的会话兴趣学习方法</td><td>文献[50]，文献[51]，文献[52]等</td></tr></table>",
    "context5": "(注：表中的代表性工作仅对典型研究进行陈列,并未将所有相关研究全部陈列。)"
  },
  "数据分析与知识发现": {
    "context1": "(1)对于会话图构建,现有研究主要包括基于单会话和基于多会话的图构建方法。然而，现有会话图模型大多仅关注项目之间的转移关系，在异质关系利用方面存在不足。推荐系统中，多源异构关系（如多类型交互关系、社交关系、知识图谱等)不仅为建模会话兴趣提供丰富的辅助信息，而且可以有效缓解会话序列的数据稀疏性。因此,如何融合多源异构关系丰富会话图结构值得进一步研究。",
    "context2": "(2)对于会话图学习，现有研究主要包括基于GCN、GraphSAGE、GAT和GGNN的4种学习方法。现有研究主要通过应用或改进这4种学习方法进行会话图学习。然而,现有方法大多为传统GNN框架的变体，无法克服GNN本身具有的缺陷，如GNN的过平滑[53]、鲁棒性较差[54]等问题。此外，一些复杂的会话图学习方法以增加计算量为代价获得性能增益[55],但是否采用复杂的传播方法应取决于计算代价和性能收益之间的权衡。",
    "context3": "(3)对于会话兴趣表示，现有研究主要包括基于注意力机制、RNN和位置的会话兴趣表示方法。然而,尚未有研究明确证明哪种会话推荐方法具有绝对优势。此外,现有研究在表示用户会话兴趣时，大多将会话兴趣单一且确定的表示。实际推荐场景中，用户会话兴趣可能是多样的[56]或不确定的[57],因此现有研究在用户兴趣不明确或用户兴趣多样时难以取得理想的会话推荐性能。"
  },
  "4评价资源": {
    "context1": "评价资源是客观评估科学研究的基础，全面整合评价资源有利于促进相关研究不断完善。通过梳理和整合相关研究中的实验设置,本文从数据集、评价标准和基线模型三个方面总结基于GNN的会话推荐方法相关的评价资源。"
  },
  "4.1数据集": {
    "context1": "在GNN会话推荐研究中，用于实验验证的数据集主要集中在5个应用领域：电子商务推荐、新闻推荐、音乐推荐、地点推荐和影视推荐。不同应用领域中典型数据集的统计信息如表5所示。",
    "context2": "表5典型数据集统计  \nTable 5Statistics for Typical Datasets",
    "context3": "<table><tr><td>应用领域</td><td>数据集名称</td><td>会话数量</td><td>交互数量</td><td>项目数量</td><td>平均长度</td><td>数据集获取网址</td></tr><tr><td rowspan=\"4\">电子 商务</td><td>Yoochoose</td><td>1 375 128</td><td>5 426 961</td><td>28582</td><td>3.95</td><td>https://www.kaggle.com/chadgostopp/recsys-challenge-2015</td></tr><tr><td>Tmall</td><td>1 774 729</td><td>13 418 695</td><td>425 348</td><td>7.56</td><td>https://tianchi.aliyun.com/dataset/dataDetail? dataId=42</td></tr><tr><td>Diginetica</td><td>780 328</td><td>982 961</td><td>43 097</td><td>5.12</td><td>https://competitions.codalab.org/competitions/11161</td></tr><tr><td>RetailRocket</td><td>59962</td><td>212182</td><td>31968</td><td>3.54</td><td>https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset</td></tr><tr><td rowspan=\"3\">新闻 推荐</td><td>GLEF 2017</td><td>16 444 442</td><td>5 540 486</td><td>742</td><td>3.37</td><td>https://www.newsreelchallenge.org/dataset</td></tr><tr><td>Globo</td><td>1 031167</td><td>2 930 849</td><td>13 092</td><td>2.84</td><td>https://www.newsreelchallenge.org/dataset</td></tr><tr><td>Adressa 16G</td><td>2215</td><td>62 908</td><td>6765</td><td>28.4</td><td>http://reclab.idi.ntnu.no/dataset</td></tr><tr><td>音乐</td><td>Last.FM</td><td>169 576</td><td>2 887349</td><td>449 037</td><td>17.03</td><td>http://millionsongdataset.com/lastfm/</td></tr><tr><td>推荐</td><td>Nowplaying</td><td>27 005</td><td>271 177</td><td>75 169</td><td>10.04</td><td>https://www.kaggle.com/chelseapower/nowplayingrs</td></tr><tr><td rowspan=\"3\">地点 推荐</td><td>Gowalla</td><td>*</td><td>245157</td><td>6871</td><td>*</td><td>http ://snap.stanford.edu/data/loc-gowalla.html</td></tr><tr><td>Foursquare</td><td>*</td><td>155365</td><td>2 675</td><td>*</td><td>https://www.kaggle. com/chetanism/foursquare-nyc-and-tokyo- checkin-dataset</td></tr><tr><td>Yelp</td><td>*</td><td>6990280</td><td>150 346</td><td>*</td><td>https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset</td></tr><tr><td rowspan=\"3\">影视 推荐</td><td>Movielens-1M</td><td>*</td><td>1000 209</td><td>3 706</td><td>*</td><td>http ://files.grouplens.org/datasets/movielens/</td></tr><tr><td>Kwai</td><td>*</td><td>775 834 643</td><td>310 681</td><td>*</td><td>https://cloud.tsinghua.edu.cn/d/eb0fc2ebab8d42189f3b/</td></tr><tr><td>Tiktok</td><td>*</td><td>1 253112</td><td>7085</td><td>*</td><td>https://www.kaggle.com/datasets/erikvdven/tiktoktrendingdecem- ber-2020</td></tr></table>",
    "context4": "（注：\\*表示数据集中未给出会话结构，需要依据研究人工划分会话结构 $^ \\circ$ ）"
  },
  "4.2评价指标": {
    "context1": "近期相关研究主要采用非位置敏感型指标和位置敏感型指标对会话推荐的准确性进行评价[3]。两类评价指标均致力于评估Top-K个推荐结果的准确性，但相对于非位置敏感型指标，位置敏感型指标在考虑推荐结果准确性的基础上，同时考虑了有效推荐项目的位置信息[58]。非位置敏感型指标主要包括：准确率(Precision）、召回率(Recall)和命中率(HitRatio,HR);位置敏感型指标主要包括：平均倒数排名（Mean Reciprocal Rank,MRR)和归一化折损累计收益（NormalizedDiscountedCumulative Gain,NDCG）。具体而言:Precision衡量用户真实交互的项目在Top-K个推荐结果中所占的比例;Recall衡量在Top-K个推荐结果中,用户真实交互的项目在其交互的所有项目中所占比例;HR 衡量Top-K个推荐结果是否命中用户真实交互的项目;MRR衡量第一个真实交互项目是否置于推荐结果的明显位置;NDCG根据推荐项目排名位置评估推荐结果的准确性。"
  },
  "4.3基线模型": {
    "context1": "在基于GNN的会话推荐方法研究中，通常有三类基线模型(传统会话推荐模型，基于RNN的会话推荐模型和基于GNN的会话推荐模型)用于对比实验。三类代表性基线模型的关键信息说明如表6所示。",
    "context2": "表6代表性基线模型的关键信息说明  \nTable 6Illustration of Key Information for Representative Baselines",
    "context3": "<table><tr><td>类别</td><td>模型</td><td>主要技术</td><td>年份</td><td>来源简称</td><td>源码获取网址</td></tr><tr><td rowspan=\"3\"></td><td>POp[59]</td><td>统计推断</td><td>1</td><td></td><td>https://github.com/rn5l/session-rec</td></tr><tr><td>传统会话推荐模型 Item-KNN[5]</td><td>K最邻近法</td><td>2001</td><td>WWW*</td><td>https://github.com/rn5l/session-rec</td></tr><tr><td>FPMC[6]</td><td>个性化分解的MC</td><td>2010</td><td>WWW*</td><td>https://github.com/khesui/FPMC</td></tr><tr><td rowspan=\"3\">基于RNN的 会话推荐模型</td><td>NARM[10]</td><td>基于注意力机制的RNN</td><td>2017</td><td>CIKM*</td><td>https://github.com/rn5l/session-rec</td></tr><tr><td>STAMp[60]</td><td>基于短期注意和记忆优先的RNN</td><td>2018</td><td>KDD*</td><td>https://github.com/rn5l/session-rec</td></tr><tr><td>GRU4REC[9]</td><td>基于门控循环单元的RNN</td><td>2016</td><td>ICLR*</td><td>https://github.com/rn5l/session-re</td></tr><tr><td rowspan=\"3\">基于GNN的 会话推荐模型</td><td>SR-GNN[13]</td><td>融合软注意力机制的GGNN</td><td>2019</td><td>AAAI*</td><td>https://github.com/CRIPAC-DIG/SR-GNN</td></tr><tr><td>TAGNN[20]</td><td>融合目标注意力机制的GGNN</td><td>2020</td><td>SIGIR*</td><td>https://github.com/CRIPAC-DIG/TAGNN</td></tr><tr><td>GCE-GNN[14]</td><td>融合位置感知注意力机制的GAT</td><td>2020</td><td>SIGIR*</td><td>https://github.com/CCIIPLab/GCE-GNN</td></tr></table>",
    "context4": "（注：一表示无特定文献支持的经验操作，\\*表示CCFA级会议。）"
  },
  "4.4评价资源总结与不足分析": {
    "context1": "本节内容的结构化总结如表7所示。",
    "context2": "(1)在数据集方面，现有研究已在不同应用场景下验证了GNN会话推荐方法的有效性，但数据集的采集年份相对陈旧。此外,数据偏见问题在现有数据集中较为常见。由于数据偏见是影响机器学习模型性能的关键因素[],因此如何消解会话推荐数据集中的偏见问题值得进一步探索。",
    "context3": "(2)在评价指标方面，现有研究主要采用非位置敏感型指标和位置敏感型指标对会话推荐的准确性进行评估，大多仅关注会话推荐的准确性评价。因此现有评价指标在多样性[8]、可解释性[69]、公平性[70]等方面的度量相对不足,这可能导致会话推荐方法面临\"信息茧房\"[71]、可信度差[]等困境。",
    "context4": "(3)在基线模型方面，现有研究主要采用非深度会话推荐模型、基于RNN的会话推荐模型以及基于GNN的会话推荐模型进行对比实验。自2022年以来，基于GNN的会话推荐方法不断成熟,基于GNN的会话推荐模型在基线模型中占比不断变多，其他会话推荐模型的占比不断变少。然而，现有研究的基线模型尚未结合特定实现流程进行细致化分类，导致对比实验结果缺乏说服力。未来研究可结合本文归纳的研究框架(即会话图构建、会话图学习和会话兴趣表示)对基线模型进行细致化分类，更有针对性地评估新型研究的创新。"
  },
  "5未来趋势": {
    "context1": "尽管基于GNN的会话推荐已取得一定进展,但现有GNN会话推荐方法的相关研究数量仍然递增态势。基于现有研究的不足，结合推荐系统的领域实时研究热点，给出基于GNN的会话推荐方法的未来研究趋势。"
  },
  "5.1基于GNN的可解释性会话推荐方法": {
    "context1": "可解释性推荐不仅可为用户提供准确的推荐结果,还可以为用户提供推荐解释,辅助用户或系统设计人员了解推荐原因,进而做出科学决策[72]。由于"
  }
}