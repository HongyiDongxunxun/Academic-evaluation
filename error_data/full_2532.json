{
  "original_filename": "full_2532.md",
  "可解释机器学习在信息资源管理领域的应用研究综述": {
    "context1": "刘智锋王继民",
    "context2": "（北京大学信息管理系 北京100871)",
    "context3": "摘要：【目的】对可解释机器学习方法及其在信息资源管理领域的应用研究进行梳理和总结,发现不足并做出展望。【文献范围】使用可解释机器学习的相关关键词构建检索式,在中国知网和Webof Science等平台中检索,根据纳入排除标准,共获取44篇相关文献进行评述。【方法】从机器学习流程出发,构建一般性的可解释机器学习框架,重点梳理可解释机器学习方法分类,然后对可解释机器学习在信息资源管理领域的应用现状进行归纳总结。【结果】一般性的可解释机器学习框架包含事前解释、可解释模型以及事后解释三个不同的模块;事后可解释方法在健康信息学、网络舆情、科学计量学以及社交网络用户行为等领域具有广泛的应用,其中常用的方法为SHAP和特征重要性分析;现有研究存在应用方法单一和融合不足、因果关系探究不够、针对多源异构数据的解释不足以及领域应用有待拓宽等问题。【局限】本文重点关注可解释机器学习的应用及存在的不足,未对算法原理进行深入阐述。【结论】未来研究应加强可解释机器学习方法的融合使用，探究基于因果机器学习的可解释机器学习,引人面向多源异构数据的可解释机器学习方法,拓宽在信息推荐、信息检索和信息计量等多个领域的应用。",
    "context4": "关键词：可解释机器学习SHAP信息资源管理研究进展  \n分类号：TP181G203  \nDOI: 10.11925/infotech.2096-3467.2023.0244",
    "context5": "引用本文：刘智锋，王继民.可解释机器学习在信息资源管理领域的应用研究综述[J].数据分析与知识发现，2024，8（1)：16-29.(Liu Zhifeng, Wang Jimin.Review of Interpretable Machine Learning for InformationResource Management[J]. Data Analysis and Knowledge Discovery,2024, 8(1): 16-29.)"
  },
  "1引言": {
    "context1": "近年来，以深度学习为核心的机器学习技术持续创新并广泛应用于学术研究[H。现有的大部分机器学习模型往往表现为黑盒模型，虽然可以根据输入给出相应的预测,但很难揭示其决策依据[2]。在诸多研究场景中，需要深入探讨各因素之间的关联与影响机制,例如社会科学领域会关注影响某个社会现象的因素，生物医学领域关注影响疾病发生的重要因素等。为了更好地解决此类研究问题,促进社会与科学规律的发现,机器学习模型不仅需要具备较高的准确性,更需要具备可解释性,为从海量数据中揭示内在影响机制和机理等方面的研究提供关键技术支持。",
    "context2": "美国国防部高级研究计划局于2016年启动\"可解释的人工智能\"项目，旨在构建具有可解释性的人工智能系统[3]。我国于2017年印发了《新一代人工智能发展规划》,提出要实现高可解释性的人工智能[4]。2019年,我国发布了《新一代人工智能治理原则一—发展负责任的人工智能》,明确指出需要不断提高人工智能的可解释性[5]。在各国纷纷出台可解释机器学习相关研究计划的这一背景下,机器学习的可解释方法得到深入研究，并逐步应用于各领域的机器学习模型解释。",
    "context3": "机器学习已在信息资源管理领域得到广泛应用，如学术文本分类[6-7]、信息抽取[8]、情感分析[]等多个任务场景,有效地解决了海量数据处理和复杂关系挖掘等方面的挑战。随着对算法可靠性和透明度的要求不断提高，以及对机器学习模型决策过程进行解释和理解的需求日益增长,可解释机器学习在该领域逐渐受到重视。虽然已有学者对可解释机器学习进行梳理和综述，如靳庆文等从人工智能(ArtificialIntelligence,AI)算法治理的视角出发,梳理相关算法及个别典型应用案例[10];张成洪等总结了可解释人工智能对管理领域的影响[],然而，尚未有研究系统地梳理可解释机器学习在信息资源管理领域的应用现状。",
    "context4": "当前,信息资源管理领域的数据量快速增长，对数据驱动知识发现的研究需求持续上升。现有的大部分机器学习方法常用于该领域的分类、回归预测和聚类等任务；这些研究更多关注预测结果的准确性，往往无法深入剖析问题背后的机制和影响因素，此类黑盒模型限制了知识提取和规律发现的能力。相较之下,可解释机器学习着重于揭示模型的决策过程和内在机制,如在文本分类中,通过分析模型内部的权重和特征选择等,使分类的原因更好理解;在回归预测中，可以解释影响预测结果的关键因素，为决策提供依据。可见,可解释机器学习有助于信息资源管理领域的学者更好地洞察数据中隐藏的知识结构,进而支持知识发现研究。因此,本文通过对国内外相关文献的调研,系统总结可解释机器学习方法在信息资源管理领域的应用情况,发现存在的不足并提出未来展望，为该领域的相关研究提供参考和借鉴。"
  },
  "2相关概念与文献筛选": "",
  "2.1相关概念": {
    "context1": "解释是人类认知的过程，可解释性则反映了人类认知过程中的可理解程度。目前,关于机器学习可解释性尚无统一标准的定义[12],但可从不同角度对其进行理解。从技术的视角来看，机器学习的可解释性可以视为开发可解释的方法、技术和工具以揭示机器学习模型的决策逻辑；从用户的视角来看，机器学习可解释性则体现为人们对机器学习模型决策过程的理解程度[13]。用户是机器学习应用的终端使用者，从用户中心视角理解机器学习模型已被广泛认同。与可解释机器学习相关的概念有可解释人工智能（Explainable Artificial Intelligence,XAI）,最早于2004年提出[14]。可解释人工智能旨在构建可理解的AI系统，在保持预测准确性的同时，模型具有可解释性,使得用户能够理解和信任[15],其内涵与可解释机器学习基本一致。"
  },
  "2.2 文献筛选": {
    "context1": "本文对可解释机器学习方法在信息资源管理领域的应用现状进行梳理,纳入分析的文献标准为：",
    "context2": "(1)论文中应用了至少一种可解释机器学习方法；",
    "context3": "(2)排除关于可解释机器学习的综述性论文和理论研究论文。",
    "context4": "检索时间为2022年12月18日。中文文献来源于中国知网。根据可解释机器学习的相关概念以及方法，通过不断尝试,制定检索式为：( $\\mathrm { S U } =$ 可解释人工智能）OR（ $\\mathrm { S U } =$ 可解释机器学习）OR（ $\\mathrm { S U } =$ SHAP OR SU $\\left\\lceil = \\right.$ LIMESU $\\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } } = \\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } }$ 部分依赖图ORSU $\\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } } = \\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } }$ 置换特征重要性）OR（（ $\\mathrm { S U } { = } { }$ 特征重要性ORSU $\\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } } = \\underline { { \\underline { { \\mathbf { \\delta \\pi } } } } }$ 特征分析ORSU $\\left[ = \\right.$ 影响因素）AND SU $\\mathbf { \\bar { \\rho } } = \\mathbf { \\rho }$ 机器学习），从中筛选出发表在信息资源管理领域18本中文社会科学引文索引来源期刊(CSSCI)的共19篇文献，发表时间在2014年至2022年。英文文献主要来自Web ofScience数据库。检索式为： $\\mathrm { T S } =$ ( XAI ORExplainable Artificial Intelligence OR Interpretable AIORInterpretableArtificialIntelligenceORInterpretable Machine Learning OR IML OR SHAPORShapley Values OR LIME OR PermutationFeature Importance OR Partial Dependence Plot)AND WC $\\circeq$ (Information Science & Library Science）,检索结果为116条，排除不相关文献以及综述文献等，获得24篇文献。",
    "context5": "此外，通过回溯检索等，最终获取44篇相关文献作为研究对象。"
  },
  "3可解释机器学习框架与方法": "",
  "3.1可解释机器学习框架": {
    "context1": "常见的机器学习过程通常包含问题提出与任务定义、数据获取、数据预处理、特征提取、模型训练与评估等环节。当模型预测具备较高的准确率时，能较好地解决传统回归和分类问题。在信息资源管理领域,机器学习方法广泛应用于回归、分类等问题，并取得了不错的效果[16]。然而,由于缺乏对模型决策原因的理解，大部分机器学习模型并不能有效地从海量数据中揭示内在机制,从而获取知识。",
    "context2": "通过对机器学习模型进行解释，可以分析模型决策背后的逻辑，从而促进从海量数据中自动揭示内在机制，加速对研究问题的理解。可解释机器学习的一般框架如图1所示。在机器学习流程基础上，首先在数据预处理阶段采用主成分分析、聚类分析等方法，并结合可视化技术深化对数据分布的理解,为选取合适的特征与模型构建提供指导。然后，采用可解释机器学习模型或事后解释方法进一步解释模型，探究不同特征对模型预测的影响，进而促进对研究问题和任务的理解。此外,模型解释还可为特征构建与模型训练提供反馈与优化方向。",
    "context3": "![](images/f632914219196e186bef070a0ffc30aab2cdce1d0dcbbd43c225996b14e93483.jpg)  \n图1可解释机器学习框架  \nFig.1Framework for Interpretable Machine Learning"
  },
  "3.2可解释机器学习方法": {
    "context1": "根据实际任务需求，可解释机器学习方法可大致分为自解释模型和事后解释方法两类[17],如图2所示。自解释模型通常是指结构相对简单且易于理解的模型,例如线性模型、逻辑回归模型、决策树等;事后解释方法则指在模型评估之后，采用与模型无关的或特定模型的解释方法对准确率最高的模型进行解释。",
    "context2": "![](images/7371437d47fc5b7e40336393dc0a93824170ea63a34ee9b33df16d64996bb6f2.jpg)  \n图2可解释机器学习方法分类  \nFig.2Classification of Interpretable Machine Learning Methods",
    "context3": "(1)自解释模型实现可解释机器学习最简单的途径是仅采用可解释的机器学习模型。常见的自解释模型有回归模型、决策树、注意力机制等。回归模型包括线性回归"
  },
  "18 数据分析与知识发现": {
    "context1": "以及逻辑回归等广义的线性回归。线性回归的预测目标由各个特征的加权和得到,因此各个系数的绝对值大小代表相应特征对预测目标的影响程度；逻辑回归是一种广义的线性模型,可以通过对数几率变换将预测目标转化为特征的加权和,因此,能够通过几率比的大小解释不同因素对结果的影响程度。",
    "context2": "决策树模型适用于分类和回归任务，常见的决策树算法有ID3、C4.5和CART等。决策树模型根据特征的重要性进行分支，直至获取所需结果。常见的特征重要性衡量指标有信息增益和基尼系数，其中,信息增益表明选择特定特征后,信息不确定性的降低程度;基尼系数则反映特征提高节点样本纯度的能力。此外,注意力机制一方面可以更精准地捕捉环境信息;另一方面，可以通过注意力的大小判断特征重要性，从而实现模型在一定程度上的可解释性。"
  },
  "(2)事后解释方法": {
    "context1": "由于模型较为简单，自解释模型在处理复杂数据时很难获得较高的准确性。然而，像集成学习、神经网络等具有较强预测能力的复杂模型，其解释性通常较差[18]。为解释复杂的模型,常采用事后解释方法，这类方法主要是针对已经训练好的模型进行解释,也适用于自解释模型。根据事后解释方法是否应用于特定的模型,可将其分为模型无关的解释方法和特定模型的解释方法。",
    "context2": "$\\textcircled{1}$ 模型无关的解释方法",
    "context3": "模型无关的解释方法已广泛应用于各种场景，主要包括部分依赖图（PartialDependence Plot,PDP）、个体条件期望图（IndividualConditionalExpectation,ICE）、置换特征重要性（PermutationFeatureImportance，PFI）、局部代理模型（LocalInterpretable Model-Agnostic Explanations,LIME）和夏普利加性解释（SHapley Additive exPlanations,SHAP)等方法。",
    "context4": "部分依赖图(PDP)展示了特征与预测目标之间的依赖关系，即特征对机器学习模型预测结果的边际效应。PDP的算法流程主要分为三个步骤：第一，选取1\\~2个需分析的特征并定义特征值的搜索范围;第二,将特征的不同取值分别代入预测函数进行预测,再对预测结果取平均;第三,以特征取值为横坐标、预测结果为纵坐标绘图,得到 $\\mathrm { P D P } ^ { [ 1 9 ] }$ 。",
    "context5": "个体条件期望图(ICE)主要分析单个实例的特征与预测结果的依赖关系，其计算原理与部分依赖图相同[20]。相较于PDP,ICE图中的每条曲线代表一个样本，能反映不同样本间的异质性。PDP和ICE通常只能反映一个或两个特征与预测结果之间的关系，它们均存在分析的特征与其他特征之间独立的假设，现实应用中可能存在一定的分析偏差。",
    "context6": "置换特征重要性通过计算特征置换前后模型预测误差的变化来衡量特征的重要性，如特征值的变化导致预测误差大幅变化，则表明该特征具有较高的重要性;反之,若特征值的变化对预测误差影响甚微,则说明该特征的重要性较低。特征重要性常采用条形图进行表示[21]。置换特征重要性最早应用于随机森林算法，后来Fisher等提出了与模型无关的版本[22]。该方法在计算某特征重要性时,得到的结果既包含该特征的净效应，也包含与其他特征的交互效应。因此只有当分析的特征相对独立时,该方法的分析结果才准确[23]。",
    "context7": "另一个解释思路是采用白盒模型来解释黑盒模型,该方法称为代理模型。如果白盒模型能近似表示整个黑盒模型的预测，则称为全局代理模型（Global Surrogate），可以采用相同的数据集训练一个近似于黑盒模型的可解释模型,并用 $R ^ { 2 }$ 等指标衡量代理模型的拟合程度。若代理模型仅模拟黑盒模型的一部分以解释单个样本的预测,则称之为局部代理模型(LIME）。2016年,LIME由Ribeiro 等提出[24],通过构建局部可解释模型,分析样本输入特征与预测结果之间的关系。LIME适用于SVM、XGBoost、深度神经网络等多种机器学习模型的解释,同时支持表格型数据、文本以及图像等不同数据类型的解释,具有较为广泛的应用。",
    "context8": "Shapley值由Shapley等提出[25],源自合作博弈理论，旨在根据玩家对总支出的贡献为各个玩家分配支出。一个玩家的贡献为该玩家加入后给总分带来的变化,Shapley值则是玩家所有可能加入情况下贡献的加权和。在机器学习模型中，特征可以类比为不同的玩家，通过类似的分配方式，计算特征对模型预测结果的贡献程度,以表示不同特征的重要性。2017年,Lundberg 等基于 Shapley值提出 SHAP[26]通过计算每个特征对预测的贡献来解释预测。SHAP可以利用瀑布图、力图、特征重要性图、特征概要图以及特征依赖图等可视化手段,清晰地提供样本与特征维度的解释。其中,瀑布图可以反映单个实例的特征取值对预测结果的影响大小和方向;特征重要性图和概要图可以显示每个特征对预测结果的影响程度;特征依赖图则可以揭示单个或两个特征与预测结果之间的依赖关系。",
    "context9": "$\\textcircled{2}$ 特定模型的解释方法",
    "context10": "深度学习已在图像识别、语音识别和自然语言处理等领域取得显著成果。由于深度神经网络为高度非线性模型，且参数量非常大，与传统的机器学习算法相比,其解释性更差;因此,当前特定模型的解释方法主要聚焦于深度神经网络。根据解决的任务不同,神经网络的解释存在较大差异，重点梳理应用于图像识别和自然语言处理神经网络模型的常见解释方法。",
    "context11": "面向图像识别与分类的神经网络解释方法可分为全局解释和局部解释方法，全局解释方法关注神经网络模型的解释，即模型如何对数据进行表示，以及神经网络不同层和不同神经元的功能;局部解释方法关注神经网络对单个实例预测结果的解释[27]。根据不同的解释粒度,全局解释方法可以进一步细分为模型级和神经元级解释方法，包括网络压缩[28]和激活最大化[29]等。局部解释方法可以分为图像级、概念级以及像素级，图像级的解释方法有基于实例的解释[30]和网络反演[31];概念级主流方法为基于概念激活向量的解释[32];像素级方法以像素为基本单位进行解释，常见的有SaliencyMap[33]SmoothGrad[34]等方法。总体而言,面向图像的神经网络解释方法主要通过将网络学习的特征映射到人类可以理解的语义空间，从而增加模型的透明度。",
    "context12": "面向自然语言处理的神经网络模型解释方法主要关注BERT等大模型的解释。预训练语言模型在文本向量化、文本分类、自动摘要等多项任务中取得显著效果，开启了基于预训练语言模型的研究新范式。注意力机制和自注意力机制的应用是预训练语言模型在语言建模取得成效的主要原因之—[35]。因此,许多研究聚焦于注意力机制的解释。一方面,通过设计具有可解释单元的注意力机制模型来增加模型的透明度，如罗望成等提出一种基于注意力的可解释混合深度学习模型,用于心率失常分类[36]。另一方面，开发注意力机制的解释方法与技术，如注意力分数图，注意力分数越高表示相应的词越受到关注[37]。Clark等通过可视化等技术分析了BERT模型不同注意力头关注和捕捉的词和句法信息[38]。可见，面向自然语言处理的预训练语言模型解释方法主要通过分析注意力机制的关注点进行解释。"
  },
  "4可解释机器学习在信息资源管理领域的应用现状": {
    "context1": "可解释机器学习有助于提高机器学习模型的透明度，并在诸如影响因素分析等研究中发挥作用。通过对44篇相关文献的梳理和归纳，发现可解释机器学习已广泛应用于网络舆情分析、社交网络用户行为分析、健康信息学研究、科学计量研究等领域，如图3所示。健康信息学和网络舆情研究的占比最大,均达到 $2 5 \\%$ ;其次为社交网络用户行为和科学计量学相关研究，占比分别为 $1 6 \\%$ 和 $14 \\%$ ;此外,商业分析、应急情报、房价预测和智库研究等其他领域占比 $20 \\%$ 。",
    "context2": "![](images/1cf6312a4f3260e8716119adb53013d275d2586c4f45e392e31551b5b1a31a67.jpg)  \n图3可解释机器学习在信息资源管理领域应用研究主题分布  \nFig.3Topics Distribution of Interpretable Machine Learning for Information Resource Management",
    "context3": "可解释机器学习在信息资源管理领域应用的研究方法如图4所示，事后解释方法的使用频次为41，而使用自解释模型的频次仅为8。采用的事后解释方法包括 SHAP、特征重要性、PDP、ICE、LIME和反向传播(BP)等,其中使用频次最高的为SHAP,占比达到 $5 1 . 2 \\%$ ;其次是特征重要性，占比为 $3 4 . 1 \\%$ 。因此，可以看出信息资源管理领域主要采用SHAP和特征重要性分析等方法对机器学习模型进行解释。",
    "context4": "![](images/a93db3a710b3767dc8f879ae18064ae8dbc0b9d72f35375ebdbe4fe80a0a445e.jpg)  \n图4可解释机器学习在信息资源管理领域应用研究方法分布  \nFig.4Methods Distribution of Interpretable Machine Learning for Information Resource Management"
  },
  "4.1健康信息学研究": {
    "context1": "随着人工智能 $^ +$ 医疗健康等政策的推行，健康信息学作为一门信息技术与健康医疗领域的交叉学科，在信息资源管理领域受到广泛关注[39],机器学习等技术已被广泛应用于疾病预警、辅助诊断及医疗管理等方面。然而,医疗健康领域决策风险较高，需要明确的决策依据。机器学习模型作为黑盒模型，透明度不够，在一定程度上阻碍了其在该领域的应用与推广。因此,信息资源管理领域的学者尝试将可解释机器学习应用于健康信息学领域的研究，分析模型的决策依据，从而增加用户的信任度。",
    "context2": "在患病与诊断预测方面,徐良辰等采用Bagging集成学习算法预测胃癌5年生存情况,并采用SHAP对模型进行分析，挖掘胃癌5年生存率的影响因素[40]。类似地,Rodriguez等基于COVID-19 患者的电子病历数据，采用梯度提升树等算法预测患者是否需要机械通气、肾脏替代治疗以及再入院等，然后采用SHAP可解释框架分析各个预测模型的重要影响因素[41]。车宏鑫等发现XGBoost在前列腺癌预测中表现最佳，并基于置换特征重要性揭示影响前列腺癌预测的关键因素[42]。Chen等采用支持向量机、随机森林、XGBoost、多层感知机和长短期记忆网络等方法对中医诊疗的涩脉进行分类预测，并通过特征重要性表示特征对预测的影响大小[43]。Yu等利用SHAP解释框架分析得到呼吸状态等信息特征对死亡预测具有重要作用[44]。",
    "context3": "尽管在大部分任务中,深度学习具有更出色的预测性能,但其解释性相对较差。因此,有学者专注于深度学习在患病预测方面的可解释性研究。例如,Amrollahi等采用前馈神经网络预测脓毒血症患者的再入院情况,并采用基于梯度计算的相关性分数反映重要特征，结果表明，除了临床相关特征外，人口统计学等特征对预测结果也具有重要影响[45]。Liu 等将长短期记忆网络（LongShort-TermMemory,LSTM)和LightGBM融合，用于新发谵妄预测,并利用SHAP揭示对预测结果具有重要影响的前20个特征[46]。Kwak等在循环神经网络基础上,加入自注意力机制,提出融合多源异构数据的可解释心血管疾病预测模型[47]。",
    "context4": "在医疗管理方面,Zhang等利用电子病历访问日志数据构建患者隔天出院概率,并使用SHAP分析对预测结果有重要影响的前20个因素，从而理解模型的决策依据[48]。Jiao等将结构化和非结构化特征融合，采用混合密度神经网络预测病例的手术时间,并通过置换特征重要性方法分析发现计划时间和程序名称是模型最重要的特征[49]。Catling等采用时序卷积神经网络预测重症监护中的拔管、插管和死亡等事件,并借助PDP分析模型所学习的特征与预测结果之间可能的临床关联[50]。综上,SHAP等事后解释方法在疾病诊断和管理领域的机器学习模型解释上具有广泛的应用。"
  },
  "4.2网络舆情研究": {
    "context1": "随着移动互联网和社交媒体的快速发展，传统社会舆论已转向网络舆情，网民的信息获取与传播机制发生了深刻变革[51]。因此,网络信息,特别是网络谣言等虚假信息传播机制受到信息资源管理领域学者的广泛关注。许多研究尝试运用可解释机器学习方法分析网络舆情传播机制和影响因素,为舆情管控提供策略。如针对热点事件微博舆情反转现象,安璐等结合XGBoost和特征重要性计算等方法，探究该现象背后的影响因素[52]。",
    "context2": "谣言治理是网络舆情研究的重点领域,学者将可解释机器学习方法应用于社交网络谣言识别等研究。如曾子明等融合微博用户特征、情感特征和微博特征等，采用LightGBM算法对突发公共卫生事件中的潜在谣言传播者进行识别,并利用SHAP方法对模型进行解释，以理解谣言传播者识别的重要影响因素[53];Sun等结合XGBoost模型和SHAP解释方法，研究谣言识别的影响因素[54];采用类似方法，Li等分析了社交网络谣言传播生命周期的长度、峰值影响因素以及辟谣有效性指数影响因素[55-56]；位志广等采用随机森林算法结合特征重要性,研究网民健康谣言分享意愿的影响因素[57]。",
    "context3": "随着自然语言处理技术在网络舆情分析中的广泛应用，学者开始探究自然语言处理模型的可解释性，并将其应用于虚假信息检测等方面。如Tao等提出一个面向文本分类的可解释深度学习框架，并将其应用于虚假新闻检测[58]；Ayoub等基于DistilBERT和SHAP,提出一个可解释的自然语言处理模型,用于识别社交媒体上与COVID-19相关的虚假信息[59];Uyheng等基于心理语言学特征等，提出一种理论驱动且可解释的机器学习模型,用于网络恶意语言的识别和解释[60]。此外,许多学者关注社交媒体政务信息的分析。如安璐等采用SHAP方法对表现效果最佳的LGBMRegressor模型进行解释,揭示政务微博信息发布有效性的影响因素[61]；易明等结合XGBoost和 SHAP,提出政务新媒体公共价值共识识别的可解释模型,揭示政务新媒体公共价值共识的影响机制[62]。综上，可解释机器学习可以有效揭示虚假信息传播等机制以及政务信息价值与有效性的影响因素。"
  },
  "4.3社交网络用户行为分析": {
    "context1": "在Web2.0环境下，社交网络用户行为发生了深刻的变化。除了浏览和查询网络信息资源,用户还可以对其进行描述、标注、评论等,并生成各种用户内容。许多信息资源管理领域的学者利用可解释机器学习研究用户评论内容、社会化标注行为以及特殊群体的用户社交行为。",
    "context2": "用户评论行为产生了海量内容,如何有效挖掘用户评论内容成为一大挑战。可解释机器学习为解决用户评论挖掘问题带来了新视角。如Kim等结合文本语义、情感等特征,采用集成学习算法预测在线消费者评论有用性,同时综合PDP、置换特征重要性和 SHAP等方法对模型进行解释，发现评论情感和评论者声望对评论有用性影响最大[21]。类似地,杨东红等采用随机森林、朴素贝叶斯、神经网络等算法识别电商平台评论信息的有用性,并基于特征重要性发现评论回复是影响预测模型的最重要因素[63]。Wang等采用轻量级梯度提升机算法（LightGradientBoostingMachine,LGBM)等机器学习模型对消费者的产品推荐文章人气进行预测，并使用SHAP对预测模型进行分析和可视化，发现相对于文章相关特征,作者相关特征对预测结果影响更大[64]。尹丽春等基于可自解释的决策树算法挖掘图书在线评论，揭示影响图书消费者满意度的因素及其作用机理[65]。",
    "context3": "社会化标注系统允许用户根据自我认知为网络信息资源打标签,高质量的标签对于网络信息资源的高效组织和管理具有重要作用。张云中等构建了包含标注主体、对象、环境、动机等多维度的标签质量影响因素模型,并结合随机森林算法和特征重要性揭示影响标签质量的关键因素[6]。采用类似的方法，门秀萍等基于用户在社交网络上的语言和行为特征预测其是否为抑郁症患者，并分析各个特征的重要性[]。此外,沈洪洲等采用决策树等算法对社交关系强度进行分类预测,并根据信息增益等指标对影响因素进行排序[8]。综上,SHAP等可解释机器学习方法在用户行为影响因素和作用机理研究中发挥了重要作用。"
  },
  "4.4科学计量学研究": {
    "context1": "随着人工智能技术与方法在科学计量学领域应用的兴起,逐步形成了AI+Informetrics新的交叉研究领域[9]。作为理解人工智能决策的重要手段，可解释机器学习拓展了科学计量学研究的方法与视角，逐渐受到科学计量学领域学者的关注。与采用回归分析等方法研究引用影响因素不同，部分学者结合论文文本和元数据等特征，采用随机森林、神经网络等算法预测学者引用，并采用特征重要性、LIME和SHAP等多种可解释方法分析模型,进而揭示影响论文被引的机制[70-71]。有研究将可解释机器学习应用于高质量专利的识别,并使用SHAP可解释框架探究影响专利质量的重要因素[72-73]。此外，Zeng等结合注意力机制和双向长短期记忆网络对论文句子进行分类预测,判断是否需要引证,并采用特征重要性分析等方法揭示影响预测的因素[74]。Ma等结合XGBoost等算法和SHAP揭示论文在社交媒体的传播机制。由此可见，可解释机器学习在科学计量学研究领域具有较大发展潜力[75]。"
  },
  "4.5其他相关主题": {
    "context1": "可解释机器学习在商业分析、反恐预警、智库研究和房价预测等其他多个领域亦有应用。在商业分析方面,Zhang 等综合应用PDP、ICE和 SHAP等解释方法，从全局和局部等多个视角解释财务危机预测模型,揭示引发财务危机的关键因素[76]。Wang等开发了一个可解释的广义可加神经网络模型，用于评估不同类型的营销活动和预算分配对商场客流量的影响[77]。Wang等采用Shapley值分析影响借贷平台风险预测的重要因素[78]。此外,Liu等结合注意力机制提出可解释的神经网络模型,用于从金融文本预测股东对内幕交易的诉讼[79]。",
    "context2": "在反恐预警方面，有研究结合集成学习算法和特征重要性分析，构建恐怖袭击预警模型，探讨影响预测的重要因素[80-81]。在房价预测方面,有研究综合应用XGBoost和 SHAP,分析影响房价的重要因素[82-83]。此外,张云中等采用信息增益计算影响智库舆论引导力的因素相对重要性，为提升新型智库引导力提供理论基础[84]。"
  },
  "5现存问题与研究展望": {
    "context1": "在学术大数据时代背景下,数据驱动的研究范式在信息资源管理领域逐渐成为主流,机器学习技术已广泛应用于各类问题的研究。然而,大部分机器学习模型在透明度方面表现不佳,限制了其在解析内在机制和影响因素研究方面的应用。因此,学者开始关注机器学习模型的可解释性，以期在保持预测准确性的同时，提供对模型内部工作原理的深刻理解，进而推动基于可解释机器学习的机理研究和知识发现。",
    "context2": "本文阐述了可解释机器学习的框架,将可解释方法分为自解释模型和事后解释方法两大类，并对常见的方法进行介绍。在此基础上,对可解释机器学习在信息资源管理领域的应用现状进行梳理总结。研究发现,SHAP与特征重要性分析等事后解释方法在信息资源管理领域采用率高，主要应用于健康信息学、网络舆情、科学计量学、社交网络用户行为分析等研究领域,揭示预测结果的重要影响因素及其作用机制。"
  },
  "5.1现存的主要问题": {
    "context1": "尽管可解释机器学习在信息资源管理领域的应用取得了一定进展,但是仍存在一些不足之处：",
    "context2": "(1)可解释机器学习方法应用与融合不足：当前应用于信息资源管理领域研究的可解释机器学习方法较为单一，多集中在特征重要性分析、SHAP等。尽管这些方法在一定程度上揭示了机器学习模型的决策过程，但对于深度学习模型和复杂结构的数据解释能力较为有限。深度学习模型往往具有多个隐藏层和神经元,内部结构较为复杂且难以理解，需采取积分梯度(Integrated Gradients)等方法。此外，现有研究很少将不同可解释方法进行有效融合，可能导致解释结果局限于单一视角。",
    "context3": "(2)可解释机器学习方法对因果关系探究不足：现有的可解释机器学习方法主要基于相关性或统计分析,这些方法一定程度上揭示了特征与预测结果之间的关联关系,但难以揭示它们之间的因果关系。因果关系是描述现实世界中因果机制的核心概念，缺乏因果关系的探究将限制可解释机器学习对于潜在作用机制的揭示,可能导致错误的结论和偏见，进而导致模型预测结果及其解释可靠性低,影响信息资源管理领域的知识发现和决策支持。",
    "context4": "(3)针对多源异构数据的解释不足：在信息资源管理领域,数据呈现多源异构的特点，除了结构化数据,还有非结构化数据。当前研究主要集中于结构化数据,解释不同特征对预测结果的影响,而对于非结构化数据如自然语言的解释较少;此外,当前应用主要面向文本数据的解释，但信息资源管理领域涉及多种类型的数据如图像等多模态数据，有必要拓展面向多模态数据的解释。",
    "context5": "(4)可解释机器学习在信息资源管理领域应用不足：虽然在健康信息学、网络舆情、网络用户行为以及科学计量学等领域已有一定应用,但可解释机器学习在信息资源管理领域的应用仍有待进一步开发。可解释机器学习在信息资源管理的重要研究方向,如信息推荐、信息检索、信息计量等应用相对较少，这些领域以往更注重算法的准确率和效率,对可解释性关注较少,使得用户无法深入理解推荐、检索以及评价的结果。"
  },
  "5.2研究展望": {
    "context1": "针对当前可解释机器学习在信息资源管理领域应用存在的不足，提出以下建议和研究展望：",
    "context2": "(1)加强可解释机器学习方法融合应用：可以引人更多的可解释机器学习方法，如将知识图谱与可解释机器学习相结合。知识图谱通过实体之间的关联关系和层次结构来表示知识，蕴含丰富的语义信息，可用于机器学习建模之前的特征及其关系的抽取,以及模型构建之后的推理,进而应用于神经网络等模型的解释[85]。因此,将知识图谱和深度神经网络相结合，开展可解释机器学习研究,将成为未来的一个重要发展方向。此外，可以在一个研究中同时应用多种不同的机器学习模型解释方法，从不同角度剖析模型的内部机理，获得更全面的解释。",
    "context3": "(2)探究基于因果机器学习的可解释机器学习：现阶段，信息资源管理领域所采用的自解释模型和事后解释方法主要通过分析某个特征与预测结果的相关性来进行解释,仅能回答特征对模型预测的影响,无法得到具有因果关系的结论。然而,基于反事实的可解释机器学习可以通过分析数据生成过程中的变化,进而挖掘潜在的作用机制,并给出具有真实性原因的解释[86]。因此,在未来的可解释机器学习研究中,有必要引入因果机制,提高对潜在作用机制的理解，从而为决策者提供更为可靠的依据。",
    "context4": "(3)引入面向多源异构数据的可解释机器学习方法：为更好地解释信息资源管理领域的多源异构数据，有必要引入面向自然语言和图像等数据的可解释方法。在图像数据解释方面，采用 SaliencyMap、SmoothGrad等方法解释图像中重要区域对模型预测的影响。在自然语言解释方面，设计具有可解释单元的注意力机制模型并对注意力机制进行可视化,捕捉自然语言中的关键信息和关联关系。",
    "context5": "(4)拓宽可解释机器学习方法在信息资源管理领域的应用：深入理解领域的需求,发现可解释机器学习在信息资源管理领域新的应用场景。比如信息推荐领域,图书馆可以借助可解释机器学习方法，通过分析和建模用户的阅读数据、兴趣偏好、社交网络等信息，为用户提供精准且可解释的个性化图书推荐系统,让用户了解推荐背后的原因;在信息检索领域,可解释机器学习可用于评估文档与查询之间的相关性,并向用户解释为何检索结果与查询式相关;在信息计量领域，可以应用可解释机器学习揭示引用行为的内在动机,研究引用模式背后的原因,为科研成果影响力的评价提供理论支持。"
  },
  "参考文献：": {
    "context1": "[1] Gunning D, Stefik M,Choi J,etal.XAI—Explainable Artificial Intelligence[J]． Science Robotics,2019,4(37).DOI: 10.1126/ scirobotics.aay7120.   \n[2] Cambria E,MalandriL,Mercorio F,etal.A Survey on XAI and Natural Language Explanations[J]. Information Processing & Management,2023,60(1):Article No.103111.   \n[3] DARPA-Explainable Artificial Intelligence (XAI) Program[EB/ OL].[2023-08-18]. https://www.darpa.mil/program/explainableartificial-intelligence.   \n[4] 新一代人工智能发展规划[EB/OL].[2023-08-18].https://www. gov.cn/zhengce/zhengceku/2017-07/20/content_5211996.htm. (New Generation Artificial Intelligence Development Plan[EB/ OL].[2023-08-18]. https://www.gov.cn/zhengce/zhengceku/2017- 07/20/content_5211996.htm.)   \n[5] 发展负责任的人工智能：新一代人工智能治理原则发布[EB/ OL].[2023-08-18]. http://www.gov. cn/xinwen/2019-06/17/ content_5401006．htm.(DevelopingResponsible Artificial Intelligence:New Generation of AI Governance Principles Released[EB/OL].[2023-08-18].http://www.gov.cn/xinwen/2019- 06/17/content_5401006.htm.)   \n[6] 罗鹏程,王一博,王继民.基于深度预训练语言模型的文献学 科自动分类研究[J].情报学报,2020,39(10):1046-1059.(Luo Pengcheng，Wang Yibo,Wang Jimin.Automatic Discipline Classification for Scientific Papers Based on a Deep Pre-Training Language Model[J]. Journal of the China Society for Scientific and Technical Information,2020,39(10):1046-1059.)   \n[7] 刘江峰,林立涛,刘畅,等.深度学习驱动的海量人文社会科学 学术文献学科分类研究[J].情报理论与实践,2023,46(2):71- 81.(Liu Jiangfeng,Lin Litao,Liu Chang,et al. Study on the Discipline Classification of Massive Humanities and Social Science Academic Literature Driven by Deep Learning[J]. Information Studies: Theory & Application,2023,46(2): 71-81.)   \n[8] 俞琰,朱晟忱.融入限定关系的专利关键词抽取方法[J].数据 分析与知识发现,2022,6(10):57-67.(Yu Yan,Zhu Shengchen. ExtractingPatentKeywordsbyIntegratingRestriction Relationship[J].Data Analysis and Knowledge Discovery,2022, 6 (10): 57-67.)   \n[9] 董克,吴佳纯.引文情感分析方法研究综述[J].图书情报知识, 2021,38(6): 60-72.(Dong Ke, Wu Jiachun. Literature Review on CitationSentimentAnalysisMethods[J].Documentation, Information & Knowledge,2021,38(6): 60-72.)   \n[10] 靳庆文,朝乐门,孟刚.AI治理中的算法解释及其实现方法研 究[J].情报资料工作,2022,43(5):16-23.(Jin Qingwen,Chao Lemen,Meng Gang.Research on Algorithm Interpretation and Implementation Method in AI Governance[J]. Information and Documentation Services,2022,43(5):16-23.)   \n[11] 张成洪,陈刚,陆天,等.可解释人工智能及其对管理的影响： 研究现状和展望[J].管理科学,2021,34(3):63-79.(Zhang Chenghong,Chen Gang,Lu Tian,et al. Explainable Artificial Intellgence and Its Impact on Management: Research Status and Prospects[J]. Journal of Management Science,2021，34(3): 63-79.)   \n[12] 李瑶,左兴权,王春露,等.人工智能可解释性评估研究综述 [J].导航定位与授时,2022,9(6):13-24.(LiYao,Zuo Xingquan, Wang Chunlu, et al. Research Progress of Artificial Inteligence InterpretabilityEvaluation[J].NavigationPositioningand Timing,2022,9(6): 13-24.)   \n[13] 刘桐,顾小清.走向可解释性;打开教育中人工智能的\"黑盒” [J].中国电化教育,2022(5):82-90.(Liu Tong,Gu Xiaoqing. Opening the“Black Box\":Exploring the Interpretability of Artificial Intelligencein Education[J].China Educational Technology,2022(5): 82-90.)   \n[14] van Lent M,Fisher W,Mancuso M.An Explainable Artificial IntelligenceSystem forSmall-Unit Tactical Behavior[C]/ Proceedings of the l6th Conference on Innovative Applications of Artifical Intelligence. 2004: 900-907.   \n[15]Adadi A, Berrada M. Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)[J]. IEEE Access,2018, 6: 52138-52160.   \n[16] 范昊,李珊珊,热孜亚·艾海提.机器学习算法在我国情报学研 究中的应用与影响——基于CSSCI期刊论文的视角[J].图书情 报知识,2022,39(5):96-108.(Fan Hao,Li Shanshan,Reziya Aihaiti.Adoption and Influence of Machine Learning Algorithms in Information Science Research in China: From the Perspectiveof CSSCI Journal Papers[J]. Documentation, If.ti 0. V.1d\\~0 20/5). n6 100)   \n[17] 纪守领,李进锋,杜天宇,等.机器学习模型可解释性方法、应 用与安全研究综述[J].计算机研究与发展,2019,56(10):2071- 2096.(Ji Shouling,Li Jinfeng,Du Tianyu,et al. Survey on Techniques,Applications and Security of Machine Learning Interpretability[J].JournalofComputerResearchand Development,2019,56(10): 2071-2096.)   \n[18]Loh H W,Ooi C P, Seoni S,et al.Application of Explainable Artificial Intelligence for Healthcare: A Systematic Review of the Last Decade (2011-2022)[J]. Computer Methods and Programs in Biomedicine,2022,226:Article No.107161.   \n[19]林志萍.可解释的机器学习及应用研究[D].广州:华南理工大 学，2021.(Lin Zhiping.Research on Interpretable Machine Learning and Its Application[D].Guangzhou: South China University of Technology,2021.)   \n[20]Goldstein A,Kapelner A, Bleich J,et al. Peeking Inside the Black Box:Visualizing Statistical Learning with Plots of Individual Conditional Expectation[J].Journal of Computationaland Graphical Statistics,2015,24(1): 44-65.   \n[21]Kim J,Lee HJ,Lee H.Mining the Determinants of Review Helpfulness:A Novel Approach Using Intelligent Feature Engineering and Explainable AI[J].Data Technologiesand Applications,2022,57(1): 108-130.   \n[22]FisherA,Rudin C,Dominici F.Model Class Reliance:Variable Importance Measures for Any Machine Learning Model Class, from the“Rashomon”Perspective[OL].arXiv Preprint,arXiv: 1801.01489.   \n[23] 于颖.机器学习方法及其可解释性技术在车险定价中的应用 基于XGBoost模型[D].大连：东北财经大学,2021.(Yu Ying.Application of Machine Learning Method and Its Interpretable Technology in Automobile Insurance Pricing— Based on XGBoost Model[D]. Dalian: Dongbei University of Finance and Economics,2021.)   \n[24] Ribeiro M T, Singh S, Guestrin C.“Why Should I Trust You?\": Explaining the Predictions of Any Classifier[C]/Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.ACM,2016: 1135-1144.   \n[25]ShapleyL S,Shubik M.A Method for Evaluating the Distribution of Power in a Commitee System[J].American Political Science Review,1954,48(3): 787-792.   \n[26] Lundberg S M, Lee S I. A Unified Approach to Interpreting Model Predictions[C]/Proceedings of the 31st International Conference on Neural Information Processing Systems.ACM, 2017: 4768-4777.   \n[27]杨朋波,桑基韬,张彪,等.面向图像分类的深度模型可解释性 研究综述[J].软件学报,2023,34(1):230-254.(Yang Pengbo, Sang Jitao, Zhang Biao,et al. Survey on Interpretability of Deep Models for Image Classification[J].Journal of Software,2023,34 (1): 230-254.)"
  }
}