{
  "original_filename": "full_1351.md",
  "综述评论": "",
  "书面汉语自动分词的现状和问题": {
    "context1": "黄祥喜",
    "context2": "(吉林大学计算机科学系）"
  },
  "摘要": {
    "context1": "书面汉语自动分词方法可以分为两大类，一类是基于算法的方法，它强调形式匹配；一类是基于知识的方法，它强调知识对分词过程的制导。现有的自动分词研究成果主要在基于算法的分词方面。本文分析了种种基于算法的分词方法的特点、不足及改进途径，指出了基于知识的分词是自动分词的发展方向，进而讨论了在实现基于知识的分词方法时所要考虑的问题。文中还介绍了我们在基于知识的分词方面所做的一些工作。"
  },
  "一、引 言": {
    "context1": "书面汉语自动分词是指计算机从输入文本中切分出反映该文本的语义的词。书面汉语自动分词的研究具有重要的理论价值和现实意义，它的最终解决对以下几个领域将产生实质性影响：",
    "context2": "·汉语语言理解  \n·计算机系统的汉语人机接口  \n·机器翻译  \n·情报检索  \n·语言文字自动处理  \n·人工智能和知识工程  \n·智能计算机  \n·汉语语言学  \n·认知心理学",
    "context3": "正是由于自动分词问题在上述研究领域的重要性，它受到人工智能界、汉语语言学界、计算机应用界及其他各界人士的广泛关注。本文拟对近几年关于这个问题的研究成果作一比较全面的整理和分析，使读者从中看到自动分词问题到底解决到了什么程度，还存在哪些需要解决的问题，其解决途径如何。"
  },
  "二、基于算法的分词": "",
  "1。最大匹配法": {
    "context1": "最大匹配法最早由苏联学者提出(1，其理论依据是：汉语词通常可以作为构词原料构成新的词。比如“火”是一个词，由它可以构成“火柴”、“火车”等新词；“火车”还可以进一步构成“火车头”、“火车站”等词。这样，当一个输入文本中出现“火车站”这样的字串时，正确的切分当然是“火车站”，而不是“火车 $^ n + \\cdots$ 站”，也不是“火” $+ \\cdot$ “车 $\" +$ “站”或“火” $\\pm \\textit { \\textbf { \\em { \\alpha } } }$ 车站”。最大匹配法反映的就是这一思想。它每次选出与输入文本匹配的最长的词。文章〔2,3)讨论了这种方法的算法实现。",
    "context2": "我们认为，最大匹配分词法的运行效率是可以通过设计精巧的词典结构和匹配算法加以改善的。现在的问题是：最大匹配分词法的分词能力如何？它对哪些文本可以作出准确的切分？对哪些文本无能为力？显然，这些问题的探讨对最大匹配分词法的实际应用是很有意义的。目前，自动分词方面的文献对这些问题几乎没有作过讨论。论文（4}通过引入词链的概念使上述问题的回答变得容易。词链的例子可以从下面两组句子中看到：",
    "context3": "（1）他的确切意图是什么？（正确切分：他的确切意图是什么？）他的确切了菜。",
    "context4": "(正确切分：他的确切了菜。）",
    "context5": "（2）他吃烤白薯。",
    "context6": "（正确切分：他吃烤白薯。）",
    "context7": "他在烤白薯。",
    "context8": "（正确切分：他在烤白薯。）",
    "context9": "第1组句子中的汉字串“的确切”属于I类词链，第2组句子中的汉字串“烤白薯”属于Ⅱ类词链。I、Ⅱ类词链具有不同的特点，其切分方法也有所不同。关于最大匹配方法的分词能力，文章（4）认为：如果一文本不含有词链，最大匹配分词算法（正向的或反向的）可以对其作出唯一准确的切分；如果一文本含有I类词链，且此词链满足一定的条件，则最大匹配分词法可以对其作出唯一准确的切分，但是对Ⅱ类词链和一般的I类词链，最大匹配分词法（不论是正向的、反向的还是双向的）无能为力。",
    "context10": "文章（5〕介绍的自动分词系统CDWS扩充了最大匹配分词法的分词能力。CDWS以最大匹配分词法为基础，同时使用了词尾字构词检错技术和一些有效的纠错知识处理词链（I类词链）。",
    "context11": "文章（6}提出的两种基于知识的分词方法—--“语境相关法”和“生成一测试法”，也是以最大匹配分词法为基础的。值得指出的是，这两种方法所使用的机器词典是一种动态结构词典一“词法ATN”,因此最大匹配法只是隐含在词法ATN的结构及运行过程中。文章（6）的方法对两类词链都能进行有效的处理，主要因为它是建立在理解基础上的。文章[6）提出了一种将分词处理和句法语义分析并行考虑的汉语理解系统结构一多ATN结构（详见第5节），这种结构保证了分词处理可以方便地使用句法语义知识。这一特点是目前的分词系统和汉语理解系统所不具备的。"
  },
  "2．基于统计信息的自动分词法": {
    "context1": "最大匹配分词法的词典组织原则之一是，当一个词是另一个词的一部分时，则把较长的词排在较短的词的前面。但还有一个实际问题未予考虑，即对同等长度的词如何排列。显然，词典中词的排列原则对词典的检索效率是有影响的。文章（7）讨论了词的频度信息与词典检索效率的关系，结论是：如果词典中的词按其频度大小排列，将能大大提高词典的检索效率，降低切分一个词的平均比较次数。但是，要把上述结论运用到词典编制中去是有困难的，主要的难点在于词的频度信息如何获得。如果这个信息得不到或不准确，基于词频的分词法就毫无意义。为此，我们需要研究词频统计方法。文章（2，8，9，10）对汉语词频统计问题作了系统探讨。文章（11）报道了汉语词频统计结果。",
    "context2": "基于词频的分词法是以词频词典（词按频率大小排列）为基础的。分词过程就是--个将输入文本与词频词典匹配的过程。这种方法没有考虑汉语的构词特性，只利用统计信息进行分词，准确率通常比较低，一般不独立使用。比较合理的作法是，将这种方法与基于汉语的构词特性的分词法（如前面的最大匹配法）结合使用。这种混合算法的词典组织原则是：",
    "context3": "·对每个词条目，增设词长和词频项；·对字头不相同的词，按其频数大小排列；·对字头相同的词，若长度不相同，则不管其频数大小，一律将较长的词放在较短的词的前面，若长度相同，则按频数大小排列。",
    "context4": "若按上述方式组织词典，原来的最大匹配分词算法无需作实质性的改动，即能处理词频信息。",
    "context5": "文章〔7）在上述思想的基础上，提出了“最佳匹配法”和“逆向最化匹配法”。但该文没有给出与这些方法相适应的词典结构，对方法本身也未作过程性描述。实际上，这里的最佳匹配法就是在最大匹配法的基础上再增加词频处理能力而得到的一种方法。逆向最佳匹配法对应于反向最大匹配法。",
    "context6": "文章(12)提出了一种用于情报检索中汉语文献自动标引的基于统计信息的分词法。所谓自动标引是指由机器抽出反映文献主题内容的关键词。该文的方法是将最大匹配法、构词字的频度信息、位置加权及非关键词（功能词）词表四者结合使用，完成自动分词和标引任务的。实验结果表明，这种方法具有 $90 \\%$ 以上的准确率。",
    "context7": "基于词频的分词法（更准确地讲是包括词频信息的最大匹配法）在分词频率上超过单纯的最大匹配法，但没有增强分词能力。它也不能处理词链{4问题。",
    "context8": "在应用基于词频的分词方法时要解决词频信息的获得问题。但这个问题也不好解决，因为它是以分词为前提的。"
  },
  "3。基于特殊字词表的自动分词法": {
    "context1": "最大匹配法反映了汉语的构词特性，它可以切分一大类语言文本。如果词典设计合理，其运行效率也比较高。若在该方法中增加词频功能，它的运行效率将得到更进一步的改善，从而成为一种实用的分词方法。但是，这个方法有一个前提，即为了使输入文本中的每一个词都得到切分，必须预先编制一部完备的机器词典。这个前提在实际中很难满足，因为所谓完备，一定是“一词不漏”,只要漏掉一词,包含该词的文本将无法作出完全切分。但是，世界上每天都产生成千上万的新词，要使机器词典达到“一词不漏”，谈何容易。且不说新词，即使在一个狭小的专门领域，现有的词也无法收集齐全。另外，不论词典结 构如何精巧，查找算法如何有效，对大规模机器词典的查找与对小规模机器词典的查找，二者在速度上是不可比的。然而，即使在一个很小的专门领域，供最大匹配分词法用的完备机器词典都是大规模的。",
    "context2": "那么，现在要问：自动分词是否可以不用大规模的完备的机器词典而改用中、小规模机.器词典，但在分词效率和分词能力上又不致损失很大？这要从汉语语言文字本身去寻找 答案。汉语语言文字中有一些特殊的字和词，相对整个汉字集或汉语词集而言，它们的数量少，但对分词极为有用。这些特殊字、词有下面几类：",
    "context3": "（1）位置特殊。汉字集中有一类字称为“定位字”，它们在构成一个词时，要么作为词的首字，要么作为词的尾字，而不能同时作为词首字和词尾字。这类字对分词的用处表现在，如果一文本中包含这类字，则可据此将该文本切分为几个较小的文本，其中每个子文本的首字或尾字是定位字。对这些文本可用某一分词方法作细切分。定位字数量较少。据统计〔13），在3,785个常用构词字中，定位字只有516个，占 $1 3 . 6 \\%$ ；不定位字有3,269个，占$8 6 . 4 \\%$ 。但是，如果把分词问题限制在某个专门领域，定位字数量会有所增加。",
    "context4": "（2）功能特殊。包括全部虚词和部分实词。这些词总数少，但在语言文本中的出现频率高。如果在切分一文本时，能首先识别并切分出这些词,将大量减少整个文本的切分工作量。文章[14整理出643个这样的词。统计表明，它们占输入句子中词的总数的一半以上。上面提到的部分实词不包括一般名词，但包括某些专有名词（如人名、地名等）。",
    "context5": "（3）结构特殊。包含“定位字”的词，包含某些词缀（前缀、后缀等）的词以及重叠词均属于结构特殊的词。",
    "context6": "（4）非汉字字符。如标点符号、外文字母等。这自然是可用于切分文本的特殊字。",
    "context7": "既然上述特殊字、词对自动分词十分有用，我们便可以将它们收集起来编成一部中、小规模词典，然后据此进行分词。这就是基于特殊字、词表的自动分词思想。目前已提出了多种基于特殊字、词表的自动分词方法 $( 1 4 \\sim 1 8 )$ ，下面分析其中的两种。",
    "context8": "（1）基于切分标志的自动分词法切分标志包括自然切分标志和非自然切分标志。非自然切分标志是指上面的定位字，自然切分标志则是指非汉字字符等。文章[17)首次提出了利用切分标志辅助分词的思想。文中收集了只充当词首字和不组词的单音词157个，只能充当词尾的字61个，拟声字145个，译音专用字180个。文章(18)在这一工作的基础上，在3,775个国际一级汉字中收集了只充当词首字的字327个，只充当词尾字的字247个，其中某些字允许出现例外（即并不只是充当词首字或词尾字）。",
    "context9": "文章[18)的自动分词方法将一个文本的切分分为文本切割、词段分词、知识辅助测试三个阶段。这种处理方法和前面两类分词方法比较起来，能够在一定程度上解决词链问题，但对词典的要求没有降低，而且还多设置了一个切分标志表。切分标志表的维护是一个大问题。第二个问题是形式分解的生成采用穷举法，而穷举生成效率低。.第三个问题是分词知识按模式存储和使用，对每一个词链都需要设置一个知识模式。在需要处理的词链较多时，知识库必须十分庞大，从而其查找效率大大降低。用这种方法处理词链的方式不具一般性。",
    "context10": "文章〔6〕研究了一种基于知识的“生成一测试”分词法。这种方法使用启发式知识限制生成，边生成边测试。分词知识按过程方式组织，分词和句子的句法、语义分析并行。它显然可以克服上述处理方式的缺陷。",
    "context11": "（2）“有穷多层列举”自动分词法〔14） 前面的最大匹配法是以人事先编制的一部完整机器词典为基础的。如果输入文本中的某个词没有收入此词典，这种方法将对此词拒识。这种方法可称为主观分词法。“有穷多层列举”法所追求的目标是，依靠一个不大的机器词典取得客观分词的效果,即文本中出现多少词就分出多少词，而不管这些词是否列人了词典。有穷多层列举自动分词法将整个分词过程分为以下五步：",
    "context12": "$\\textcircled{1}$ 利用“有穷类单词词表”切分出文本中的全部虚词和部分实词；",
    "context13": "$\\textcircled{2}$ 切分重叠词；",
    "context14": "$\\textcircled{3}$ 切分具有形态特征的词；",
    "context15": "$\\textcircled{4}$ 切分经过上述三步的处理所剩下的某些特殊的单字词；",
    "context16": "$\\textcircled{5}$ 利用“多音词词表”和“单音词词表”切分一般的多音词和单音词。",
    "context17": "“有穷多层列举法”的主要特色，正如方法提出者所指出的，是分词所用的词典的规模很小， $\\textcircled{1} \\sim \\textcircled { 5 }$ 步建立的各类表的单词总数为6,424个（其中有些单词还重复计数)，只占《现代汉语词典》（全部词条56,000个）的 $10 \\%$ 左右。这样一个小规模词典是否可以达到“有穷多层列举法”的目标，即取得客观分词的效果呢？方法提出者认为，这是可以达到的。但我们认为存在如下问题：",
    "context18": "首先，汉语中双音词占绝对优势，这一点已由方法提出者指出。还应该指出的一点是，双音词中，许多词都是由单音词按类似句法的方式构成的，这种构词方式通常称为“句法学构词法”。然而，“有穷多层列举法”切分单音词和双音词的顺序是：先利用“单音词词表”和“有穷类单词词表”（其中有些词是单音词）切分出文本中的单音词（ $( \\textcircled{1} \\sim \\textcircled{5} )$ 步），然后在第 $\\textcircled{5}$ 步分出双音词。这种处理方式将把含有上述两个表中的单音词的大部分双音词切分错。为避免此类错误，文章〔14)提出了建立组词词表的设想。如果建立组词词表，处理顺序将是，先测定组词词表中的词，然后测定“单音词词表”中的词。但“有穷类单词词表”中的单音词仍没有处理。如果第 $\\textcircled{1}$ 步就测定组词词表中的词，由于组词词表十分庞大（该表可依照《常用构词字典》建立，此字典收词的数量已超过《现代汉语词典》的总条目数），将违反“有穷多层列举”方法的基本点。",
    "context19": "其次，即使组词词组表建立好了，“有穷多层列法”仍有问题没有考虑到。它不能处理词链问题。（注意，该方法中第 $\\textcircled{1} \\sim \\textlangle \\textbar { 5 } \\textrangle$ 步设立的各类表中的词可以生成词链！）",
    "context20": "综上所述，现有种种基于算法的分词方法具有下述缺陷：",
    "context21": "（1）现有的种种分词方法的核心是形式匹配。形式匹配法效率低且分词能力不强，只能切分不含词链的文本。",
    "context22": "（2）现有分词方法所用的词典是一种表式词典。这种词典对英语语言处理是合适的，但不一定适于汉语语言处理。它不能反映汉语语言文本中存在的字与字之间的动态结合关系。",
    "context23": "（3）现有的分词方法都没有考虑理解问题。实际上，人在阅读一个句子时，分词和理解过程是并行的。",
    "context24": "为了解决上述问题，我们需要探讨新的词典结构、分词方式及系统结构。"
  },
  "三、机器词典的结构": {
    "context1": "我们说汉语语言文本中字与字之间存在一种动态结合关系，指的是文本中几个字之间是否或怎样构成一个词并不全是可以预先规定的。如果可以预先规定，那么，我们用一个表式词典就够了。有些字的组词方式取决于一定的上下文。在不同的上下文语境中，它们所构成的词是不一样的。词链中的字就是具有这一性质的字。现有的词典没有考虑上下文语境，各个词是孤立存在的，显然不能反映上述特征。为了反映上述特征，我们必须对现有的表式词典结构进行根本性的改造。文章〔6)对此作了系统研究，提出了一种动态词典结构。这种结构是以W。A．Woods1970年提出的“扩充转移网络（ATN）”理论为基础建立起来的。因此，我们把这种动态词典也称为“词法ATN”。",
    "context2": "词法ATN的主要优点是：",
    "context3": "（1）可以描述汉语中具有不同性质和不同结构的词；（2）可以测定语言文本中字与字之间的动态结合关系具体是哪一种，有助于识别词链；（3）使汉语的词法分析（自动分词）与语言理解系统的其他部分如句法分析、语义处理、言谈（语用）分析等的双向交互成为可能为了使词法ATN能高效率地运行，需要研究合理的ATN分析算法。这方面已有许多成果。"
  },
  "四、基于知识的分词": {
    "context1": "人在切分一个句子时，并不只是利用了他所具有的词典知识（即词汇量），而是综合利用了他所具有的各种知识，包括除词典之外的其他语言知识及非语言知识。形式匹配分词法只是考虑了使用词典知识，所以不能全面解决自动分词问题。我们需要开发综合利用各种知识进行分词的方法和技术。，",
    "context2": "对分词有用的知识一般可分为以下几类：",
    "context3": "（1）字典知识。包括字形、字的读音、字的功能(取决于一定的应用目的)、字的构词能力的说明（该字可以构成哪些词，它能否充当词首字、词尾字、词中字以及这几种可能的组合），等等。",
    "context4": "（2）词典知识。包括词形、词的句法范畴、语义特征、结构描述、语音信息、词间关系说明，等等。",
    "context5": "（3）上下文知识。包括字串关系知识（对一个字串成为一个词，上下文中应该出现什么样的字串的说明）、词间关系知识（对一个字串成为一个词，上下文中应出现具有什么性质的词的说明）、句法结构知识（对一个字串成为一个词，上下文中应具有什么句法结构的说明）、语义结构知识（对一个字串成为一个词，上下文应具有什么样的语义结构的说明）、言谈知识（或称语用学知识），等等。",
    "context6": "（4）专门领域知识。这种知识可以限制词典的收词范围和收词量，限制和确定词的意义，限制句型，减少歧义，等等。",
    "context7": "（5）常识知识。人在分词过程中经常利用的常识。",
    "context8": "上述种种知识的获取、表示和利用是基于知识的分词法中的关键问题。共中最关键的是分词知识的获取。知识工程中的许多研究成果(19对研究这些问题是有借鉴意义的。"
  },
  "五、自动分词与理解": {
    "context1": "我们说人的分词与理解过程是并行的，是指人在切分句子中的某个片段时，并不只是孤立地看这个片段，而是将该片段的切分与其上下文(句内上下文或句外上下文)的分析结合起来进行的。更确切地说，通过对片段的上下文的理解来确定片段的切分。另一方面，对上下文的理解又是以对上下文的分词为基础的，而这一上下文的分词又涉及对另一的上下文的理解。这个过程是循环往复的。",
    "context2": "然而，目前不论是自动分词的研究还是语言理解的研究，均未反映上述基本观点。现在研究的种种汉语语言理解系统结构基本上是一种串行结构（见图1），这是英语语言理解系统的一种典型结构。在这些系统中，要么不考虑分词问题，假设词已分好（20\\~3）；要么将分词仅作为理解的第一步包含在词法分析模块中 $\\left\\{ 2 4 { \\sim } 2 7 \\right\\}$ 。",
    "context3": "![](images/fefa27f33db84600792ff19a4ededae2c8a48ff762e76bd3182d0df4f2c52701.jpg)  \n图1 汉语语言理解系统的串行结构",
    "context4": "![](images/b498ae19255c2312222473f404adc795d8101d8b3c919fdae5aeaa0b0140897d.jpg)  \n图2 汉语语言理解系统的理想结构",
    "context5": "我们认为，图2表示的是“人的分词和理解过程是并行的”这一基本观点的一种比较理想的汉语语言理解系统结构。图中，实线表示控制流，虚线表示数据流。这种结构是黑板结构(28和图1的串行结构的综合。它的主要优点是保证了汉语语言理解系统的各个部分之间可以进行“双向信息传递”。这个特点对分词是十分重要的。",
    "context6": "文章（6给出了图2结构的简化实现。它没有考虑言谈（语用)知识问题；将句法知识和语义知识综合表示（以“语义文法”（29>方式）；暂设有两个知识源，即词法知识源和句法语义综合知识源，分词处理模块包含在词法知识源中。在这种简化结构下，研究了两仲基于知识和理解的自动分词方法，即“语境相关法”和“生成一测试法”。这两种方法在词链的处理方式上各有特色，前者强调在分词过程中进行部分句法语义处理，后者强调上下文信息对分词过程的控制（即限制词链中词的选择）。目前，这种简化的汉语语言理解系统结构正在用来实现一个石油专家系统的汉语人机接口。"
  },
  "参考文献": {
    "context1": "［1］梁南元，刘源：书面汉语计算机自动分词，《中文信息》，1986，（1），5—10［2］王锡龙，王树义，谷新英：用于汉语分词的计算机算法，《语文现代化》，1985，（8），49-58",
    "context2": "［3］黄祥喜：基于算法的分词方法研究，吉林大学计算机科学系，1988［5］梁南元：书面汉语的自动分词与一个自动分词系统一CDWS，《北京航空学院学报》，1984，（4），97-104",
    "context3": "〔6］黄祥喜：书面汉语的计算机分词和理解：基于知识的方法，吉林大学博士论文，1988·［7］梁南元，刘源：0M（最佳匹配）自动分词方法，《中文信息》，1985，（2），18-19[8］刘源：电子计算机进行汉语字词频度统计的问题和方法,中文信息处理国际研讨会文集，1983，（3）",
    "context4": "[9］管纪文，王锡龙，谷新英等：词频统计软件方案，现代汉语词频统计第二次软件会议论文，1982",
    "context5": "[10］王锡龙，谷新英，王树义：用于词频统计的自校正双向扫描分词算法，吉林大学计算机科学系，1983",
    "context6": "[11]梁航：现代汉语字频统计通过国家鉴定，《中文信息》，1982，（2），35[12]龙泽云，邓钦和：一种汉语自动分词标引方法—统计分析法，《情报科学技术》，1988，（1），46-47",
    "context7": "[13]陶沙：现代汉语的构词字和常用字，《语言和计算机，1986，（3），125一131[14]张普，张光汉：现代汉语“有穷多层列举”自动分词方法的讨论，《语言和计算机》,1986，（8），112-124",
    "context8": "[15]王永成，肖玮瑛：自动编制中文标题的主题词轮排索引及自动抽词，《南京大学学报》,1984，（1），39--44",
    "context9": "16]吴蔚天：在微机上实现汉字抽词，第五次全国机器检索研讨会论文，西安，1986",
    "context10": "17]王以德；程序辅助分词初探，中国中文信息开究会基础理论学术会议论文，1.982[18]管纪文，谷新英：结合上下文辅助分词的学习系统，中国中文信息研究会中文信息处理国际研讨会，北京，1983年10月12日",
    "context11": "[19]管纪文，刘大有，黄祥喜等：知识工程原理，吉林大学出版社，1988[20] Yang Yiming: Combining Prediction, Syntactic Analysis and Semantic Analysisin Chinese Sentence Analysis, Proceedings of the Tenth International Joint Con-ference on Artificial Intelligence (IJCAI),Netherlands,1987, 679-681,IJCAI-10,Italy，1987年8月",
    "context12": "[21]范继淹，徐志敏：RJD-80型汉语人机对话系统的语法分析，《中国语文》,1982，（3)，223一232",
    "context13": "[22]陈群秀，黄昌宁，曾宪奕：计算机的通用汉语接口，《中文信息》，1988，（1），2一6  \n[23]吴智君：自然语言的计算机分析和理解，《语言研究》，1985，（1），1一14",
    "context14": "[24]王锡龙：汉语语言的理解系统，中文信息处理国际研讨会文集，1983，（2），1—10[25]欧阳文道：中文信息五维模型和分词、析句、辩义的算法研究，中文信息处理国际研讨会文集，1983，（3），153-158"
  },
  "THE STATUS QUO AND PROBLEM OF AUTOMATICDIVIDING OF CHINESE PHRASES IN WRITTEN FORM": {
    "context1": "Huang Xiang-xi",
    "context2": "(Department of Gomputer Science, Jilin University)"
  },
  "Abstract": {
    "context1": "There are two kind of methods for phrase dividing in written Chinese.One is based on various algorithms, it puts stress on form match. The other is based on knowledge,it emphasizes the function of knowledge in the process of phrase dividing. The existing research achievements of automatic phrase dividing are mainly based on algorithms. This paper analyses the characteristics and pitfalls of various phrase dividing methods bascd on algorithms， and proposes ways of imprcvement. It points out that the phrase dividing based on knowledge is the direction of development，and discusses thc prohlcms which should be taken into consideration when implementing the meihod of plrase dividing based on knowledge. Some of the author's work in phrase dividing based on knowledge is introduced in this paper."
  }
}