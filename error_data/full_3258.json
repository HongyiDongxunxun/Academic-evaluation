{
  "original_filename": "full_3258.md",
  "国内网页去重技术研究：现状与总结": {
    "context1": "李志义梁士金华南师范大学经济与管理学院广州 510006",
    "context2": "[摘要]针对国内 2000一2010年之间有关网页去重技术的研究成果进行计量分析，重点从网页结构、网页特征、网页内容、同源网页、元搜索等方面总结和分析去重技术的基本研究现状，并兼论基于布尔逻辑模型与傅立叶系数的网页去重以及网页去重技术在一些特殊领域的应用研究。",
    "context3": "[关键词]重复网页同源网页网页去重 [分类号] $\\mathrm { 7 P 3 9 1 }$",
    "context4": "NationalResearch on Deleting Dup licated W eb Pages: Sta tus and Summ ary",
    "context5": "Li Zhiyi Liang Shijin",
    "context6": "Econam ic & Management Colege of South China NomalUniversity， Guangzhou 510006",
    "context7": "[Abstract]This paperusesthebblimetricmethodtanalyzethenationalresearch fndings onthetechnobgyofdeletingduplicated web pages in he yearof2Ooo—20l0，summaries and analyzesitsbasic statusfrm structurecharacteristicscontentshmobgyweb pagesmeta searchetcItalsodiscusesthetechnolgyofdeletingduplicated web pagesbased onBolean bgic modelandFourier coeffc ient and its applied researches in sme special fields   \n[K eywords] duplicated web pageshmobgous pagesdeletion of dup licated web pages",
    "context8": "互联网发展至今，就其开放性、共享性等特点和对人类的影响而言，无疑成为 20世纪最伟大的创造。同时，网络技术和信息技术发展的直接结果就是网站与网页数量的直线上升。据中国互联网络信息中心于2010年1月发布的第 25次《中国互联网发展状况统计报告》显示，目前我国网站数量已达323万个，年增长率达到 $12 \\%$ ，且继续保持持续平稳增长的势头。网站的快速增长意味着网络信息的膨胀式增加，面对如此巨大的信息量，人们发明了搜索引擎以解决因网络的快速发展而带来的信息爆炸、信息孤岛、信息搜集困难等现象。但是，搜索引擎的采集器在自动抓取到大量网页信息的同时，也收集到很多重复网页。因此，如何去除重复网页就成为搜索引擎领域研究的热点和重点。",
    "context9": "![](images/bb1d5a71de872be3af12cb56046082659f1141f49a86a171f90caf5ebc07adb6.jpg)  \n图1发文时间及数量",
    "context10": "从图1中发现，相关论文的数量在 2000一2004年仅有两篇，而在 $2 0 0 5 - 2 0 0 8$ 年总体上表征为逐年递增，并于2008年达到顶峰，随后又有逐步下降的趋势;但整体而言，2008年和 2009年是该研究的高峰期。分析发表的载体性质,看出论文主要集中在期刊和学位论文，而核心期刊论文数量不足全部文献的2/5,可见该研究的质量有待提高。如表1所示："
  },
  "1文献计量分析": {
    "context1": "在CNK数据库中，以“网页去重、查重、消重、排重\"以及“重复、近似、相似、镜像网页\"等为题名关键词进行检索,得到 56篇左右的相关文献。分析其发文年份及数量，如图1所示：",
    "context2": "表1发文载体及数量",
    "context3": "<table><tr><td rowspan=1 colspan=1>载体</td><td rowspan=1 colspan=1>期刊 (核心)论文</td><td rowspan=1 colspan=1>学位论文</td><td rowspan=1 colspan=1>会议论文</td></tr><tr><td rowspan=1 colspan=1>数量(篇）</td><td rowspan=1 colspan=1>36(21)</td><td rowspan=1 colspan=1>15</td><td rowspan=1 colspan=1>5</td></tr></table>"
  },
  "2基本研究现状": {
    "context1": "文献[1]指出网页去重的方法有三类：基于抽取指纹信息的方法、基于聚类的方法、基于URL的方法。其中前一种方法主要应用网页结构和特征，第二种方法面向网页内容，而后一种方法则针对同源网页。本文不但兼顾此三者，还将探析元搜索的去重策略。以上方法在实施过程中，一般采用向量空间模型和网页特征相结合的方式表示网页;而近年来，出现了利用布尔模型和傅立叶系数表示网页的方式，本文也会涉及此方面的去重技术。"
  },
  "2.1基于网页自身的网页去重": {
    "context1": "从构造成分上看，网页为了高效、华丽地展示有用信息(网页标题、正文、导航与超链接信息等)融合了相当部分的无用信息（ $\\mathrm { h m }$ 1标记）。因此，网页被认为是一种结构不够严谨的信息组织形态。从网页特征提取的角度讲，抽取无用信息可提炼出网页结构;抽取有用信息可分析出网页特征码、特征串、特征句、网页内容等,这些正是网页去重的着眼点。",
    "context2": "2.11网页结构去重通常,网页结构被定位在物理层面 (网页的物理结构)和逻辑层面 (网页内容的组织或描述形式)。本文查找到的相关论文将网页结构限制在前一层面，这主要是考虑到后一层面的复杂性和保证去重效果。",
    "context3": "文献[2]构建了一种基于网页文本结构的网页去重系统,其基本思想是：用extrcatlg算法生成目录结构树以表征网页正文;以层次指纹为视角的网页相似性比较。其去重过程分三步： $\\textcircled{1}$ 计算根节点(即网页标题)的相似度，即先将两网页标题转化成相关关键词的集合，再看两集合的交集情况，若为非空，则转入 $\\textcircled{2}$ 。$\\textcircled{2}$ 从首层节点开始，将节点代表的网页内容段进行MD5编码(即得到层次指纹），同时分析重叠指纹的数目，若数目为0则两网页不重复;若该数目与两网页的首层节点数相等，则两网页完全重复；否则部分重复，并转入 $\\textcircled{3}$ 。 $\\textcircled{3}$ 选取具有重叠指纹的节点为根节点，并对此节点以下的子树进行与 $\\textcircled{2}$ 类似的判断过程。又如，黄仁等人[3]也实现了网页文本结构去重的方式，但引入了特征码长句提取思想。",
    "context4": "2.12网页特征去重网页特征去重的基础是对网页实施预处理操作后，运用相关技术提取网页的特征，而目前对网页特征的描述主要运用三种形式一一特征码、特征串、特征句。",
    "context5": "·基于特征码的网页去重。网页的特征码分为主码和辅助码，前者一般是网页正文各自然段前几个字符的组合;后者则由各自然段中标点符号前后几个字符按次序链接而成。文献[4]通过识别网页之间主码和辅助码的相似状况来判断网页是否重复，并用二叉树实现去重，具体情况为：当主码完全相同时，可知两网页重复，此时的二叉树保持不变。当主码部分相同时，需比较辅助码，如果两网页不重复则将被比较网页的特征作为新节点插入到二叉树中;否则替换二叉树中的原有节点。另外,由于XML是一种较好的描述Web信息的方式，因此搜索引擎的爬虫程序也会抓取到许多重复XML信息。鉴于此,有学者将 XML信息的特征码用高效 $\\mathrm { B } ^ { + }$ 树建立索引，大大提高了去重效率[5]。",
    "context6": "·基于特征串的网页去重。特征串的析取流程为：将句子中能代表其大意的若干字符选为该句的特征码;以句子在网页正文中的出现顺序组织各句子的特征码,即网页的特征串。该方法既保证了网页内容的完整性，又兼顾了网页各部分的内在连续性。",
    "context7": "文献[1]以网页的文本内容和结构信息为切入点，围绕模糊匹配思想论证面向特征串的中文网页去重技术。而文献[6]在讨论特征串提取方法之后，构造特征串索引散列表 (是以网页文本的段落数、字数、句子数为维数的三维散列空间）。文献[7]则提出一种以字频为基础的特征串查重算法。",
    "context8": "2.13网页内容去重网页内容去重主要应用聚类分析的方法，即认为网页相似度达到一定的阀值，则将此类网页归为同一类别。在国内的相关研究中，根据聚类方式的不同，可分为如下两类：",
    "context9": "·基于主题的网页去重。网页的主体内容可采用主题概念表示[8],因此只需知道主题间的相似情形就可了解网页是否重复。郭晨娟、李战怀从网页主题中抽象出系列概念对象集合，辅之于与概念对象对应的索引文件对象以找出两者间的映射关系，即形成网页相似度处理模型[9]。杜海刚、李先国则以网页主题为依据探寻网页特征关键词，并在此条件下使用倒排索引搭建用于识别网页重复程度的模型[10]。而文献[11]分析网页重复的特征,组建组块向量空间提取网页主题句向量，然后计算该向量的相似度实现网页去重,其公式[1]如下：",
    "context10": "$\\mathrm { D } _ { \\mathrm { T } _ { N } } \\mathrm { _ = } \\frac { \\displaystyle \\sum _ { i = 1 } ^ { \\mathrm { m } } \\sum _ { j = 1 } ^ { \\mathrm { n } } \\mathrm { D } _ { \\mathrm { A } , \\mathrm { ~ B } _ { j } } } { \\displaystyle \\mathrm { n } \\times \\mathrm { m } }$ 其中， $\\mathrm { D } _ { \\mathrm { T A } }$ TB为两网页主题句向量 $\\mathrm { T } _ { \\mathrm { A } } , \\mathrm { T } _ { \\mathrm { B } }$ 的相似度; $\\mathrm { T _ { \\mathrm { A } } = ( A _ { 1 } , \\ A _ { 2 } , \\cdots , A _ { i } , \\cdots , A _ { m } ) }$ ）， $\\mathrm { { T _ { B } } }$ $= ( \\mathrm { \\bf ~ B } _ { 1 } , \\mathrm { \\bf ~ B } _ { 2 } , \\cdots , \\mathrm { \\bf ~ B } _ { \\mathrm { j } } , \\cdots , \\mathrm { \\bf ~ B } _ { \\mathrm { n } } )$ ； $\\mathrm { A _ { i } } = \\mathrm { { \\{ \\Phi _ { a _ { i } } } } $ $_ { \\mathrm { a } _ { \\mathrm { n } } } \\nwarrow \\in \\mathrm { T } _ { \\mathrm { A } }$ ， $\\mathrm { B _ { j } }$ $= \\{ \\mathrm { b _ { l } , \\ b _ { z } , \\cdots , \\mathrm { b _ { n } } \\} } \\in \\mathrm { T _ { B } , \\ A _ { i } }$ 和 $\\mathrm { B _ { j } }$ 为主题句分词后的分词向量; $\\mathrm { D _ { A i B j } = _ { c o s } ( T _ { A i } , \\Delta T _ { B j } ) }$ ， $\\mathrm { { T _ { A } } }$ 和 $\\mathrm { { T _ { B j } } }$ 为权重向量。",
    "context11": "·基于最大正文块的网页去重。最大正文块去重的成功要件在于找到合适的网页最大块(常用做法是通过比较网页文本块的文本长度),同时比较其相似度以达到聚类的目的。在网页预处理方面，文献［12]根据网页布局结构构建DOM树并以树的节点描述网页的有用或无用信息。在去重操作方面，则运用 tidyGetehild tidyGetwext等工具遍历DOM树以生成最大正文块，当遍历结束后，利用tidyRelease工具删除DOM树;同时，分别以经典算法Bisecting Kmeans $^ +$ 十和MD5值指纹算法按查准率、查全率和运行效率等指标实施网页查重实验。"
  },
  "2.2同源网页去重": {
    "context1": "网络中存在很多内容相异或相同，但是URL相同的网页,即同源网页。此类网页的去重是搜索引擎中去重操作的重点。文献［13]在分析BloomFilter算法和两种改进算法 (计数型和拆分型Bloom Filter)的基础上，应用URL散列函数运算实现同源网页去重。但是，该去重策略的精准性令人堪忧，所以需要在URL散列的择取和相应参数的设定方面下足功夫，才能达到较好的可用性。在应对非同源网页去重的相关问题时，此策略显得无能为力，因此适合于元搜索引擎的网页去重。与此类似，高凯等人用如下公式[计算网页URL的哈希值得出对应的哈希文件（保存已下载的URL),并通过访问该文件实现同源网页的去重。",
    "context2": "$\\mathrm { H _ { \\colon } U {  } \\Big \\{ } A _ { i } \\mid A _ { i } = \\big [ \\sum _ { k = 1 } ^ { n _ { i } } 3 6 ^ { n _ { i } - k } C _ { \\downarrow } \\big ] ~ m o d \\big \\} $ 其中， $\\mathrm { U } ^ { = }$ $\\{ \\mathrm { U } _ { 1 }$ $, \\mathrm { { U } } _ { 2 } , \\cdots , \\mathrm { { U } } _ { \\mathrm { { n } } } \\ ]$ 表示 URL集合; $\\mathrm { A } _ { \\mathrm { i } }$ 为 $\\mathrm { { U } _ { i } }$ 的哈希地址;$\\mathbf { n } _ { \\mathrm { i } }$ 是 $\\mathrm { { U } _ { i } }$ 的字符串长度; $\\mathrm { C _ { k } }$ 是 $\\mathrm { U _ { i } }$ 中左起第 $\\mathbf { k }$ 个字符的码值；S是哈希表大小。"
  },
  "2.3 布尔逻辑模型和傅立叶系数网页去重": {
    "context1": "2.3.1 基于布尔逻辑模型的去重在经典的文本信息检索领域，文档的表示有三种理论模型：布尔逻辑模型、向量空间模型和概率模型。严格结构化和紧密性导致布尔逻辑模型具备天然的简单性和易实施性，但由于该模型无法体现文本的特征项对文本内容的贡献度，所以其文本表示能力又是脆弱的。即便如此，文献[14]还是从计算网页相异度的视角，将重复网页界定为相异度小于某个阈值的网页，并用布尔逻辑模型描述网页，从而使基于布尔模型的网页去重算法浮于水面。该算法用二进制数0表示某特征没有在文本中出现，用1则表示出现，且用汉明距离计算网页相异度以实现网页去重。",
    "context2": "以上算法的运算量非常突出，需得到较为理想的系统资源支持,否则算法效率会受到较大影响。连浩等人对其进行完善，认为特征个数是两网页比较与否的决定因素，如果个数小于某个特定值,则取其异或值,并用下面的公式[15]判断相异度;否则两网页不重复。",
    "context3": "$\\mathrm { F _ { i j } = \\frac { \\sum ( \\ l _ { \\ t f , \\Omega _ { i } } - \\Delta _ { t f , \\Omega _ { j } } ) } { \\sum \\ _ { t f , \\Omega _ { i } } + \\sum \\ _ { t f , \\Omega } \\ / 2 } }$ ∑+∑）（E分别为特征w在网页文档 $\\mathrm { i }$ j冲的出现频率）",
    "context4": "2.3.2基于傅立叶系数的去重本技术的核心是傅立叶系数的明确化，关键是网页的重复性检测，前提是排除网页无用信息的干扰。陈锦言等人设计如下流程完成傅立叶系数去重[16]：计算出网页文本中相异字符的数量以及字符间的相互关系，以这些关系的统计值构成字符语义关系矩阵 (即字符语义值的运算转化成矩阵运算);在矩阵中，采用相应的算式得出字符的相对语义距离 (即字符语义值),而各语义值的排列则形成网页初始表现形态一—语义值数列;对该数列施加傅立叶变换得到傅立叶系数，择取系数的前面几项组建网页的系数向量 (即实现网页的傅立叶系数表示);最后，用系数向量计算各网页间的相似性。",
    "context5": "上述过程从分词的角度将网页文本切分成若干字符，通过相邻关系简易地显示出语义特征，具有很大的局限性，必然制约去重结果的可信性。而分词技术在理论和实践上都臻于成熟，因此分词理论可以更广泛地适用于该去重技术。但令人惋惜的是，笔者并未查询到国内学者在这方面的研究成果。"
  },
  "2.4元搜索的网页去重": {
    "context1": "从工作机理上看，元搜索主要完成三项工作：用户请求的接收和规范;成员搜索引擎的调用及检索;搜索结果的处理(汇总、去重、排序和输出)。从使用体验上看，用户希望尽可能多地检索到相关网页，又不愿意看到高重复率现象的出现。但是，元搜索引擎不会直接通过网页内容、特征、结构等辨别重复网页，而是分析成员搜索引擎返回的结果信息，进而检查出重复网页并去重。因此，现行方式一般是利用网页摘要或主题实现重复网页的辨识。",
    "context2": "文献[17]中论述的网页去重方法，首先把收到的用户请求分割成必要的关键词;然后，用这些关键词对成员搜索引擎返回结果的摘要进行特征码提取;最后，以相似因子的概念计算摘要的重复度，其公式为",
    "context3": "FazzyFacto1 $\\cdot = _ { \\mathrm { p _ { n } } } \\ / _ { \\mathrm { W _ { n } } }$ （ $\\mathrm { p } _ { \\mathrm { n } }$ 是所要比较的两个特征码中,相同字符的数量; ${ \\bf W } _ { \\mathrm { n } }$ 是特征码中字符的总量）。文献[18]的消重思路与文献［17]的大同小异，只是在提取摘要特征码后,进一步分析出特征串，并以此计算相似度。而文献[19]将动态生成的网页摘要以集合的形式表示，为每一摘要语句赋与一个由三个参数组成的权重以描述其特征，从而形成一系列的权重向量，那么重复度比较就由摘要的权重向量决定。",
    "context4": "周小平等人[20]则推出“二次去重\"的理念，即判断网页正文主题之间的相似性情形，如果符合一定的阀值，则认为是重复网页并退出算法程序;如果不符合，则对比文摘之间的相似性情形，进而知道网页的重复与否。"
  },
  "3特殊领域的应用研究": {
    "context1": "网页去重技术蕴含于信息搜索技术，是后者发展的有力支撑;而且随着研究的深入，被广泛地移植到某些特殊的领域并发扬光大，这些研究拓展了该技术的应用范围。"
  },
  "3.1企业竞争情报领域的网页去重": {
    "context1": "文献[21]将网页去重技术与企业数据仓库领域相结合，利用倒排索引实现了网页去重的自动化，有助于企业决策信息的智能化。文献[22]在Web环境下企业的竞争情报平台中整合网页去重技术，重点是通过基于特征码的散列算法降低网页重复性量化操作的复杂性。"
  },
  "3.2P2P网页去重": {
    "context1": "P2P网络的特征有： $\\textcircled{1}$ 网络中心被淡化,各节点的地位对等; $\\textcircled{2}$ 小世界性，即各节点的资源高度集中，主题性强，而且极易挖掘出节点间的最短访问路径。这导致 P2P网络中的资源具有高重复率，以及资源的搜索需从两个方向入手：节点内搜索和节点间搜索。本文将与之相对应的去重技术命名为“二维去重”,包括内去 (消)重和外去 (消)重[23]两个维度,这正是鄢靖丰等人推导出的“基于小世界现象的网页消重\"的逻辑基础。"
  },
  "3.3分布式系统中的网页去重": {
    "context1": "以分布式的原理设计搜索引擎系统已成为满足大型搜索引擎检索需求 (响应时间短、准确率高、覆盖面广)的必然之举。张元丰等人[24]借鉴Google的经验，用Map/Reduce构筑分布式并行去重策略，并控制分布式系统的计算节点数目以验证去重的效率、稳定性以",
    "context2": "及系统资源耗费情况。",
    "context3": "此外，去重技术还在新闻网页去重 [25-27]等具体领域得以应用。"
  },
  "4结语": {
    "context1": "总体而言，网页去重技术作为搜索引擎技术的子集，依托搜索引擎的发展而被国内学者普遍关注，研究兴趣点越来越多，研究成果越来越丰富。但是，也存在一些问题：从宏观角度看，发文总量偏少，核心期刊的集中度偏低导致高质量的成果不多;从微观角度看，研究内容涵盖了网页去重理论和应用的方方面面，但大部分研究仅是在经典去重技术上的改进，在一定程度上使后者的局限性降低、适用性增加;国内学者在实践层次上的欠缺使得其成果没有经典技术的可用性高，因此去重效果仍需通过多次周密的实践进行严格考证。"
  },
  "参考文献：": {
    "context1": "[1]孙秀丽·高校图书馆数字资源建设与利用的调查分析·大学图书馆学报，2008(6)：45-50.  \n［2]贾西兰，王琼，吴英梅·高校数字图书馆建设新思考·大学图书馆学报，2010(2):59-63.  \n［3]张世明·高校图书馆数字资源建设现状及发展策略·情报探索，2008(9):44-46.  \n［4]李文兵·试论大学排名的功能与作用·中国石油大学学报(社会科学版)，2010,26(1):99—103.  \n[5]武书连·挑大学选专业（2009高考志愿填报指南）．北京：中国统计出版社，2009.58-69.  \n［6]朱德全，宋乃庆·现代教育统计与测评技术·重庆：西南师范大学出版社，1998.  \n［7]叶中行，王蓉华,徐晓岭，等·概率论与数理统计·北京：北京大学出版社，2009.411—412.  \n［8]李卫东·我国财经类院校图书馆数字资源建设调查分析·图书馆建设，2008(5):37-39.",
    "context2": "[作者简介]张国臣，男，1963年生，副教授，参考咨询部副主任，发表论文12篇。"
  },
  "4.2优化图书馆数据资源构成": {
    "context1": "2008年，李卫东调查国内 28所财经类院校图书馆数字资源建设发现：财经类数据库在各馆数据库中占据主体,其中外文财经类数据库比例偏低[8]。在数据库总量够用的前提下，不同类型院校教学与科研的比重不同，在数据库引进和更新方面要有所侧重。研教型或教研型院校应该提高外文数据库和中文数据库占数据库总量的比例，适当调配外文数据库与中文数据库的比例;教学型院校可以在多媒体数据库建设方面具有倾向性，使有限的数字资源获得较高的保障率。"
  },
  "4.3扩大信息资源共建共享规模": {
    "context1": "信息资源共享建设是解决数字信息急剧增长及用户对信息资源的无限需求与图书馆对信息载体有限的收集和处理能力的矛盾的最佳途径。例如，由北京世纪超星公司提供技术平台，对外经济贸易大学牵头，中央财经大学、首都经济贸易大学、北京工商大学和北京物资学院加盟，将北京地区5所财经类高校的图书馆资源进行整合，建立了包括图书目录、电子图书、电子报纸、中文数据库和外文数据库在内的图书馆资源共享平台，于 2010年1月投入使用。资源共享平台的建立克服了各图书馆孤军奋战、势单力薄的弊端，解决了重复建设、发展缓慢的问题，发挥了数字资源的优势，加快了图书馆数字化发展进程。"
  },
  "5结语": {
    "context1": "图书馆作为教学科研的学术性服务机构，是学校基础建设的重要组成部分，也是学校培养高素质人才、增强学术竞争力的重要保障。高校图书馆在数据资源建设上，要具有前瞻性和创新性，跟踪信息环境的新变化，为构建满足用户需求的数字图书馆而努力。"
  },
  "（上接第121页）": {
    "context1": "[15]连浩，刘悦，许洪波，等·改进的基于布尔模型的网页查重算法·计算机应用研究，2007(2)：36－39.  \n[16]陈锦言，孙济洲,张亚平·基于傅立叶变换的网页去重算法·计算机应用，2008(4)：948-950.  \n[17]谢蕙，秦杰,胡双双·基于用户查询关键词的网页去重方法研究·现代图书情报技术，2008(7)：43一46.  \n[18]谢蕙，秦杰·基于元搜索的网页去重方法研究·计算机系统应用，2008(8).94-96.  \n[19]李宪雷·元搜索关键技术研究与实现·[学位论文]·北京：北京工业大学，2008(6):22-36.  \n[20]周小平，黄家裕，刘连芳，等·基于网页正文主题和摘要的网页去重算法·广西科学院学报，2009,25（4)：251－253.  \n[21]白广慧，连浩，刘悦，等.网页查重技术在企业数据仓库中的应用：计算机应用，2005(7)：1713—1715.  \n[22]杨申彦，黄青松·网页去重在基于Web企业竞争情报平台中的应用与研究·云南民族大学学报，2008(4).380－382.  \n[23]鄢靖丰，程菊明，熊德兰，等·基于小世界现象的网页消重和排序·计算机工程，2008,34（23)：136－138.  \n[24]张元丰，董守斌，张凌，等.基于Map/Reduce的网页消重并行算法·广西师范大学学报，2007,25(2)：153—156.  \n[25]钱爱兵，江岚·基于后缀树的中文新闻重复网页识别算法·现代图书情报技术，2008(3).55-61  \n[26]王鹏，张永奎，张彦，等·基于新闻网页主题要素的网页去重方法研究·计算机工程与应用，2008(28)：177－180.  \n[27]罗永莲，罗永秀，张永奎·突发事件新闻网页的去重方法研究·计算机应用与软件，2008(8)：24－26.",
    "context2": "[作者简介]李志义，男，1968年生，副教授，硕士生导师，发表论文30多篇。梁士金，男，1984年生，硕士研究生，发表论文1篇。"
  }
}