{
  "original_filename": "full_9530.md",
  "词向量语义表示研究进展": {
    "context1": "李枫林,柯佳",
    "context2": "(武汉大学 信息管理学院，湖北武汉430072)",
    "context3": "摘要：【目的/意义】词是语言的最小单元,词的向量表示决定了机器学习模型的构建方法。深度学习的神经网络训练得到的词向量,通过无监督的机器学习方法从海量数据中自动学习词汇的语义特征,无需人工标注和复杂繁琐的特征工程,端到端的完成各种自然语言处理任务,带来了一种新的研究范式,成为学术界的研究热点。【方法/过程】介绍了词向量语义表示及优化方法,存在的问题及解决方法，最后指出了词向量未来的研究方向【结果/结论】将句法特征、词形特征、(知识库)先验语义知识融入到神经网络模型能增强词向量的语义表示能力,针对词向量存在的一词多义、解释性差等问题,总结了最新的研究成果。",
    "context4": "关键词：词向量;分布式表示;深度学习;神经网络 中图分类号：G254 DOI:10.13833/j.issn.1007-7634.2019.05.025"
  },
  "Research Progress of Word Vector Semantic Representation": {
    "context1": "LI Feng-lin,KE Jia",
    "context2": "(Department of Information Management, Wuhan University,Wuhan 430072,China)",
    "context3": "Abstract:【Purpose/significance]Word is thesmallest unit oflanguage,word vector representation determines constructing machine learning model.Deep learning neural network train the word vector,unsupervised automaticaly learning feature from the massdata,without annotationandcomplex feature engineering,end-to-end tocomplete avarietyof natural language procesing tasks,ring anewresearch paradigm,become a hottopicresearch.【Method/processThis paperintroduces heemanticrepresentationandoptimizationmethodof wordembedding,thedificultiesandsolutionsofresearch,finally pointsoutthe future research directionofword mbedding.【Result/conclusion]The syntacticfeatures,morphological featuresand (knowledge base)prior knowledge incorporated into the neural network can enhance word embedding semantic representation, summarize the latest research progress of polysemy, poor interpretation and so on.",
    "context4": "Keywords: word embedding;distributed representation;deep learning;neural network;"
  },
  "1引言": {
    "context1": "语义是语言表达的含义,语言具有歧义性、多样性、复杂性等特点，为了让计算机更好地理解、表示语言，必须要建立语言模型,找到合适的词表示方法。传统的0-1词向量表示，无法反映单词之间的语义关联,容易产生数据稀疏和维度灾难问题。近年来，深度学习技术快速发展,神经网络训练得到的词向量，通过无监督的机器学习方法从海量数据中自动学习词汇的语义特征，无需人工标注和复杂繁琐的特征工程，端到端的完成各种自然语言处理任务，提升了性能。因此，如何找到通用的、适应迁移学习的词向量表示方法成为近年来学者研究的热点问题。",
    "context2": "目前基于深度学习的词向量研究主要集中在两个方面：$\\textcircled{1}$ 神经网络融入不同的外部信息(特征),优化词向量的语义表示能力； $\\textcircled{2}$ 针对词向量某一方面存在的问题,提出解决方法。本文旨在对近年来基于神经网络的语言模型的词表示方法展开研究，为学者今后的研究提供参考和帮助。",
    "context3": "表10-1词向量与词嵌入对比",
    "context4": "<table><tr><td>词向量</td><td>0-1词表示</td><td>词嵌人</td></tr><tr><td>特点</td><td>离散符号</td><td>低维、连续、实数</td></tr><tr><td>语义</td><td>无法反映词与词的语义关联</td><td>反映词的相关性、相似性</td></tr><tr><td>语料要求</td><td>大量</td><td>海量</td></tr><tr><td>先验知识依赖度</td><td>依赖度高</td><td>依赖度低</td></tr><tr><td>优点</td><td>易操作、易解释、有理论基础</td><td>无监督训练、可迁移学习</td></tr><tr><td>缺点</td><td>根据任务、需要复杂的特征工程</td><td>缺乏理论基础、可解释性差</td></tr><tr><td>句子/篇章表示</td><td>共现矩阵、特征工程降维</td><td>RecNN、CNN、RNN、LSTM、GRU建模</td></tr><tr><td>完成nlp任务关键</td><td>人工标注、复杂特征工程</td><td>自动学习特征,关键在于模型选择、调整参数</td></tr></table>"
  },
  "2传统的词表示方法- -独热表示(one一hot)": {
    "context1": "文本表示将语言形式化描述或数学描述，以便计算机自动处理,处理的不是抽象的语义，而是计算模型。为了让计算机理解语言，需要将不同粒度的语言(词汇、短语、句子)转换成计算机可以处理的数据结构。计算机语言表示的基础是1954 年Harris 提出的分布假说(distributional hypothe-sis)：“上下文相似的词,其语义也相似”。1957年,Firth[2完善了这一假说,指出\"词的语义由其上下文决定(You shallknow a word by the company it keeps)”。",
    "context2": "词汇是语言的最小单元,词的向量表示决定了机器学习模型的构建方法。多年来，自然语言处理领域最常用的词表示方法是独热表示(one-hot representation）。单词是一种符号化的离散表示,向量的维度为词表的长度,词表中的每个词表示为一个高维向量，每个词只有一个维度的值为1,代表当前词在词表中的位置,其它维度都为0。这种独热词表示方法虽然简洁有效,但存在两个缺点：",
    "context3": "(1)仅仅将词符号化，不包含任何语义信息，词与词之间独立，无法反映词与词之间的语义关联,即\"语义鸿沟”,也无法考虑文本词序信息。",
    "context4": "(2)词表示的高维度和稀疏性，易出现\"维数灾难”(curse of dimensionality)问题。以统计语言模型(StatisticalLanguageModeling)为例,使用one-hot词表示，即使是三元语言模型,也需要平滑技术解决数据稀疏问题。"
  },
  "3 深度学习词表示方法——词嵌入(wordembedding)": "",
  "3.1神经网络语言模型": {
    "context1": "语言模型,就是将一句话看作一个单词序列 $\\mathbf { W } _ { 1 } , \\mathbf { W } _ { 2 } , \\cdots \\mathbf { W } _ { \\mathrm { n } }$ 训练句子中单词出现的联合概率分布，以预测下一个出现的单词,它已被广泛应用在语音识别、信息检索领域。",
    "context2": "1986年,Hinton 等人[3提出单词分布式表示(distributedrepresentation)的构想,1989年,Miikkulainen 等人[4提出神经网络语言模型的概念,语言模型最终目标,是预测某个单词在下文出现的可能性。2000年, $\\mathrm { X u }$ 等人[5首次尝试使用神经网络建模二元语言模型,2003年，Bengio 等人正式提出神经网络概率语言模型(neural probabilistic language mod-el),尝试一种通用框架学习单词分布式表示和任意N元语言模型。"
  },
  "3.2词嵌入": {
    "context1": "词嵌入不同于过去的单词one-hot离散符号表示，而是将单词表示为一个连续、低维、实数向量(通常100-300维)，每一维代表了一定的语义。神经网络可以对更复杂的上下文建模，自动学习单词复杂的特征(如句法和语义),而且在训练的过程中，可以根据任务调整参数,更新词向量，达到性能最优。词向量空间距离表示词汇语义相似度，向量距离越近,代表语义越相似。我们通常把神经网络模型训练得到的词向量称为词嵌入，下文提到的词向量均是指神经网络语言模型训练得到的词嵌入。",
    "context2": "早期受计算机硬件性能所限,对词嵌入的研究主要集中在模型的加速[7-12]上。2008年,Collobert等人[13]提出C&W模型,这是首个直接以训练词向量为目标的神经网络模型，并将词向量用于完成命名实体识别、词性标注、语义角色标注等自然语言处理任务，但模型训练速度慢,效率低。",
    "context3": "2013年,Mikolov 等人[4]从十多亿个Google新闻单词中训练得到词向量——Word2vec,并提出了两个神经网络语言模型——连续词袋模型CBOW（Continuous Bag of Words）和Skip-gram模型,又提出了层次 Softmax(Hierarchical Softmax)和负采样(Negative Sampling)策略[15],加快模型训练速度。Word2vec是目前使用最广泛的神经网络词向量。",
    "context4": "Pennington等人[1]针对词向量Word2Vec训练模型没有考虑词序，没有考虑文本全局统计信息的不足，提出全局词——词共现矩阵,得到一种新的词向量Glove。表1比较了0-1独热词向量与神经网络词嵌入的特点。"
  },
  "4 词嵌入语义优化": {
    "context1": "改进现有词嵌入，融入更多的先验知识,提出更好的通用任务框架,是词嵌入的主要研究内容。近年来,学者提出了许多语义优化方案以求得到更好的词向量。",
    "context2": "词嵌入语义优化研究围绕着两个方面展开： $\\textcircled{1}$ 选取什么特征优化词汇语义表示能力; $\\textcircled{2}$ 怎样表示这些特征。我们将",
    "context3": "这些特征分为句法、词形、(外部知识库)语义三类特征。"
  },
  "4.1句法(syntactic)信息作为特征": {
    "context1": "Mikolov[4]之前的神经网络建模的上下文大多是线性(linear)的,即目标词的前几个词和后几个词。由于nlp任务的目标不同,窗口上下文过小会导致捕获的特征不足,太大会导致噪声。斯坦福大学计算机科学与语言学系教授Christopher Manning[17]一直坚持认为：自然语言具有递归(Recursion)特性,句子是由词汇、短语递归组合而成,句法结构是理解语义的基础,应该将句法信息作为先验知识(特征)融入到神经网络模型中。",
    "context2": "Levy[18]和Bansal[19]在 Skip-Gram模型[14]的基础上,将句法依存信息作为“上下文特征”,捕获更多的句法特征优化词向量。Ling等人[2针对之前的模型没有考虑上下文的词序,提出了一个改进模型优化词向量，更好的完成句法相关任务。",
    "context3": "Cross 等人[2]利用句子单词的依存关系改善词向量的语义表示能力。Kohn[22]根据不同的句法特征对词向量聚类发现,在特定的多语言评估方法下表现最佳。Peters等人[23提出深度语境化(deep contextualized)的词向量-ELMo,双层Bi-LSTM对单词不同特征编码，低层Bi-LSTM编码句法（词性),高层Bi-LSTM编码词义，消除歧义。相比之前的词向量做了两个方面的改进： $\\textcircled{1}$ 模型考虑了句子的句法和语义信息,可以根据不同的语境(contextualized)改变单词语义，解决一词多义的问题; $\\textcircled{2}$ 考虑了单词的形态特征，词向量表示是完全基于字符的(purely character based),能处理语料中的未登录词,泛化能力更强，在多个 $\\mathrm { n l p }$ 任务上性能优良，模型缺点在于参数太多，计算量很大。",
    "context4": "将句法作为特征加入到神经网络模型中，弥补了之前神经网络窗口较小无法捕获单词远距离依赖关系,过滤了窗口内偶然共现的单词,模型训练得到的模型更侧重于描述单词语法方面的相似性，反映词与词之间的语法关系，能更好的完成句法相关的任务(关系抽取、情感分析)。"
  },
  "4.2单词词形(morphology)作为特征": {
    "context1": "分布式假设2忽略了单词内部结构所包含的语义信息,通常英语单词的前缀、后缀体现了一定的语义,例如前缀“un\"代表了否定含义,\"ly\"可以区分“形容词\"和\"副词”。将单词词形作为特征还可以解决未登录词(outof vocabulary)的向量表示问题。",
    "context2": "Luong 等人[24]将单词的每个词素(morpheme)看作一个基本单元,利用递归神经网络组合上下文词素语义及特征。Qiu等人[25]在神经网络模型中加入单词词形联合训练。Bi-an 等人[2将单词的形态信息(前缀后缀等)、词性(POS)和语义信息(同义词等)添加到CBOW模型[4中共同训练。Bo-tha 等人[27提出一个结合单词词形的可扩展训练框架。Bo-janowski[28]和Joulin[29将输入的每一个单词向量表示为其上下文 $\\mathrm { n - g r a m } \\left( \\mathrm { n } { = } 3 \\sim 6 \\right)$ 向量之和，词向量连接一起输入到神经网络模型中,转变成用中心词的n-gram 词向量预测目标词,利用英文词形(前缀和后缀)相似性优化词向量,模型以维基百科为训练语料,在157种语言上进行训练,速度快,效率高,得到了词向量fasttext。词向量fasttext不仅可以分类文本,还能学习到训练语料中的未登录词。Cao 等人[将词形和词向量一起联合训练,将单词看作字符序列,字符char作为最小的单元,每个字符都用一个正向LSTM和一个反向LSTM建模拼接，再用注意力机制学习单词表示，模型的特点在于能用未知词的词干来预测该词的词向量，从某种程度上解决了未登录词的词表示问题,同时也得到了一个效果不错的词形分析器。"
  },
  "4.3利用知识库的先验语义知识": {
    "context1": "虽然神经网络模型训练得到的词向量反映了词的相关关系(related),但受模型窗口大小的限制，较难学习单词的细粒度语义。Rubinstein等人[3实验对比四种词嵌入发现,使用神经网络学习得到的词嵌人，不能很好的捕获名词的属性(attributive properties）。而外部知识库包含了丰富的名词属性信息。",
    "context2": "将WordNet、FreeBase等外部知识库的先验语义关系融入到模型中,是一种有效提升词嵌入语义表示能力的方法。利用知识库优化词向量主要有两种方法：一种是修改已有模型的目标函数,引入外部知识库知识(例如单词间关系、单词的属性)作为约束,优化单词语义表示的能力。一种是\"改装\"(retrofitting)[32],在后处理阶段改进预训练的词嵌入。",
    "context3": "$\\mathrm { Y } _ { \\mathrm { U } }$ 等人[33]在CBOW模型[4的基础上，提出关系约束模型RCM（RelationConstrainedModel),利用已有先验知识库（WordNet[34]和Paraphrase Database[3sl)的同义词关系作为约束,提升词向量语义表示能力。但他们的工作忽略了知识库中单词之间关系的不同。Bian等人[将词典WordNet、英文朗文词典等知识库中词法、句法以及语义等外部知识引入到CBOW 模型[14]中。 $\\mathrm { X u }$ 等人[]基于 Skip-gram 模型[14],利用知识库中单词关系和属性信息作为距离约束，提出融合关系知识和分类知识的训练框架RC-NET。Tissier等人[38将剑桥、牛津、柯林斯字典的共现词当作正例和负例加入到神经网络的负采样过程,能相互解释的词不能被用作负样本。",
    "context4": "Faruqui 等人[32]将语义词典(WordNet[34]、Paraphrase Da-tabase[35]、FrameNet[39-40])中的关联单词构建单词网络,并使用信念传播 BP(Belief Propagation)更新单词的向量表示,使得语义相关的单词的空间向量距离更近，将单词的关系加入到嵌入空间中。",
    "context5": "表2 词嵌入语义优化方法",
    "context6": "<table><tr><td>模型融入特征</td><td>解决的问题</td></tr><tr><td>句法特征</td><td>更好的处理句法相关任务(关系抽取、情感分析)</td></tr><tr><td>词形特征</td><td>初始化未登录词</td></tr><tr><td>知识库语义特征 捕获单词细粒度语义</td><td></td></tr></table>",
    "context7": "尽管语义优化取得了一定的进展(表2),但如何将先验知识与词嵌入相结合仍面临着挑战：无论是修改目标函数还是后处理,都只能加入有限的先验知识,目前仍然缺乏一个通用的框架来支持向神经网络中加入任意的先验知识。"
  },
  "5 词嵌入存在的问题": "",
  "5.1一词多义(polysemy)问题": {
    "context1": "词嵌入最大的一个问题就是用一个向量(空间上的点)表示一个单词无法处理多义词的情况,比如单词bank,在不同的上下文语境中,可能指\"银行”,可能指\"岸边”。目前解决一词多义问题主要有三种方法：",
    "context2": "(1)无监督(聚类)方法解决一词多义。",
    "context3": "Huang 等人[4结合目标词的局部上下文窗口（前后5个单词)和全局语义信息(tf-idf值),利用k-means算法聚类每个单词的上下文表示,依据聚类结果对训练语料中的单词重新标注不同的词义，为每个单词建立多个词向量表示。Tian 等人[42]在skip-gram模型[4基础上,融合EM算法和概率模型解决单词一词多义问题。Neelakantan等人[43]在Skip-Gram模型[4]的基础上,提出非参模型NP-MSSG,将一个单词的每个义项都用一个向量表示,在不同的上下文中选择一个最相关的义项进行更新，再用非参估计自动学习每个单词的义项。Qiu等人[44针对单词的不同词性学习单词的向量表示,词性标注分为细、中、粗三种粒度。",
    "context4": "Liu 等人[45]提出了TWE 模型（TopicalWord Embed-dings）,尝试利用主题模型解决单词的一词多义问题。在Skip-Gram模型基础上引入LDA[4的话题信息,结合不同的主题得到每个单词不同义项的向量表示。Liu等人[4使用张量神经网络扩展Skip-Gram模型[4],提出一个通用的模型NTSG(Neural Tensor Skip-Gram)学习话题相关的单词表示。Nguyen等人[48]在Liu[45]基础上,提出MSWE模型,TWE模型[45]在给定上下文中会选取最合适的含义,而MSWE则使用权重混合表示给定上下文中单词和不同含义的关联程度。",
    "context5": "(2)利用外部知识库将词表示拆分(de-conflating)为多个含义表示（sense representations）。这一方法的缺点在于受限于知识库的语种,有些小语种没有相对完善的外部知识库。",
    "context6": "Chen 等人[49]修改 Skip－gram 模型[14]目标函数,提出了联合词义表示和词义消歧的统一模型,根据语义词典Word-$\\mathrm { N e t } ^ { [ 3 4 ] }$ 对单词的注释生成词向量,对词义相近的预训练词嵌入取平均值,作为词嵌入的初始值。Chen 等人[5将外部知识库WordNet语义信息融入到卷积神经网络。Iacobacci等人[利用语义词典BabelNet[2将语料中的单词标记成含有多个词义的向量SensEmbed。Rothe等人[53提出自动编码器架构AutoExtend,利用语义词典WordNet的三种数据类型(data type):words,synsets(同义词),lexemes(一词多义,一词多形)作为约束,基于两个限制：词向量对应于其多个词义向量之和,同义词向量对应于其词汇/词义之和。例如,词向量mouse对应于mouse(老鼠)和mouse(鼠标)两个词义向量之和,同义词向量\"预订\"对应于词义向量reserve、hold、book之和,解决多义词问题。Jauhar等人[54采用本体扩展的方法，学习多义词的词嵌入。Johansson等人[ss]将后处理预训练词嵌入看作优化问题：多义词的词嵌入可以分解为其多个词义的词嵌入的组合，同时某个词义的词嵌入应该靠近其瑞典语语义网络SALDO的邻居节点，他们的方法可以扩展到其他语义网络。Niu 等人[在Skip-Gram模型[基础上,将中文知识库HowNet的词汇义原(sememe)融入模型,提出了SAT(sememe attention over target model)模型,根据上下文消除中心词歧义，使用注意力机制计算上下文对该词汇各个含义(sense)的权重,最后使用含义嵌入(sense embedding)的加权平均值作为词的向量表示,优化词向量。Mancini等人[修改词向量Word2Vec的目标函数,在同一向量空间中同时学习多义词的词嵌入。",
    "context7": "(3)利用平行语料(另一种语言)解决一词多义问题。例如英语单词mouse既可以指动物\"老鼠”,也可以指\"鼠标”，但在中文语料中，不存在这种歧义。",
    "context8": "Zou 等人[58最早提出把双语词汇映射到同一空间学习词向量，用于解决机器翻译任务。Guo等人[59利用双语(英汉)平行语料库学习单词词嵌入,利用源语言多义词在另一语言中有不同的翻译词这一原理,聚类单词翻译,并映射回原语言语料库中对应的单词,训练得到一个单词的语义表示。Faruqui 等人[o]使用典型相关分析(canonical correlationanalysis)学习平行语料单词表示,显著提升了英文单词词嵌入语义质量。Upadhyay等人[利用多语言并行数据学习多义词单词嵌入。"
  },
  "5.2反义词问题": {
    "context1": "词向量不仅反映了词汇的相似性，也反映了词汇的相关性[2],通过无监督方式得到的词向量会出现反义词空间距离很近的情况，例如形容词\"好\"和\"坏”，这给情感分类[63.64]任务带来了困难。",
    "context2": "Liu等人[5]在Skip-gram模型[4]基础上,提出SWE模型,将知识库中词语间的关系(同义，上下位等)表示成不等式约束,解决反义词之间的距离比同义词之间的距离更近的问题。Nguyen等人[]在Skip-gram模型[4]基础上提出dLCE模型，改变了模型的目标函数,加入知识库词汇对比信息，加强了决定词向量相似度的特征，使得训练得到的词向量能有效识别反义词(Antonym)和同义词(Synonym）。模型思想在于,同义词集合(Synset)具有反义词集合不具备的文本特征。"
  },
  "5.3未登录词处理": {
    "context1": "使用预训练的词嵌入存在无法处理未登录词的问题，未登录词是指没有在训练语料中出现过的单词。通常的做法是使用子字级别(sub word)的词嵌入来训练未登录词。",
    "context2": "Pinter 等人[在已有的词嵌入基础上,学习一个字符级别的序列向量表示。利用双向循环神经网络学习字母组成语义的规律，当遇到未登录词猜测其词向量表示。这种方法在缺少语料的情况下特别有用，尤其对于中文非常有效（中文单个汉字表达了一定的语义信息)。Bojanowski等人[8利用词缀包含的语义信息训练未登录词。Herbelot等人[将上下文单词向量求和平均值作为未登录词的初始值,再用高学习率快速优化未登录词的向量表示。"
  },
  "5.4 词向量的可解释性": {
    "context1": "神经网络训练得到的词向量在完成nlp任务时取得了良好的性能,但词向量的不可解释性一直是其备受争议的问题,研究者尝试用数学推导的方法发现其与传统文本语义表示方法的联系与区别。",
    "context2": "Levy 等人[]通过数学公式推导证明,词向量word2vec的Skip-Gram模型如果采用负采样(SGNS)学习,实质就是对传统语义相关性的互点信息矩阵PMI[](Pointwise Mutual In-formation)进行因式分解。Tsvetkov 等人[2]尝试解释词嵌入每一维度代表的意义，学习矩阵间行向量的对齐方式，不过这一方法仅限于英语,扩展性较差。Arora等人[73用对数线性生成模型解释了词嵌入Word2Vec为何能够学习、捕获单词间的类比关系。Gittens等人[4用数学公式推导证明了Skip-Gram 模型[14]训练得到的词嵌入具有可加性(composi-tionality)。Faruqui等人[]使用多个知识库构建了一个可解释的词向量表示。将现有各种词嵌入，与可解释的词向量表示对齐，尝试解释每一个特征所对应的分布式表示。但得到的最终表示噪声较多且高维稀疏。"
  },
  "5.5词嵌入评价": {
    "context1": "由于单词语义由语境决定,无法简单衡量词嵌入的好坏,学者通常通过不同的任务判断词向量语义表示能力，词向量评价一直是充满争议的热点研究问题,缺乏统一的评价标准。Schnabel等人[针对词语相似性计算只反映了词汇关系的相关程度(strengthof the relation）,没有指出是哪种关系类别(typeof relation）,提出词向量的评价分为内部评价(Intrinsic evaluation)和外部评价(extrinsic evaluation）两类评价方法。"
  },
  "5.5.1 内部评价": {
    "context1": "内部任务评价是指语法、语义相关的评价[14],主要包括词汇语义相似度(Semantic similarity)和词汇类比(word anal-ogy)任务。用人工标注的单词相似性打分值作为评判基准，比较神经网络训练得到的词嵌入与基准值的差异来判断单词的相似(相关)性。这两个任务相对简单,运算速度快,通过一个数值来反映两个单词的语义关系。",
    "context2": "2013年,Mikolov等人[4]首先提出用类比任务评价词向量,用单词的余弦相似度比较词之间的类比关系,预先定义14类类别任务,语义类比5类,例如“France”-“Paris” $=$ \"Ja-pan”-“Tokyo”,语法类比9类,比如“walk”－“walking” $=$ \"swim”-\"swimming”。",
    "context3": "2014年,Baroni等人[7]为了对比传统one-hot词向量和神经网络训练得到的词向量在语义表示方面的差异，完成了五个内部评价任务：语义相关性(Semantic relatedness）、同义词检测（Synonym detection）、概念分类（Concept categoriza-tion）、主谓搭配（Selectional preferences）、类比（Analogy）,分析了词向量不同的语言学特性。",
    "context4": "Yaghoobzadeh[78]认为，每个词都是由多个构面(facet)组成的,体现着语法、语义、词形(morphologoly）、时态、人称等等多方面，内部评价的缺点在于可解释性差,无法解释为何词向量A在某个NLP任务上的性能优于词向量B,无法判断从哪方面改进模型提升性能,另外由于人工标注的词汇相似性打分相对主观,存在不一致(inconsistency）,分数未必可靠(unreliable),用这样的分数作为标准本身就值得商榷。基于单词相似性的评价方法对训练数据大小、领域及词表敏感，扩展性不强。",
    "context5": "Faruqui等人[指出词向量内部评价存在几个问题： $\\textcircled{1}$ 用于评价单词相似性(similarity)的任务语料标注主观,容易与相关性(relatedness）混淆; $\\textcircled{2}$ 到底语义相似还是任务相似？ $\\textcircled{3}$ 数据集没标准分割，本质上都在调测试集，缺乏合适的数据集，而且针对不同的领域任务可能需要不同的数据集； $\\textcircled{4}$ 内部评价同外部任务评价的相关性低； $\\textcircled{5}$ 无视统计显著性； $\\textcircled{6}$ 词频影响相似度； $\\textcircled{7}$ 一词多义问题。"
  },
  "5.5.2 外部评价": {
    "context1": "外部评价是将词向量作为神经网络模型的输入，完成特定nlp任务，例如将词向量作为命名实体识别、词性标注任务的输入[1s]。外部任务通常很复杂,计算速度慢。Tsvetkov等人[8实验发现计算词语相似度的内部评价方法与后续的nlp任务性能弱相关。",
    "context2": "2016年、2017年自然语言处理实证方法会议EMN-LP(Conference on Empirical Methods in Natural Language Pro-cessing)引人“向量空间表示研讨会RepEval\"(EvaluatingVector SpaceRepresentations）,旨在找到一种适应多种用途的（general-purpose）、能满足迁移学习的词嵌入评价方法，用于完成文本挖掘任务。会议指出：基于外部nlp任务的词向量评价比内部评价更合适,简单的内部评价比较方法在大多数情况下是无用的，只能说明词向量反映了一定的语法、语义信息。"
  },
  "6未来的研究方向": "",
  "6.1面向具体任务的(task-specific)词嵌入": {
    "context1": "如果能找到适合具体任务的词嵌入，就能大大提高效率,节约时间成本。Tang[81.82]和Maas[83]在情感分类任务中，直接在词嵌入学习模型的目标函数中引入情感极性信息，训练得到带有情感极性的词嵌入。"
  },
  "6.2更大语言单元(短语、句子)语义组合": {
    "context1": "利用词向量组合得到短语[84-87]、句子[88-90]甚至篇章[90-91]的向量表示,是完成信息抽取[86.92]、文本分类[93-94]、情感分析[63.95]机器翻译[%]等自然语言处理任务的基础。通过不同的神经网络建模短语、句子,获得更大语言单元的语义是一个热点研究问题,目前短语、句子语义组合方法已经从无监督的方法[15.97-99]转向可迁移的监督学习[100]和多任务学习[101]。"
  },
  "6.3发现单词(语义、语法)关系": {
    "context1": "词向量自身的研究也是一个值得深入研究的领域。Fu等人[02]利用词向量发现单词之间的上下位关系。Soricut等人[103]则利用词向量区分词形相似(前后缀)的单词。 $\\mathrm { K i m } ^ { [ 1 0 4 ] }$ 和 Kulkarni[105]利用网络在线语料和谷歌图书词频统计器(GoogleBooks Ngrams),使用词嵌入考察20 世纪单词语义演化。发现单词随时间的语义变化及变化规律[106-110]是近年热门的研究方向。"
  },
  "6.4多语言词向量表示": {
    "context1": "目前对于词向量的研究大多针对英文语料,设计能够使用尽可能少的平行语料完成跨语言nlp任务，使得资源较少的语言也能够学习出单词的向量表示进而完成机器翻译[]任务,是一个值得研究的方向。Ruder等人[2对于这一领域进行了详细的综述。",
    "context2": "中文词嵌入与英文词嵌入的不同在于：英文单词、词形(前后缀)反映语义，中文字、偏旁部首一定程度上反映语义，例如\"江”“河”、“湖”、“海”。Chen等人[提出将单个汉字作为目标词的上下文，和词语一起输入神经网络模型训练，得到了词向量CWE(character-enhanced word embeddding)。Li等人[14]借助CBOW 和 Skip-Gram 模型的思想,提出了charCBOW和charSkipGram两个模型，将繁体字的部首信息加入到字向量的训练过程,作者认为一个字的部首比字的其他部分包含更多的语义信息，模型将原有隐藏层向量相加改成了向量首尾相连。徐健[]为了解决中文汉字在不同词汇中语义不一致的问题,提出了两个中文词向量模型：基于相似度的中文字词向量模型 SCWE（Similarity-based CharacterandWordEmbedding)和基于相似度的中文字词联合学学习模型 SJLCWE(Similarity-based Joint Learning of CharacterandWordEmbeddings）。模型利用网络在线翻译词典,通过字词对齐模型计算字、词语相似度，SCWE解决了因为忽略汉字权重而导致的词语语义偏离问题,SJLCWE通过加入汉字丰富了词语的上下文语义。Zhao[和Li[]等人使用神经网络模型从海量的中文语料(百度、维基、人民日报、社交媒体、新闻、古汉语书籍等)训练得到词向量——Ngram2vec,并提出了中文词向量评价方法。"
  },
  "7结语": {
    "context1": "词汇作为语言的基本单元,其表示方式决定了机器学习模型的选择和算法，是自然语言处理的基础。传统的0-1词表示无法反映词与词之间的语义关联,存在数据稀疏和维度灾难的问题;神经网络语言模型训练得到的词向量，不仅能反映语义，而且将词向量作为神经网络模型的输入，能提升自然语言处理任务的性能，解决了人工标注样本不足的问题,避免了复杂繁琐的\"特征工程”;但由于神经网络类似一个“黑盒”,缺乏足够的理论依据,实验依靠经验,数据规模、语料领域、任务类型、网络结构、超参数设置都会影响实验结果,所以目前对词嵌入的研究还在初级阶段,未来还有许多问题值得深入研究。",
    "context2": "本文介绍了近几年词嵌入语义优化方法,将句法、词形、(知识库)先验语义特征融入到神经网络模型中能强化词向量的语义表示能力,针对词向量存在的一词多义、可解释性差等问题,梳理了最新的研究成果,最后指出了未来的研究方向，希望本文能对学者今后的研究提供参考和帮助。"
  },
  "参考文献": {
    "context1": "1 Harris Z S.Distributional Structure[J].Word,1954, $1 0 ( 2 - 3 ) \\colon 1 4 6 - 1 6 2 .$   \n2 Firth JR.A Synopsis of Linguistic TheoryUl .Studies in Linguistic Analysis,1957,(2):1-32.   \n3 Hinton G E.Learning distributed representations of concepts[C]//Proceedings of the eighth annual conference of the cognitive science society,1986: 12.   \n4 Miikkulainen R,Dyer M G. Natural Language Processing with Modular Neural Networks and Distributed Lexicon[C]// Cognitive Science,1991:343 -399.   \n5 Xu W,Rudnicky A. Can artificial neural networks learn language models?[C]//Sixth International Conference on Spoken Language Processing,2000.   \n6Bengio Y,Ducharme R,Vincent P,et al.A neural probabilistic language model[Jl . Journal of machine learning research,2003,3(Feb): 1137-1155.   \n7 Morin F,Bengio Y.Hierarchical Probabilistic Neural Network Language Model[C]//International conference on artificial intelligence and statistics,2005.   \n8 Mnih A,Hinton G. Three new graphical models for statistical language modelling[C]//Proceedings of the 24th international conference on Machine learning. ACM,2007: 641-648.   \n9 Mnih A,Hinton G E.A scalable hierarchical distributed language model[C]//Advances in neural information processing systems,2009: 1081-1088.   \n10 Mikolov T,Karafiát M,Burget L,et al. Recurrent neural network based language model[C]//Eleventh Annual Conference of the International Speech Communication Association,2010.   \n11 Mnih A,Teh Y W.A fast and simple algorithm for training neural probabilistic language models[EB/OL]. https://arxiv. org/ftp/arxiv/papers/1206/1206.6426.pdf,2012-06-06.   \n12 Mnih A, Kavukcuoglu K. Learning word embeddings efficiently with noise-contrastive estimation[C]//Advances in neural information processing systems,2013: 2265-2273.   \n13Collobert R,Weston J.A unified architecture for natural language processing: Deep neural networks with multitask learning[C]//Proceedings of the 25th international conference on Machine learning,ACM,2008: 160-167.   \n14 Mikolov T,Chen K,Corrado G,et al. Efficient Estimation of Word Representations in Vector Space[EB/OL]. https:// arxiv.org/pdf/1301.3781.pdf,2013-09-07.   \n15 Mikolov T, Sutskever I, Chen K,et al. Distributed representations of words and phrases and their compositionality [C]//Advances in neural information processing systems, 2013: 3111-3119.   \n16 Pennington J,Socher R,Manning C.Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),2014: 1532-1543.   \n17 Manning C D. Computational linguistics and deep learning [] .Computational Linguistics,2015,41(4): 701-707.   \n18 Levy O, Goldberg Y. Dependency-based word embeddings [C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2014: 302-308.   \n19Bansal M,Gimpel K,Livescu K. Tailoring continuous word representations for dependency parsing[C]//Procedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014: 809-815.   \n20 Ling W,Dyer C, Black A W, et al. Two/too simple adaptations of word2vec for syntax problems[C]//Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2015: 1299-1304.   \n21 Cross J, Xiang B, Zhou B. Good, Better,Best: Choosing Word Embedding Context[J] . Computer Science,2015,31 (3):624-634.   \n22 Kohn A. What’s in an embedding? Analyzing word embeddings through multilingual evaluation[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015: 2067-2073.   \n23Peters M E,Neumann M,Iyer M,et al. Deep contextualized word representations[EB/OL]. https://arxiv.org/pdf/ 1802.05365.pdf,2018-03-22.   \n24 Luong T, Socher R,Manning C. Better word representations with recursive neural networks for morphology[C]// Proceedings of the Seventeenth Conference on Computational Natural Language Learning,2013: 104-113.   \n25 Qiu S, Cui Q, Bian J,et al. Co-learning of word representations and morpheme representations[C]//Proceedings of COI I)n14+h。tl It>>t>1 C>f->）^ Computational Linguistics: TechnicalPapers,2014: 141-150.   \n26 Bian J, Gao B,Liu TY. Knowledge-Powered Deep Learning for Word Embedding[C]//European Conference on Machine Learning and Knowledge Discovery in Databases. Springer-Verlag New York, Inc,2014:132-148.   \n27Botha J,Blunsom P. Compositional morphology for word representations and language modelling[C]//International Conference on Machine Learning,2014: 1899-1907.   \n28 Bojanowski P, Grave E,Joulin A,et al. Enriching word vectors with subword information[EB/OL]. https://arxiv.org/ pdf/1607.04606.pdf,2017-06-19.   \n29 Joulin A,Grave E,Bojanowski P,et al.Bag of tricks for efficienttextclassification[EB/OL].https://arxiv.org/pdf/ 1607.01759.pdf,2016-08-09.   \n30 Cao K,Rei M.A joint model for word embedding and word morphology[EB/OL].htps://arxiv.org/pdf/16062601 1.pdf,2016-06-08.   \n31Rubinstein D,Levi E, Schwartz R,et al. How well do distributional modelscapture different typesof semantic knowledge?[C]//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 2: Short Papers),2015: 726-730.   \n32Faruqui M, Dodge J, Jauhar S K,et al. Retrofitting word vectors to semantic lexicons[EB/OL]. htps://arxiv.org/pdf/ 1411.4166.pdf,2015-03-22.   \n33 Yu M, Dredze M. Improving lexical embeddings with semantic knowledge[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2014: 545-550.   \n34Miller G A.WordNet: a lexical database for English[J] . Communications of the ACM,1995,38(11): 39-41.   \n35Ganitkevitch J，Vandurme B,Callison-Burch C.PPDB: The Paraphrase Database[C]//Conference of the North American Chapter of the Association for Computational Linguistics,2013.   \n36 Bian J, Gao B,Liu T Y. Knowledge-powered deep learning for word embedding[C]//Joint European conference on machine learning and knowledge discovery in databases. Springer,Berlin,Heidelberg,2014: 132-148.   \n37 Xu C,Bai Y,Bian J,et al. RC-NET: A General Framework for Incorporating Knowledge into Word Representations[C]//ACM International Conference on Conference on Information and Knowledge Management,ACM,2014: 1219-1228.   \n38 Tissier J, Gravier C, Habrard A. Dict2vec: Learning Word Embeddings using Lexical Dictionaries[C]//Conference on Empirical Methods in Natural Language Processing (EMNLP 2017),2017: 254-263.   \n39 Baker C F,Fillmore C J, Lowe JB. The berkeley framenet project[C]//Proceedings of the 17th international conference on Computational linguistics-Volume 1.Association for Computational Linguistics, 1998: 86-90.   \n40 Fillmore C J, Johnson C R,Petruck MR L. Background to framenet[J] . International journal of lexicography,2003, 16(3): 235-250.   \n41 Huang E H, Socher R, Manning C D,et al. Improving word representations via global context and multiple word prototypes[C]//Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012:873-882.   \n42 Tian F,Dai H, Bian J, et al. A probabilistic model for learning multi-prototype word embeddings[C]//Proceedings of COLING 2014,the 25th International Conference on Com putational Linguistics: Technical Papers,2014: 151-160.   \n43Neelakantan A，Shankar J，Passos A，et al.Efficient non-parametric estimation of multiple embeddings per word in vector space[EB/OL]. https://arxiv.org/pdf/1504.0 6654.pdf,2015-04-24.   \n44 Qiu L, Cao Y, Nie Z et al. Learning Word Representation Considering Proximity and Ambiguity[C]//Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. Quebec,Canada,2014: 1572- 1578.   \n45 Liu Y, Liu Z, Chua T S, et al. Topical word embeddings [C]// Twenty-Ninth AAAI Conference on Artificial Intelligence. AAAI Press,2015:2418-2424.   \n46Blei D M,Ng A Y,Jordan MI. Latent dirichlet allocation U].Journal of machine Learning research,2OO3,3(Jan): 993-1022.   \n47Liu P,Qiu X,Huang X. Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model [C]//Proceedingsof the Twenty-Fourth International Joint Conference on Artificial Intelligence.Buenos Aires, Argentina,2015: 1284 - 1290.   \n48 Nguyen D Q, Nguyen D Q,Modi A,et al. A Mixture model for learning multi-sense word embeddings[C]// Joint conference on lexical and computational semantics, 2017: 121-127.   \n49 Chen X, Liu Z, Sun M.A unified model for word sense representation and disambiguation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),2014: 1025-1035.   \n50 Chen T,Xu R,He Y,et al. Improving distributed representation of word sense via WordNet Gloss composition and context clustering[] ．Atmospheric Measurement Techniques,2015,4(3):5211-5251.   \n51 Iacobacci I, Pilevar MT,Navigli R.Sensembed: Learning sense embeddings for word and relational similarity[C]// Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),2015: 95-105.   \n52 Navigli R,Ponzetto S P. BabelNet: The automatic construction,evaluation and aplication of a wide-coverage multilingual semantic network[J]．Artificial Intelligence, 2012, (193): 217-250.   \n53Rothe S, Schitze H. Autoextend: Extending word embeddings to embeddings for synsets and lexemes[EB/OL]. https: //arxiv.org/pdf/1507.01127.pdf,2015-07-04.   \n54Jauhar S K,Dyer C,Hovy E.Ontologically grounded multi-sense representation learning for semantic vector space models[C]//Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2015: 683-693.   \n55 Johansson R,Pina L N. Embedding a semantic network in a word space[C]//Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2015: 1428-1433.   \n56 Niu Y, Xie R,Liu Z,et al. Improved Word Representation Learning with Sememes[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2017: 2049-2058.   \n57Mancini M, Camacho-Collados J, Iacobacci I,et al. Embedding words and senses together via joint knowledge-enhanced training[EB/OL]. htps://arxiv.org/pdf/1612.02703. pdf,2017-06-21.   \n58 Zou W Y,Socher R,Cer D,et al. Bilingual word embeddings for phrase-based machine translation[C]//Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,2013: 1393-1398.   \n59Guo J， Che W, Wang H,et al. Learning sense-specific word embeddings by exploiting bilingual resources[C]// Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, 2014: 497-507.   \n60 Faruqui M, Dyer C. Improving vector space word representations using multilingual correlation[C]//Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,2014: 462-471.   \n61 Upadhyay S,Chang K W, Taddy M,et al. Beyond bilingual: Multi-sense word embeddings using multilingual context[EB/OL].https://arxiv.org/pdf/1706.08160.pdf,2017-06 -25.   \n·博士论坛·   \n62Kiela D,Hill F,Clark S. Specializing Word Embeddings for Similarity or Relatedness[C]//Conference on Empirical Methods in Natural Language Processing,2015:2O44-2048.   \n63 Socher R, Pennington J, Huang E H,et al. Semi-supervised recursive autoencoders for predicting sentiment distributions[C]//Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics,2011: 151-161.   \n64 Wang X, Liu Y, Chengjie S U N, et al. Predicting polarities of tweets by composing word embeddings with long short-term memory[C]//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan  \nguage Processing,2015: 1343-1353.   \n65 Liu Q, Jiang H,Wei S,et al. Learning semantic word embeddings based on ordinal knowledge constraints[C]//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),2015: 1501-1511.   \n66Nguyen K A, Walde S S, Vu N T. Integrating distributional lexical contrast into word embeddings for antonym-synonym distinction[EB/OL].https://arxiv.org/pdf/16050776 6.pdf,2016-05-25.   \n67Pinter Y,Guthrie R,Eisenstein J. Mimicking word embeddingsusing subword rnns[EB/OL]. htps://arxiv.org/pdf/ 1707.06961.pdf,2017-07-21.   \n68 Bojanowski P, Grave E,Joulin A, et al. Enriching word vectors with subword information[EB/OL]. https://arxiv.org/   \npdf/1607.04606.pdf,2017-06-19.   \n69Herbelot A, Baroni M. High-risk learning: acquiring new word vectors from tiny data[EB/OL]. htps://arxiv.org/pdf/ 1707.06556.pdf,2017-07-20.   \n70 Levy O,Goldberg Y. Neural word embedding as implicit matrix factorization[C]//Advances in neural information   \nprocessing systems,2014: 2177-2185.   \n71 Hanks P,Hanks P.Word association norms,mutual information,and lexicography[C]// Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1989:76-83.   \n72 Tsvetkov Y, Faruqui M, Ling W,et al. Evaluation of word vector representations by subspace alignment[C]//Proceed  \nings of the 2015 Conference on Empirical Methods in Nat  \nural Language Processing,2015: 2049-2054.   \n73 Arora S,Li Y, Liang Y,et al. Random Walks on Context Spaces: Towards an Explanation of the Mysteries of Semantic Word Embeddings[] .Physical Review B, 2015,86(3):   \n41-48.   \n74 Gittens A, Achlioptas D, Mahoney M W. Skip-gram-zipf+ umui viuiuI auuiuviy[C]//1iuuIgs Ui ui JJu Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2017: 69-76.   \n75Faruqui M,Dyer C. Non-distributional word vector representations[EB/OL].https://arxiv.org/pdf/1506.05230.pdf, 2015-06-17.   \n76Schnabel T,Labutov I,Mimno D,et al. Evaluation methods for unsupervised word embeddings[C]//Conference on Empirical Methods in Natural Language Processing,2015: 298-307.   \n77Baroni M, Dinu G,Kruszewski G. Don't count, predict! A systematiccomparisonofcontext-counting vs. context-predicting semantic vectors[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2014: 238-247.   \n78Yaghoobzadeh Y, Schitze H. Intrinsic Subspace Evaluation of Word Embedding Representations[C]//Meeting of the Association for Computational Linguistics,2016:236-246.   \n79 Faruqui M, Tsvetkov Y,Rastogi P,et al. Problems with evaluation of word embeddings using word similarity tasks[EB/ OLJ.https://arxiv.0rg/pdf/1605.02276.pdf,2016-06-22.   \n80 Tsvetkov Y, Faruqui M,Ling W, et al. Evaluation of word vector representations by subspace alignment[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015: 2049-2054.   \n81 Tang D,Wei F,Qin B,et al. Building large-scale twitter-specific sentiment lexicon: A representation learning approach[C]//Proceedings of COLING 2014,the 25th International Conference on Computational Linguistics: Technical Papers,2014: 172-182.   \n82 Tang D,Wei F, Yang N, et al. Learning sentiment-specific word embedding for twitter sentiment classification[C]// Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014: 1555-1565.   \n83Maas AL,Daly RE,Pham P T,et al.Learning word vectors for sentiment analysis[C]//Meeting of the Association for Computational Linguistics: Human Language Technologies.Association for Computational Linguistics，2011: 142-150.   \n84 Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality [C]//Advances in neural information processing systems, 2013:3111-3119.   \n85 Yu M, Dredze M. Learning composition models for phrase embeddings]. Transactions of the Association for Computational Linguistics,2015,(3): 227-242.   \n86 Hashimoto K, Tsuruoka Y. Adaptive joint learning of comnanitianal and non-comnaritional nhao omhaddinm[ER/ OL].https://arxiv.0rg/pdf/1603.06067.pdf,2016-06-08.   \n87 Socher R,Huval B, Manning C D,et al. Semantic compositionality through recursive matrix-vector spaces[C]//Proceedings of the 2O12 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics,2012:1201-1211.   \n88 Kim Y. Convolutional Neural Networks for Sentence Classification[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014: 1746-1751.   \n89 Kalchbrenner N,Grefenstette E,Blunsom P,et al.A Convolutional Neural Network for Modellng Sentences[C]// Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014: 655-665.   \n90 Le Q,Mikolov T.Distributed representations of sentences and documents[C]//International Conference on Machine Learning,2014: 1188-1196.   \n91Li J, Luong M T, Jurafsky D.A hierarchical neural autoencoder for paragraphs and documents[EB/OL]. https://arxiv. org/pdf/1506.01057.pdf,2015-06-06.   \n92 Zeng D,Liu K,Lai S,et al. Relation clasification via convolutional deep neural network[C]//Proceedings of COLING 2014,the 25th International Conference on Computational Linguistics: Technical Papers,2014: 2335-2344.   \n93Kim Y. Convolutional Neural Networks for Sentence Classification[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014: 1746-1751.   \n94 Lai S,Xu L, Liu K,et al. Recurrent Convolutional Neural Networks for Text Classfication[C]//Association for the Advancement of Artificial Intelligence,2015: 2267-2273.   \n95Socher R,Perelygin A,Wu J, et al. Recursive deep models for semantic compositionality over a sentiment treebank [C]//Proceedings of the 2013 conference on empirical methods in natural language processing,2013: 1631-1642.   \n96 Cho K,Van Merrienboer B,Gulcehre C,et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[EB/OL]. https://arxiv.org/pdf/ 1406.1078.pdf,2014-09-03.   \n97 Arora S,Liang Y,Ma T,et al. A Simple but Tough-to-Be at Baseline for Sentence Embeddings[EB/OL]. https://penreview.net/pdf?id $\\equiv$ SyK00v5xx,2014-05-03.   \n98 Kiros R, Zhu Y, Salakhutdinov R R,et al. Skip-thought vectors[C]//Advances in neural information processing systems,2015: 3294-3302.   \n99Logeswaran L, Lee H. An efficient framework for learning sentence representations[EB/OL].https://arxiv.org/pdf/180 3.02893.pdf,2018-03-07.   \n100 Conneau A, Kiela D, Schwenk H, et al. Supervised learning of universal sentence representations from natural language inference data[EB/OL].htps://arxiv.org/pdf/1705.0 2364.pdf,2018-07-08.   \n101 Subramanian S, Trischler A,Bengio Y,et al. Learning general purpose distributed sentence representations via large scale multi-task learning[EB/OL]. htps://arxiv.org/pdf/ 1804.00079.pdf,2018-03-30.   \n102 Fu R,Guo J,Qin B,et al. Learning semantic hierarchies via word embeddings[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2014: 1199-1209.   \n103Soricut R,Och F. Unsupervised morphology induction using word embeddings[C]//Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2015: 1627-1637.   \n104 Kim Y, Chiu Y,Hanaki K,et al. Temporal Analysis of Language through Neural Language Models[J] .computational social science,2014,(7): 61-65.   \n105Kulkarni V,Al-RfouR,Perozi B,et al. Statisticall ignificant detection of linguistic change[C]//Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Stering Committee,2015: 625-635.   \n106Hamilton W L,Leskovec J, Jurafsky D. Diachronic word embeddings reveal statistical laws of semantic change[EB/ OL]. https://arxiv.0rg/pdf/1605.09096.pdf,2018-10-25.   \n107Bamler R，Mandt S. Dynamic word embeddings via skip-gram filtering. arXiv preprint[EB/OL].https://www. researchgate.net/publication/314092405_Dynamic_Word _mbeddings,2018-10-25.   \n108 Dubossarsky H,Weinshall D,Grossman E. Out control: Laws of semantic change and inherent biases in word representation models[C]//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017: 1136-1145.   \n109 Szymanski T. Temporal word analogies: Identifying lexical replacement with diachronic word embeddings[C]//Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2017: 448-453.   \n110 Rosin G D,Adar E,Radinsky K. Learning word relatedness over time[EB/OL].https://arxiv.org/pdf/1707.08081. pdf,2017-07-30.   \n111 Chandar A P S, Lauly S, Larochelle H,et al. An autoencoder approach to learning bilingual word representations [C]//Advances in Neural Information Processing Systems, 2014: 1853-1861."
  }
}